{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Desktop-First Observability Platform for Local Development"},{"location":"#introducing-ollyscale","title":"Introducing ollyScale","text":"<p>ollyScale is an OpenTelemetry-native observability platform evolved from the excellent TinyOlly project.</p> <p>Repository: https://github.com/ryanfaircloth/ollyscale</p> <pre><code>git clone https://github.com/ryanfaircloth/ollyscale\n</code></pre>"},{"location":"#why-ollyscale","title":"Why ollyScale?","text":"<p>Why send telemetry to a cloud observability platform while coding? Why not have one on your desktop?</p> <p>ollyScale is a lightweight OpenTelemetry-native observability platform built to visualize and correlate logs, metrics, and traces. No 3rd party observability tools - just Python (FastAPI), Redis, OpenAPI, and JavaScript.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Development-focused - Perfect your app's telemetry locally before production</li> <li>Full OpenTelemetry support - Native OTLP ingestion (gRPC &amp; HTTP)</li> <li>Pre-built Docker images - Deploy in ~30 seconds from Docker Hub</li> <li>Multi-architecture - Supports linux/amd64 and linux/arm64 (Apple Silicon)</li> <li>Trace correlation - Link logs, metrics, and traces automatically</li> <li>Metrics Explorer - Analyze cardinality, labels, and raw series data</li> <li>Service catalog - RED metrics (Rate, Errors, Duration) for all services</li> <li>Interactive service map - Visualize dependencies and call graphs</li> <li>OpenTelemetry Collector management - Remote configuration management via OpAMP protocol</li> <li>REST API - Programmatic access with OpenAPI documentation</li> <li>Zero vendor lock-in - Works with any OTel Collector distribution</li> </ul> <p>Local Development Only</p> <p>ollyScale is not designed to compete with production observability platforms! It's for local development only and is not focused on infrastructure monitoring at this time.</p>"},{"location":"#platform-support","title":"Platform Support","text":"<p>Tested on:</p> <ul> <li>Docker Desktop (macOS Apple Silicon)</li> <li>Minikube Kubernetes (macOS Apple Silicon)</li> <li>May work on other platforms</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Ready to try ollyScale? Check out the Quick Start Guide to get running in under 5 minutes!</p>"},{"location":"#screenshots","title":"Screenshots","text":"Trace Waterfall with Correlated Logs Real-time Logs with Filtering Metrics with Chart Visualization Service Catalog with RED Metrics Interactive Service Dependency Map OTel Collector Configuration (OpAMP) <p>Built for the OpenTelemetry community</p> <p> GitHub \u2022     Issues </p>"},{"location":"CARDINALITY-PROTECTION/","title":"Cardinality Protection","text":"<p>Span waterfall showing distributed trace complexity</p> <p>ollyScale includes built-in protection against metric cardinality explosion with a configurable limit on unique metric names.</p>"},{"location":"CARDINALITY-PROTECTION/#configuration","title":"Configuration","text":""},{"location":"CARDINALITY-PROTECTION/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>MAX_METRIC_CARDINALITY</code> 1000 Maximum unique metric names <code>REDIS_TTL</code> 1800 Metric retention (seconds)"},{"location":"CARDINALITY-PROTECTION/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<p>Update Helm values in <code>charts/ollyscale/values.yaml</code>:</p> <pre><code>otlpReceiver:\n  env:\n    MAX_METRIC_CARDINALITY: \"2000\" # Increase limit\n    REDIS_TTL: \"3600\" # 1 hour retention\n</code></pre> <p>Then upgrade the deployment:</p> <pre><code>cd charts\nhelm upgrade ollyscale ./ollyscale -n ollyscale\n</code></pre>"},{"location":"CARDINALITY-PROTECTION/#docker-deployment","title":"Docker Deployment","text":"<p>Update <code>docker-compose-ollyscale-core.yml</code> in the <code>docker/</code> directory:</p> <pre><code>environment:\n  MAX_METRIC_CARDINALITY: 2000\n  REDIS_TTL: 3600\n</code></pre>"},{"location":"CARDINALITY-PROTECTION/#monitoring","title":"Monitoring","text":"<p>The UI displays cardinality warnings when approaching the limit:</p> <ul> <li>Yellow Warning: 70-90% of limit reached</li> <li>Red Alert: 90%+ of limit reached</li> </ul> <p>Check current cardinality via the API:</p> <pre><code>curl http://localhost:5005/api/stats\n</code></pre> <p>Response:</p> <pre><code>{\n  \"traces\": 145,\n  \"logs\": 892,\n  \"metrics\": 850,\n  \"metrics_max\": 1000,\n  \"metrics_dropped\": 23\n}\n</code></pre>"},{"location":"ai-agent-deployment/","title":"ollyScale AI Agent Demo - Deployment Guide","text":""},{"location":"ai-agent-deployment/#overview","title":"Overview","text":"<p>The ollyScale AI Agent demo has been refactored to use Helm charts with OTel operator auto-instrumentation. This provides a cleaner, more maintainable deployment approach compared to the previous manual Kubernetes manifests.</p>"},{"location":"ai-agent-deployment/#what-changed","title":"What Changed","text":""},{"location":"ai-agent-deployment/#before-old-approach","title":"Before (Old Approach)","text":"<ul> <li>Manual Kubernetes YAML files in <code>scripts/k8s/ai-agent-demo/</code></li> <li>Manual OTLP configuration via environment variables</li> <li>Separate deployment and cleanup scripts</li> <li>No GitOps integration</li> </ul>"},{"location":"ai-agent-deployment/#after-new-approach","title":"After (New Approach)","text":"<ul> <li>Helm chart in <code>charts/ollyscale-ai-agent/</code></li> <li>OTel operator auto-instrumentation (zero-code)</li> <li>ArgoCD GitOps deployment</li> <li>Integrated with local registry and build scripts</li> </ul>"},{"location":"ai-agent-deployment/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           ollyscale-ai-agent                 \u2502\n\u2502                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  AI Agent    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502    Ollama      \u2502  \u2502\n\u2502  \u2502              \u2502      \u2502   (tinyllama)  \u2502  \u2502\n\u2502  \u2502 OTel Auto-   \u2502      \u2502                \u2502  \u2502\n\u2502  \u2502 Instrumented \u2502      \u2502  Persistent    \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502  Storage       \u2502  \u2502\n\u2502         \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502         \u2502                                   \u2502\n\u2502         \u2502 OTLP (auto-configured)            \u2502\n\u2502         \u25bc                                   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                      \u2502\n\u2502  \u2502 Agent Collector  \u2502                      \u2502\n\u2502  \u2502   (in ollyscale)  \u2502                      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"ai-agent-deployment/#deployment","title":"Deployment","text":""},{"location":"ai-agent-deployment/#automated-recommended","title":"Automated (Recommended)","text":"<p>The AI agent demo is automatically deployed via ArgoCD when you run:</p> <pre><code>make up\n</code></pre> <p>This will:</p> <ol> <li>Create the KIND cluster</li> <li>Install ArgoCD</li> <li>Deploy infrastructure (including OTel operator)</li> <li>Deploy ollyScale platform</li> <li>Deploy AI agent demo</li> </ol>"},{"location":"ai-agent-deployment/#manual-build-and-deploy","title":"Manual Build and Deploy","text":"<p>If you make changes to the AI agent code:</p> <pre><code># Build all images including AI agent\ncd charts\n./build-and-push-local.sh v2.2.0-my-changes\n\n# ArgoCD will automatically sync the new version\n# Or force sync:\ncd ../.kind\nterraform apply -auto-approve\n</code></pre>"},{"location":"ai-agent-deployment/#chart-only-update","title":"Chart-only Update","text":"<p>If you only changed the Helm chart (no code changes):</p> <pre><code>cd charts\nhelm package ollyscale-ai-agent\nhelm push ollyscale-ai-agent-0.1.0.tgz oci://registry.ollyscale.test:49443/ollyscale/charts --insecure-skip-tls-verify\n\n# Update ArgoCD\ncd ../.kind\nterraform apply -auto-approve\n</code></pre>"},{"location":"ai-agent-deployment/#configuration","title":"Configuration","text":""},{"location":"ai-agent-deployment/#default-values","title":"Default Values","text":"<p>See <code>charts/ollyscale-ai-agent/values.yaml</code> for all configuration options.</p> <p>Key defaults:</p> <ul> <li>Namespace: <code>ollyscale-ai-agent</code></li> <li>Ollama model: <code>tinyllama</code></li> <li>Ollama storage: 10Gi persistent volume</li> <li>Ollama resources: 2 CPU / 4Gi RAM (limit)</li> <li>Agent resources: 500m CPU / 512Mi RAM (limit)</li> <li>HTTPRoute: Disabled (no external access by default)</li> </ul>"},{"location":"ai-agent-deployment/#customization","title":"Customization","text":"<p>To override values, edit <code>.kind/modules/ollyscale/argocd-applications/observability/ollyscale-ai-agent.yaml</code>:</p> <pre><code>spec:\n  source:\n    helm:\n      valuesObject:\n        ollama:\n          model: llama2 # Use different model\n          resources:\n            limits:\n              cpu: \"4000m\"\n              memory: \"8Gi\"\n\n        agent:\n          httpRoute:\n            enabled: true # Enable external access\n</code></pre>"},{"location":"ai-agent-deployment/#verification","title":"Verification","text":""},{"location":"ai-agent-deployment/#check-deployment-status","title":"Check Deployment Status","text":"<pre><code># Check ArgoCD application\nkubectl get application -n argocd ollyscale-ai-agent\n\n# Check pods\nkubectl get pods -n ollyscale-ai-agent\n\n# Check services\nkubectl get svc -n ollyscale-ai-agent\n</code></pre>"},{"location":"ai-agent-deployment/#view-logs","title":"View Logs","text":"<pre><code># Agent logs (should show successful LLM calls)\nkubectl logs -n ollyscale-ai-agent deployment/ai-agent -f\n\n# Ollama logs (should show model downloads)\nkubectl logs -n ollyscale-ai-agent deployment/ollama -f\n</code></pre>"},{"location":"ai-agent-deployment/#verify-auto-instrumentation","title":"Verify Auto-Instrumentation","text":"<pre><code># Check if init container was injected\nkubectl get pod -n ollyscale-ai-agent -l app=ai-agent \\\n  -o jsonpath='{.items[0].spec.initContainers[*].name}'\n# Should output: opentelemetry-auto-instrumentation-python\n\n# Check OTLP endpoint configuration\nkubectl get pod -n ollyscale-ai-agent -l app=ai-agent \\\n  -o jsonpath='{.items[0].spec.containers[0].env[?(@.name==\"OTEL_EXPORTER_OTLP_ENDPOINT\")].value}'\n# Should output: http://agent-collector.ollyscale.svc.cluster.local:4318\n</code></pre>"},{"location":"ai-agent-deployment/#check-traces-in-ollyscale-ui","title":"Check Traces in ollyScale UI","text":"<ol> <li>Port-forward to ollyScale UI:</li> </ol> <pre><code>kubectl port-forward -n ollyscale svc/ollyscale-ui 5002:5002\n</code></pre> <ol> <li> <p>Open browser: http://localhost:5002</p> </li> <li> <p>Filter for service: <code>ai-agent</code></p> </li> <li> <p>Look for GenAI spans with attributes:</p> </li> <li><code>gen_ai.system</code>: \"ollama\"</li> <li><code>gen_ai.request.model</code>: \"tinyllama\"</li> <li><code>gen_ai.prompt</code>: User prompt text</li> <li><code>gen_ai.completion</code>: LLM response</li> </ol>"},{"location":"ai-agent-deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ai-agent-deployment/#ollama-not-ready","title":"Ollama Not Ready","text":"<p>If Ollama pod stays in \"NotReady\":</p> <pre><code># Check if model is downloading\nkubectl logs -n ollyscale-ai-agent deployment/ollama\n\n# Verify model is pulled\nkubectl exec -n ollyscale-ai-agent deployment/ollama -- ollama list\n</code></pre> <p>Model download can take several minutes depending on network speed.</p>"},{"location":"ai-agent-deployment/#agent-cant-connect-to-ollama","title":"Agent Can't Connect to Ollama","text":"<p>If agent logs show connection errors:</p> <pre><code># Verify Ollama service\nkubectl get svc -n ollyscale-ai-agent ollama\n\n# Check Ollama pod is ready\nkubectl get pods -n ollyscale-ai-agent -l app=ollama\n</code></pre>"},{"location":"ai-agent-deployment/#no-traces-appearing","title":"No Traces Appearing","text":"<p>If traces aren't showing in ollyScale:</p> <pre><code># Verify instrumentation annotation\nkubectl get deployment -n ollyscale-ai-agent ai-agent -o yaml | grep instrumentation\n\n# Check OTel operator logs\nkubectl logs -n opentelemetry-operator-system deployment/opentelemetry-operator\n\n# Verify Python instrumentation resource exists\nkubectl get instrumentation -n ollyscale python-instrumentation\n</code></pre>"},{"location":"ai-agent-deployment/#cleanup","title":"Cleanup","text":""},{"location":"ai-agent-deployment/#remove-ai-agent-demo","title":"Remove AI Agent Demo","text":"<pre><code># Delete via ArgoCD\nkubectl delete application -n argocd ollyscale-ai-agent\n\n# Or delete namespace directly\nkubectl delete namespace ollyscale-ai-agent\n</code></pre>"},{"location":"ai-agent-deployment/#full-cluster-teardown","title":"Full Cluster Teardown","text":"<pre><code>make down\n</code></pre>"},{"location":"ai-agent-deployment/#migration-notes","title":"Migration Notes","text":"<p>If you were using the old K8s deployment method:</p> <ol> <li>The old files in <code>scripts/k8s/ai-agent-demo/</code> have been removed</li> <li>Old deployment scripts no longer work</li> <li>Use <code>make up</code> to deploy the new Helm-based version</li> <li>Configuration is now in the Helm chart values</li> <li>Deployment is managed by ArgoCD</li> </ol>"},{"location":"ai-agent-deployment/#files-reference","title":"Files Reference","text":"<ul> <li>Helm Chart: <code>charts/ollyscale-ai-agent/</code></li> <li>ArgoCD App: <code>.kind/modules/ollyscale/argocd-applications/observability/ollyscale-ai-agent.yaml</code></li> <li>Terraform Config: <code>.kind/modules/ollyscale/variables.tf</code> (ai_agent_image, ai_agent_tag)</li> <li>Build Script: <code>charts/build-and-push-local.sh</code></li> <li>Application Code: <code>apps/ai-agent-demo/</code></li> </ul>"},{"location":"ai-agent/","title":"AI Agent Demo","text":"<p>This demo showcases GenAI observability with OpenTelemetry - automatic instrumentation of LLM calls using the <code>opentelemetry-instrumentation-ollama</code> package.</p>"},{"location":"ai-agent/#what-is-genai-observability","title":"What is GenAI Observability?","text":"<p>GenAI observability captures telemetry from LLM interactions including:</p> <ul> <li>Prompts and responses - Full text of user prompts and model outputs</li> <li>Token usage - Input and output token counts</li> <li>Latency - Response time for each LLM call</li> <li>Model information - Which model was used</li> </ul>"},{"location":"ai-agent/#quick-start","title":"Quick Start","text":""},{"location":"ai-agent/#docker","title":"Docker","text":"<pre><code># Start ollyScale core first\ncd docker\n./01-start-core.sh\n\n# Deploy AI agent demo (pulls pre-built images from Docker Hub)\ncd ../docker-ai-agent-demo\n./01-deploy-ai-demo.sh\n</code></pre> <p>This starts:</p> <ul> <li>Ollama with TinyLlama model for local LLM inference</li> <li>AI Agent with automatic GenAI span instrumentation</li> </ul> <p>Access the UI at <code>http://localhost:5005</code> and navigate to the AI Agents tab.</p> <p>For local development: Use <code>./01-deploy-ai-demo-local.sh</code> to build locally</p> <p>Stop: <code>./02-stop-ai-demo.sh</code></p> <p>Cleanup (remove volumes): <code>./03-cleanup-ai-demo.sh</code></p>"},{"location":"ai-agent/#how-it-works","title":"How It Works","text":"<p>The demo uses zero-code auto-instrumentation - no OpenTelemetry imports in the application code:</p> <pre><code># agent.py - NO OpenTelemetry imports needed!\nfrom ollama import Client\n\nclient = Client(host=\"http://ollama:11434\")\n\n# This call is AUTO-INSTRUMENTED\nresponse = client.chat(\n    model=\"tinyllama\",\n    messages=[{\"role\": \"user\", \"content\": \"What is OpenTelemetry?\"}]\n)\n</code></pre> <p>The magic happens in the Dockerfile:</p> <pre><code># Install auto-instrumentation packages\nRUN pip install opentelemetry-distro opentelemetry-instrumentation-ollama\n\n# Run with auto-instrumentation wrapper\nCMD [\"opentelemetry-instrument\", \"python\", \"-u\", \"agent.py\"]\n</code></pre>"},{"location":"ai-agent/#what-youll-see","title":"What You'll See","text":"<p>In the AI Agents tab:</p> Field Description Prompt The user's input to the LLM Response The model's output Tokens In Number of input tokens Tokens Out Number of output tokens Latency Response time in milliseconds Model Model name (e.g., <code>tinyllama</code>) <p>Click any row to expand the full span details in JSON format.</p>"},{"location":"ai-agent/#supported-llms","title":"Supported LLMs","text":"<p>The OpenTelemetry GenAI semantic conventions work with any instrumented LLM provider:</p> <ul> <li>Ollama - Local LLM inference (this demo)</li> <li>OpenAI - GPT models via <code>opentelemetry-instrumentation-openai</code></li> <li>Anthropic - Claude models</li> <li>Other providers - Any with OpenTelemetry instrumentation</li> </ul>"},{"location":"ai-agent/#configuration","title":"Configuration","text":"<p>The demo is configured via environment variables in <code>docker-compose.yml</code>:</p> <pre><code>ai-agent:\n  environment:\n    - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317\n    - OTEL_SERVICE_NAME=ai-agent-demo\n    - OLLAMA_HOST=http://ollama:11434\n</code></pre>"},{"location":"ai-agent/#troubleshooting","title":"Troubleshooting","text":"<p>No AI traces appearing?</p> <ul> <li>Ensure ollyScale core is running</li> <li>Check agent logs: <code>docker logs ai-agent-demo</code></li> <li>Verify Ollama is ready: <code>docker logs ollama</code></li> </ul> <p>Model download taking long?</p> <ul> <li>TinyLlama is ~600MB, first download may take a few minutes</li> <li>Check Ollama logs for download progress</li> </ul> <p>Agent errors?</p> <ul> <li>Ollama needs time to load the model after container starts</li> <li>The agent waits 10 seconds before first call</li> </ul>"},{"location":"api/","title":"REST API &amp; OpenAPI","text":"<p>OpenAPI documentation with interactive Swagger UI</p> <p>ollyScale provides a comprehensive REST API for programmatic access to all telemetry data in OpenTelemetry-native format.</p>"},{"location":"api/#interactive-api-documentation","title":"Interactive API Documentation","text":"<p>Access the auto-generated OpenAPI documentation:</p> <ul> <li>Swagger UI: <code>http://localhost:5005/docs</code> - Interactive API explorer</li> <li>ReDoc: <code>http://localhost:5005/redoc</code> - Alternative documentation</li> <li>OpenAPI Spec: <code>http://localhost:5005/openapi.json</code> - Machine-readable schema</li> </ul> <p>All APIs return OpenTelemetry-native JSON with:</p> <ul> <li>Resources: <code>service.name</code>, <code>host.name</code>, etc.</li> <li>Attributes: Metric labels and span attributes</li> <li>Full Context: Trace/span IDs, timestamps, status codes</li> </ul>"},{"location":"api/#api-endpoints-overview","title":"API Endpoints Overview","text":"<p>The REST API provides endpoints for:</p> Endpoint Method Description <code>/api/traces</code> GET List recent traces with filtering <code>/api/traces/{trace_id}</code> GET Get detailed trace with all spans <code>/api/spans</code> GET List recent spans with filtering <code>/api/logs</code> GET Retrieve logs with trace correlation <code>/api/metrics</code> GET Query time-series metrics <code>/api/service-map</code> GET Get service dependency graph <code>/api/service-catalog</code> GET List services with RED metrics <code>/api/stats</code> GET System stats and cardinality info <code>/admin/stats</code> GET Detailed admin statistics <code>/health</code> GET Health check endpoint <p>All endpoints return data in standard OpenTelemetry format, ensuring compatibility with OpenTelemetry tooling and standards.</p>"},{"location":"api/#common-api-workflows","title":"Common API Workflows","text":""},{"location":"api/#1-get-all-recent-traces","title":"1. Get All Recent Traces","text":"<p>Retrieve the last 50 traces:</p> cURL <p><code>bash     curl http://localhost:5005/api/traces?limit=50</code></p> Python <pre><code>import requests\n\n    response = requests.get('http://localhost:5005/api/traces', params={'limit': 50})\n    traces = response.json()\n\n    for trace in traces:\n        print(f\"Trace {trace['trace_id']}: {trace['service_name']} - {trace['name']}\")\n    ```\n\n=== \"JavaScript\"\n``javascript\n    fetch('http://localhost:5005/api/traces?limit=50')\n      .then(response =&gt; response.json())\n      .then(traces =&gt; {\n        traces.forEach(trace =&gt; {\n          console.log(`Trace ${trace.trace_id}: ${trace.service_name} - ${trace.name}`);\n        });\n      });\n    ``\n\n**Response Format:**\n\n```json\n[\n  {\n    \"trace_id\": \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\",\n    \"service_name\": \"demo-frontend\",\n    \"name\": \"GET /products\",\n    \"start_time\": 1701234567890000000,\n    \"duration_ms\": 125.4,\n    \"status_code\": 200,\n    \"method\": \"GET\",\n    \"route\": \"/products\",\n    \"span_count\": 5\n  }\n]\n</code></pre>"},{"location":"api/#2-get-detailed-trace-with-waterfall","title":"2. Get Detailed Trace with Waterfall","text":"<p>Retrieve a complete trace with all spans for waterfall visualization:</p> cURL <p><code>bash     curl http://localhost:5005/api/traces/a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6</code></p> Python <pre><code>import requests\n\n    trace_id = \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\"\n    response = requests.get(f'http://localhost:5005/api/traces/{trace_id}')\n    trace = response.json()\n\n    print(f\"Trace: {trace['name']}\")\n    print(f\"Total spans: {len(trace['spans'])}\")\n    print(f\"Duration: {trace['duration_ms']}ms\")\n\n    for span in trace['spans']:\n        indent = \"  \" * span.get('level', 0)\n        print(f\"{indent}{span['name']} ({span['duration_ms']}ms)\")\n    ```\n\n**Response Format:**\n\n```json\n{\n  \"trace_id\": \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\",\n  \"name\": \"GET /products\",\n  \"service_name\": \"demo-frontend\",\n  \"start_time\": 1701234567890000000,\n  \"duration_ms\": 125.4,\n  \"span_count\": 5,\n  \"spans\": [\n    {\n      \"span_id\": \"1234567890abcdef\",\n      \"parent_span_id\": null,\n      \"name\": \"GET /products\",\n      \"service_name\": \"demo-frontend\",\n      \"start_time\": 1701234567890000000,\n      \"duration_ms\": 125.4,\n      \"status\": { \"code\": 1 },\n      \"attributes\": {\n        \"http.method\": \"GET\",\n        \"http.route\": \"/products\",\n        \"http.status_code\": 200\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"api/#3-find-logs-for-a-specific-trace","title":"3. Find Logs for a Specific Trace","text":"<p>Correlate logs with a trace using trace_id:</p> cURL <p><code>bash     curl \"http://localhost:5005/api/logs?trace_id=a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\"</code></p> Python <pre><code>import requests\n\n    trace_id = \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\"\n    response = requests.get('http://localhost:5005/api/logs',\n                           params={'trace_id': trace_id})\n    logs = response.json()\n\n    for log in logs:\n        print(f\"[{log['severity']}] {log['body']}\")\n    ```\n\n**Response Format:**\n\n```json\n[\n  {\n    \"timestamp\": 1701234567890000000,\n    \"trace_id\": \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\",\n    \"span_id\": \"1234567890abcdef\",\n    \"severity\": \"INFO\",\n    \"body\": \"Processing product request\",\n    \"service_name\": \"demo-frontend\",\n    \"attributes\": {\n      \"user_id\": \"12345\"\n    }\n  }\n]\n</code></pre>"},{"location":"api/#4-query-metrics","title":"4. Query Metrics","text":"<p>Retrieve metrics data:</p> cURL <p><code>bash     curl http://localhost:5005/api/metrics</code></p> Python <pre><code>import requests\n\n    response = requests.get('http://localhost:5005/api/metrics')\n    metrics = response.json()\n\n    for metric in metrics:\n        print(f\"{metric['name']} ({metric['type']})\")\n        for series in metric.get('series', []):\n            labels = ', '.join(f\"{k}={v}\" for k, v in series.get('attributes', {}).items())\n            print(f\"  [{labels}] = {series.get('value', 'N/A')}\")\n    ```\n\n**Response Format:**\n\n```json\n[\n  {\n    \"name\": \"http.server.duration\",\n    \"type\": \"histogram\",\n    \"description\": \"HTTP request duration\",\n    \"unit\": \"ms\",\n    \"series\": [\n      {\n        \"attributes\": {\n          \"http.method\": \"GET\",\n          \"http.route\": \"/products\",\n          \"service.name\": \"demo-frontend\"\n        },\n        \"data_points\": [\n          {\n            \"timestamp\": 1701234567890000000,\n            \"count\": 42,\n            \"sum\": 5250.5,\n            \"bucket_counts\": [10, 20, 10, 2]\n          }\n        ]\n      }\n    ]\n  }\n]\n</code></pre>"},{"location":"api/#5-get-service-catalog-with-red-metrics","title":"5. Get Service Catalog with RED Metrics","text":"<p>List all services with Rate, Errors, and Duration metrics:</p> cURL <p><code>bash     curl http://localhost:5005/api/service-catalog</code></p> Python <pre><code>import requests\n\n    response = requests.get('http://localhost:5005/api/service-catalog')\n    services = response.json()\n\n    for service in services:\n        print(f\"\\n{service['service_name']}\")\n        print(f\"  Request Rate: {service.get('request_rate', 0):.2f} req/s\")\n        print(f\"  Error Rate: {service.get('error_rate', 0):.2f}%\")\n        print(f\"  P50 Latency: {service.get('p50_latency', 0):.2f}ms\")\n        print(f\"  P95 Latency: {service.get('p95_latency', 0):.2f}ms\")\n    ```\n\n**Response Format:**\n\n```json\n[\n  {\n    \"service_name\": \"demo-frontend\",\n    \"span_count\": 1523,\n    \"request_rate\": 12.5,\n    \"error_rate\": 2.3,\n    \"p50_latency\": 45.2,\n    \"p95_latency\": 125.7,\n    \"p99_latency\": 250.3,\n    \"first_seen\": 1701234567890000000,\n    \"last_seen\": 1701238167890000000\n  }\n]\n</code></pre>"},{"location":"api/#6-get-service-dependency-map","title":"6. Get Service Dependency Map","text":"<p>Retrieve the service dependency graph:</p> cURL <p><code>bash     curl http://localhost:5005/api/service-map</code></p> Python <pre><code>import requests\n\n    response = requests.get('http://localhost:5005/api/service-map')\n    graph = response.json()\n\n    print(\"Services:\", len(graph['nodes']))\n    for node in graph['nodes']:\n        print(f\"  - {node['service_name']} ({node['type']})\")\n\n    print(\"\\nConnections:\", len(graph['edges']))\n    for edge in graph['edges']:\n        print(f\"  {edge['source']} \u2192 {edge['target']} ({edge['call_count']} calls)\")\n    ```\n\n**Response Format:**\n\n```json\n{\n  \"nodes\": [\n    {\n      \"service_name\": \"demo-frontend\",\n      \"type\": \"server\",\n      \"span_count\": 1523\n    },\n    {\n      \"service_name\": \"demo-backend\",\n      \"type\": \"server\",\n      \"span_count\": 3046\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": \"demo-frontend\",\n      \"target\": \"demo-backend\",\n      \"call_count\": 1523\n    }\n  ]\n}\n</code></pre>"},{"location":"api/#7-check-system-statistics","title":"7. Check System Statistics","text":"<p>Get Redis memory usage and cardinality metrics:</p> cURL <p><code>bash     curl http://localhost:5005/api/stats</code></p> Python <pre><code>import requests\n\n    response = requests.get('http://localhost:5005/api/stats')\n    stats = response.json()\n\n    print(f\"Total Traces: {stats.get('total_traces', 0)}\")\n    print(f\"Total Spans: {stats.get('total_spans', 0)}\")\n    print(f\"Total Logs: {stats.get('total_logs', 0)}\")\n    print(f\"Total Metrics: {stats.get('total_metrics', 0)}\")\n    print(f\"Unique Metric Names: {stats.get('unique_metric_names', 0)}\")\n    print(f\"Redis Memory: {stats.get('redis_memory_mb', 0):.2f} MB\")\n    ```\n\n**Response Format:**\n\n```json\n{\n  \"total_traces\": 1523,\n  \"total_spans\": 7615,\n  \"total_logs\": 15230,\n  \"total_metrics\": 45,\n  \"unique_metric_names\": 12,\n  \"redis_memory_mb\": 45.7,\n  \"cardinality_limit\": 1000,\n  \"cardinality_usage_pct\": 1.2,\n  \"uptime_seconds\": 3600\n}\n</code></pre>"},{"location":"api/#advanced-filtering","title":"Advanced Filtering","text":""},{"location":"api/#filter-spans-by-service","title":"Filter Spans by Service","text":"<pre><code>curl \"http://localhost:5005/api/spans?service=demo-frontend&amp;limit=100\"\n</code></pre>"},{"location":"api/#filter-logs-by-severity","title":"Filter Logs by Severity","text":"<pre><code>curl \"http://localhost:5005/api/logs?severity=ERROR&amp;limit=50\"\n</code></pre>"},{"location":"api/#time-based-queries","title":"Time-based Queries","text":"<p>All endpoints support <code>start_time</code> and <code>end_time</code> parameters (Unix nanoseconds):</p> <pre><code># Get traces from the last hour\nSTART=$(date -u -d '1 hour ago' +%s)000000000\nEND=$(date -u +%s)000000000\ncurl \"http://localhost:5005/api/traces?start_time=$START&amp;end_time=$END\"\n</code></pre>"},{"location":"api/#client-generation","title":"Client Generation","text":"<p>Generate API clients in any language using the OpenAPI spec:</p> <pre><code># Download OpenAPI spec\ncurl http://localhost:5005/openapi.json &gt; ollyscale-openapi.json\n\n# Generate Python client\nopenapi-generator-cli generate \\\n  -i ollyscale-openapi.json \\\n  -g python \\\n  -o ./ollyscale-python-client\n\n# Generate Go client\nopenapi-generator-cli generate \\\n  -i ollyscale-openapi.json \\\n  -g go \\\n  -o ./ollyscale-go-client\n\n# Generate TypeScript client\nopenapi-generator-cli generate \\\n  -i ollyscale-openapi.json \\\n  -g typescript-fetch \\\n  -o ./ollyscale-ts-client\n</code></pre>"},{"location":"api/#rate-limits","title":"Rate Limits","text":"<p>ollyScale is designed for local development and has no rate limits. However:</p> <ul> <li>Memory limits apply based on Redis configuration (default: 256MB)</li> <li>Cardinality protection limits unique metric names to 1000 (configurable)</li> <li>TTL: All data expires after 30 minutes</li> </ul>"},{"location":"api/#authentication","title":"Authentication","text":"<p>ollyScale is designed for local development and does not include authentication. Do not expose ollyScale to the internet without adding authentication via a reverse proxy.</p>"},{"location":"api/#need-help","title":"Need Help?","text":"<ul> <li>View interactive examples in Swagger UI</li> <li>Open an issue on GitHub</li> <li>Read the technical architecture</li> </ul>"},{"location":"build-system/","title":"ollyScale Build System - Deliverables &amp; Dependencies","text":"<p>Document Version: 2.1 Last Updated: January 15, 2026 Status: Active</p>"},{"location":"build-system/#overview","title":"Overview","text":"<p>This document maps ollyScale's deliverable artifacts and traces their build dependencies. We work backwards from what we ship to understand what needs to be built and in what order.</p> <p>Purpose:</p> <ul> <li>Identify what artifacts we publish to registries</li> <li>Understand rebuild requirements when source changes</li> <li>Optimize build order and minimize rebuilds</li> <li>Document the delivery pipeline</li> </ul>"},{"location":"build-system/#our-deliverables","title":"Our Deliverables","text":"<p>ollyScale produces two types of artifacts that are published to OCI registries:</p>"},{"location":"build-system/#1-oci-container-images-4-images","title":"1. OCI Container Images (4 images)","text":""},{"location":"build-system/#2-helm-charts-2-charts","title":"2. Helm Charts (2 charts)","text":"<p>All other scripts, configurations, and source code exist solely to produce these deliverables.</p>"},{"location":"build-system/#deliverable-1-oci-container-images","title":"Deliverable #1: OCI Container Images","text":"<p>We publish 4 container images to OCI-compatible registries:</p> <pre><code>DELIVERABLE: OCI Container Images\n\u251c\u2500 ollyscale/ollyscale          (Python backend - FastAPI + OTLP receiver)\n\u251c\u2500 ollyscale/webui             (Static frontend - nginx + TypeScript/Vite)\n\u251c\u2500 ollyscale/opamp-server      (OpAMP configuration server)\n\u2514\u2500 ollyscale/demo              (Demo application)\n</code></pre>"},{"location":"build-system/#image-ollyscaleollyscale","title":"Image: <code>ollyscale/ollyscale</code>","text":"<p>What it is: Python backend application that runs as either API server or OTLP receiver</p> <p>Published to:</p> <ul> <li>Production: <code>ghcr.io/ryanfaircloth/ollyscale/ollyscale:VERSION</code></li> <li>Local dev: <code>registry.ollyscale.test:49443/ollyscale/ollyscale:VERSION</code></li> </ul> <p>Run modes (controlled by <code>MODE</code> env var):</p> <ul> <li><code>MODE=ui</code> \u2192 FastAPI REST API server on port 5002</li> <li><code>MODE=receiver</code> \u2192 gRPC OTLP receiver on port 4343</li> </ul> <p>Built from:</p> <ul> <li>Dockerfile: <code>apps/frontend/Dockerfile</code></li> <li>Base image: <code>python:3.14-slim</code></li> <li>Source files (all from <code>apps/frontend/</code>):</li> <li><code>main.py</code> - Entry point that selects mode</li> <li><code>models.py</code> - Pydantic data models</li> <li><code>requirements.txt</code> - Python dependencies</li> <li><code>app/</code> - FastAPI routers and API endpoints</li> <li><code>receiver/</code> - gRPC receiver for receiver mode</li> <li><code>common/</code> - Shared utilities (storage, OTLP parsing) docker buildx build --platform linux/amd64,linux/arm64 \\   -f apps/frontend/Dockerfile \\   -t ghcr.io/ryanfaircloth/ollyscale/ollyscale:v2.1.8 \\   --push apps/frontend/</li> </ul>"},{"location":"build-system/#local-dev-single-arch","title":"Local dev (single-arch)","text":"<p>podman build -f apps/frontend/Dockerfile \\   -t registry.ollyscale.test:49443/ollyscale/ollyscale:v2.1.x-feature \\   apps/frontend/ <pre><code>**Rebuild triggers**:\n\n- Change to any file in `apps/frontend/`\n- Change to `requirements.txt`\n- Change to Dockerfile\n\n---\n\n### Image: `ollyscale/webui`\n\n**What it is**: Static web frontend served by nginx\n\n**Published to**:\n\n- Production: `ghcr.io/ryanfaircloth/ollyscale/webui:VERSION`\n- Local dev: `registry.ollyscale.test:49443/ollyscale/webui:VERSION`\n\n**Functionality**:\n\n- TypeScript/Vite SPA with modern build tooling\n- Served by nginx on port 80\n- Connects to backend API at `/api/*`\n\n**Built from**:\n\n- **Dockerfile**: `apps/ollyscale-ui/Dockerfile`\n- **Base image**: `node:20-alpine` (build), `nginx:alpine` (runtime)\n- **Source files** (from `apps/ollyscale-ui/`):\n  - `src/` - TypeScript modules and main entry point\n  - `src/modules/` - API client, traces, serviceMap, metrics, etc.\n  - `index.html` - HTML template\n  - `vite.config.js` - Vite build configuration\n  - `package.json` - NPM dependencies\n  - `nginx/` - nginx configuration\n\n**Build command**:\n\n```bash\n# Production (multi-arch)\ndocker buildx build --platform linux/amd64,linux/arm64 \\\n  -f apps/ollyscale-ui/Dockerfile \\\n  -t ghcr.io/ryanfaircloth/ollyscale/webui:v2.1.8 \\\n  --push apps/ollyscale-ui/\n\n# Local dev (single-arch)\npodman build -f apps/ollyscale-ui/Dockerfile \\\n  -t registry.ollyscale.test:49443/ollyscale/webui:v2.1.x-feature \\\n  apps/ollyscale-ui/\n</code></pre></p> <p>Rebuild triggers:</p> <ul> <li>Change to any file in <code>apps/ollyscale-ui/src/</code></li> <li>Change to <code>package.json</code></li> <li>Change to Dockerfile</li> <li>Change to nginx configuration</li> </ul>"},{"location":"build-system/#image-ollyscaleopamp-server","title":"Image: <code>ollyscale/opamp-server</code>","text":"<p>What it is: OpAMP server for remote OpenTelemetry Collector configuration</p> <p>Published to:</p> <ul> <li>Production: <code>ghcr.io/ryanfaircloth/ollyscale/opamp-server:VERSION</code></li> <li>Local dev: <code>registry.ollyscale.test:49443/ollyscale/opamp-server:VERSION</code></li> </ul> <p>Functionality:</p> <ul> <li>OpAMP protocol endpoint on port 4320</li> <li>REST API for config management on port 4321</li> </ul> <p>Built from:</p> <ul> <li>Dockerfile: <code>apps/opamp-server/Dockerfile</code></li> <li>Base image: <code>golang:1.25-alpine</code> (build), <code>scratch</code> (runtime)</li> <li>Source files (from <code>apps/opamp-server/</code>):</li> <li><code>main.go</code> - OpAMP server implementation</li> <li><code>go.mod</code> - Go module definition</li> </ul> <p>Build command:</p> <pre><code># Production (multi-arch)\ndocker buildx build --platform linux/amd64,linux/arm64 \\\n  -f apps/opamp-server/Dockerfile \\\n  -t ghcr.io/ryanfaircloth/ollyscale/opamp-server:v2.1.8 \\\n  --push apps/opamp-server/\n\n# Local dev (single-arch)\npodman build -f apps/opamp-server/Dockerfile \\\n  -t registry.ollyscale.test:49443/ollyscale/opamp-server:v2.1.x-feature \\\n  apps/opamp-server/\n</code></pre> <p>Rebuild triggers:</p> <ul> <li>Change to <code>main.go</code></li> <li>Change to <code>go.mod</code></li> <li>Change to Dockerfile</li> </ul>"},{"location":"build-system/#image-ollyscaledemo","title":"Image: <code>ollyscale/demo</code>","text":"<p>What it is: Unified demo application with frontend and backend</p> <p>Published to:</p> <ul> <li>Production: <code>ghcr.io/ryanfaircloth/ollyscale/demo:VERSION</code></li> <li>Local dev: <code>registry.ollyscale.test:49443/ollyscale/demo:VERSION</code></li> </ul> <p>Functionality:</p> <ul> <li>Generates sample traces, logs, and metrics</li> <li>Can run as frontend or backend via <code>MODE</code> env var</li> </ul> <p>Built from:</p> <ul> <li>Dockerfile: <code>apps/demo/Dockerfile</code></li> <li>Base image: <code>python:3.12-slim</code></li> <li>Source files (from <code>apps/demo/</code>):</li> <li><code>frontend.py</code> - Demo frontend service</li> <li><code>backend.py</code> - Demo backend service</li> <li><code>requirements.txt</code> - Python dependencies</li> </ul> <p>Build command:</p> <pre><code># Production (multi-arch)\ndocker buildx build --platform linux/amd64,linux/arm64 \\\n  -f apps/demo/Dockerfile \\\n  -t ghcr.io/ryanfaircloth/ollyscale/demo:v2.1.8 \\\n  --push apps/demo/\n\n# Local dev (single-arch)\npodman build -f apps/demo/Dockerfile \\\n  -t registry.ollyscale.test:49443/ollyscale/demo:v2.1.x-feature \\\n  apps/demo/\n</code></pre> <p>Rebuild triggers:</p> <ul> <li>Change to <code>frontend.py</code> or <code>backend.py</code></li> <li>Change to <code>requirements.txt</code></li> <li>Change to Dockerfile</li> </ul>"},{"location":"build-system/#deliverable-2-helm-charts","title":"Deliverable #2: Helm Charts","text":"<p>We publish 2 Helm charts to OCI registries:</p> <pre><code>DELIVERABLE: Helm Charts (OCI format)\n\u251c\u2500 ollyscale              (Main platform chart)\n\u2514\u2500 ollyscale-demos        (Demo applications chart)\n</code></pre>"},{"location":"build-system/#chart-ollyscale","title":"Chart: <code>ollyscale</code>","text":"<p>What it is: Complete ollyScale platform deployment</p> <p>Published to:</p> <ul> <li>Production: <code>oci://ghcr.io/ryanfaircloth/ollyscale/charts/ollyscale:VERSION</code></li> <li>Local dev: <code>oci://registry.ollyscale.test:49443/ollyscale/charts/ollyscale:VERSION</code></li> </ul> <p>Contains:</p> <ul> <li>Backend API deployment (uses <code>ollyscale/ollyscale:VERSION</code> with <code>MODE=ui</code>)</li> <li>Frontend webui deployment (uses <code>ollyscale/webui:VERSION</code>)</li> <li>OTLP receiver deployment (uses <code>ollyscale/ollyscale:VERSION</code> with <code>MODE=receiver</code>)</li> <li>OpAMP server deployment (uses <code>ollyscale/opamp-server:VERSION</code>)</li> <li>Redis StatefulSet</li> <li>OTel Collector DaemonSet (optional)</li> <li>OpenTelemetry Instrumentation CRs (optional)</li> <li>Services, ConfigMaps, Secrets</li> </ul> <p>Built from:</p> <ul> <li>Chart location: <code>charts/ollyscale/</code></li> <li>Chart.yaml: Metadata and version</li> <li>values.yaml: Default configuration</li> <li>templates/: Kubernetes manifests</li> <li><code>webui-deployment.yaml</code></li> <li><code>frontend-deployment.yaml</code></li> <li><code>otlp-receiver-deployment.yaml</code></li> <li><code>opamp-server-deployment.yaml</code></li> <li><code>redis-statefulset.yaml</code></li> <li><code>otelcol-daemonset.yaml</code></li> <li><code>instrumentation.yaml</code></li> <li><code>service-*.yaml</code></li> <li><code>configmap-*.yaml</code></li> </ul> <p>Dependencies:</p> <ul> <li>Requires container images to exist:</li> <li><code>ollyscale/webui:VERSION</code></li> <li><code>ollyscale/ollyscale:VERSION</code></li> <li><code>ollyscale/opamp-server:VERSION</code></li> <li>May reference external charts (Redis Operator, OTel Operator)</li> </ul> <p>Build command:</p> <pre><code># Package chart\nhelm package charts/ollyscale/ -d charts/\n\n# Push to OCI registry\nhelm push charts/ollyscale-0.1.1-v2.1.x-feature.tgz \\\n  oci://registry.ollyscale.test:49443/ollyscale/charts\n</code></pre> <p>Version format:</p> <ul> <li>Production: <code>0.1.1</code> (semantic version)</li> <li>Local dev: <code>0.1.1-v2.1.x-description</code></li> </ul> <p>Rebuild triggers:</p> <ul> <li>Change to any file in <code>charts/ollyscale/templates/</code></li> <li>Change to <code>values.yaml</code></li> <li>Change to <code>Chart.yaml</code></li> <li>New container image version (update <code>appVersion</code>)</li> </ul>"},{"location":"build-system/#chart-ollyscale-demos","title":"Chart: <code>ollyscale-demos</code>","text":"<p>What it is: Demo applications for ollyScale</p> <p>Published to:</p> <ul> <li>Production: <code>oci://ghcr.io/ryanfaircloth/ollyscale/charts/ollyscale-demos:VERSION</code></li> <li>Local dev: <code>oci://registry.ollyscale.test:49443/ollyscale/charts/ollyscale-demos:VERSION</code></li> </ul> <p>Contains:</p> <ul> <li>Demo frontend deployment (uses <code>ollyscale/demo:VERSION</code> with <code>MODE=frontend</code>)</li> <li>Demo backend deployment (uses <code>ollyscale/demo:VERSION</code> with <code>MODE=backend</code>)</li> <li>Traffic generator (optional)</li> <li>Services to wire frontend \u2192 backend</li> </ul> <p>Built from:</p> <ul> <li>Chart location: <code>charts/ollyscale-demos/</code></li> <li>Chart.yaml: Metadata and version</li> <li>values.yaml: Demo configuration</li> <li>templates/: Kubernetes manifests</li> <li><code>deployment-frontend.yaml</code></li> <li><code>deployment-backend.yaml</code></li> <li><code>job-traffic-generator.yaml</code></li> <li><code>service-*.yaml</code></li> </ul> <p>Dependencies:</p> <ul> <li>Requires container image: <code>ollyscale/demo:VERSION</code></li> <li>Expects <code>ollyscale</code> chart to be deployed (for OTLP endpoint)</li> </ul> <p>Build command:</p> <pre><code># Package chart\nhelm package charts/ollyscale-demos/ -d charts/\n\n# Push to OCI registry\nhelm push charts/ollyscale-demos-0.1.5.tgz \\\n  oci://registry.ollyscale.test:49443/ollyscale/charts\n</code></pre> <p>Rebuild triggers:</p> <ul> <li>Change to any file in <code>charts/ollyscale-demos/templates/</code></li> <li>Change to <code>values.yaml</code></li> <li>Change to <code>Chart.yaml</code></li> <li>New demo image version</li> </ul>"},{"location":"build-system/#dependency-graph-backwards-from-deliverables","title":"Dependency Graph (Backwards from Deliverables)","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    DELIVERABLE ARTIFACTS                        \u2502\n\u2502                  (What we publish to registries)                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                \u2502                               \u2502\n                \u25bc                               \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502  OCI IMAGES (3)   \u2502           \u2502  HELM CHARTS (2)  \u2502\n     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n     \u2502 1. ollyscale       \u2502           \u2502 1. ollyscale       \u2502\n     \u2502 2. opamp-server   \u2502           \u2502 2. ollyscale-demos \u2502\n     \u2502 3. demo           \u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n               \u2502                               \u2502\n               \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502          \u2502\n               \u25bc          \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502              BUILD INPUTS                           \u2502\n     \u2502          (Dockerfiles + Source Code)                \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n               \u2502                                     \u2502\n               \u25bc                                     \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502  apps/ollyscale/     \u2502            \u2502 Chart.yaml          \u2502\n     \u2502  Dockerfile         \u2502            \u2502 values.yaml         \u2502\n     \u2502       +             \u2502            \u2502 templates/*.yaml    \u2502\n     \u2502  \u251c\u2500 main.py         \u2502            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n     \u2502  \u251c\u2500 models.py       \u2502                     \u25b2\n     \u2502  \u251c\u2500 requirements.txt\u2502                     \u2502\n     \u2502  \u251c\u2500 app/            \u2502                     \u2502\n     \u2502  \u251c\u2500 receiver/       \u2502            (Helm charts reference)\n     \u2502  \u251c\u2500 common/         \u2502            (image tags from above)\n     \u2502  \u251c\u2500 static/         \u2502\n     \u2502  \u2514\u2500 templates/      \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key relationships:</p> <ol> <li>Helm charts depend on container images - Charts reference image tags</li> <li>Container images depend on source code - Dockerfiles copy source into images</li> <li>Charts must be rebuilt when image versions change (update <code>appVersion</code>)</li> </ol>"},{"location":"build-system/#build-scripts-by-target","title":"Build Scripts by Target","text":""},{"location":"build-system/#production-builds-ghcr-multi-platform","title":"Production Builds \u2192 GHCR (Multi-platform)","text":"<p>Location: <code>scripts/build/</code> Registry: <code>ghcr.io/ryanfaircloth/ollyscale/*</code> Platforms: <code>linux/amd64</code>, <code>linux/arm64</code></p> Script Builds Command <code>02-build-core.sh VERSION</code> ollyscale:VERSIONwebui:VERSIONopamp-server:VERSION Uses Docker BuildxMulti-platform <code>02-build-demo.sh VERSION</code> demo:VERSION Uses Docker BuildxMulti-platform <code>02-build-all.sh VERSION</code> All images above Calls other scripts <code>03-push-core.sh VERSION</code> N/A - pushes only Pushes to GHCR <code>03-push-demo.sh VERSION</code> N/A - pushes only Pushes to GHCR <code>03-push-all.sh VERSION</code> N/A - pushes only Pushes all to GHCR <p>Typical workflow:</p> <pre><code>cd scripts/build\n./02-build-all.sh v2.1.8    # Build all images (multi-arch)\n./03-push-all.sh v2.1.8     # Push to ghcr.io\n</code></pre> <p>Chart publishing (separate process):</p> <pre><code>cd charts\n./package.sh                # Package charts\n./push-oci.sh               # Push to public registry\n</code></pre>"},{"location":"build-system/#local-development-builds-local-registry-single-platform","title":"Local Development Builds \u2192 Local Registry (Single-platform)","text":"<p>Location: <code>charts/</code> Registry: <code>registry.ollyscale.test:49443/ollyscale/*</code> Platforms: Native only (faster builds)</p> Script Builds Description <code>build-and-push-local.sh VERSION</code> 1. All 4 container images2. ollyscale Helm chart Complete pipeline:- Build images- Push to local registry- Update Chart.yaml- Package chart- Push chart to OCI <p>What it does:</p> <pre><code># Example: ./build-and-push-local.sh v2.1.x-tail-sampling\n\n# Step 1: Build images\npodman build -f apps/ollyscale/Dockerfile \\\n  -t registry.ollyscale.test:49443/ollyscale/ollyscale:v2.1.x-tail-sampling \\\n  apps/ollyscale/\n\npodman build -f apps/ollyscale-ui/Dockerfile \\\n  -t registry.ollyscale.test:49443/ollyscale/webui:v2.1.x-tail-sampling \\\n  apps/ollyscale-ui/\n\npodman build -f apps/opamp-server/Dockerfile \\\n  -t registry.ollyscale.test:49443/ollyscale/opamp-server:v2.1.x-tail-sampling \\\n  apps/opamp-server/\n\npodman build -f apps/demo/Dockerfile \\\n  -t registry.ollyscale.test:49443/ollyscale/demo:v2.1.x-tail-sampling \\\n  apps/demo/\n\n# Step 2: Push images to local registry (external endpoint)\npodman push --tls-verify=false registry.ollyscale.test:49443/ollyscale/ollyscale:v2.1.x-tail-sampling\n# ... (webui, opamp-server, demo)\n\n# Step 3: Update Chart.yaml version\nsed -i '' \"s/^version: .*/version: 0.1.1-v2.1.x-tail-sampling/\" charts/ollyscale/Chart.yaml\n\n# Step 4: Generate values-local-dev.yaml with INTERNAL registry\ncat &gt; values-local-dev.yaml &lt;&lt;EOF\nui:\n  image:\n    repository: docker-registry.registry.svc.cluster.local:5000/ollyscale/ollyscale\n    tag: v2.1.x-tail-sampling\nwebui:\n  image:\n    repository: docker-registry.registry.svc.cluster.local:5000/ollyscale/webui\n    tag: v2.1.x-tail-sampling\n# ... etc\nEOF\n\n# Step 5: Package chart\nhelm package charts/ollyscale/ -d charts/\n\n# Step 6: Push chart to OCI registry\nhelm push charts/ollyscale-0.1.1-v2.1.x-tail-sampling.tgz \\\n  oci://registry.ollyscale.test:49443/ollyscale/charts\n</code></pre> <p>Deploy to cluster:</p> <pre><code># Update ArgoCD Application to use new chart version\ncd .kind\nterraform apply -replace='kubectl_manifest.observability_applications[\"observability/ollyscale.yaml\"]' -auto-approve\n</code></pre>"},{"location":"build-system/#deprecated-scripts-do-not-use","title":"Deprecated Scripts (Do Not Use)","text":"<p>These scripts are no longer maintained and should not be used:</p> Status Replacement Notes \ud83d\uddd1\ufe0f Removed <code>charts/build-and-push-local.sh</code> Old k8s/ scripts deleted \ud83d\uddd1\ufe0f Removed ArgoCD + Terraform pattern GitOps deployment recommended"},{"location":"build-system/#registry-endpoints-critical","title":"Registry Endpoints (Critical!)","text":"<p>ollyScale uses different registry endpoints for build/push vs runtime deployment. This is a common source of confusion.</p>"},{"location":"build-system/#the-same-physical-registry-different-access-points","title":"The Same Physical Registry, Different Access Points","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          LOCAL KUBERNETES CLUSTER REGISTRY                   \u2502\n\u2502                  (One registry pod)                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502                  \u2502                  \u2502\n        \u25bc                  \u25bc                  \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  EXTERNAL    \u2502  \u2502   NODEPORT   \u2502  \u2502   INTERNAL   \u2502\n\u2502  (Build)     \u2502  \u2502  (Alt Build) \u2502  \u2502  (Runtime)   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 registry.    \u2502  \u2502 localhost:   \u2502  \u2502 docker-      \u2502\n\u2502 ollyscale.    \u2502  \u2502 30500        \u2502  \u2502 registry.    \u2502\n\u2502 test:49443   \u2502  \u2502              \u2502  \u2502 registry.svc \u2502\n\u2502              \u2502  \u2502              \u2502  \u2502 .cluster.    \u2502\n\u2502              \u2502  \u2502              \u2502  \u2502 local:5000   \u2502\n\u2502              \u2502  \u2502              \u2502  \u2502              \u2502\n\u2502 Use: podman  \u2502  \u2502 Use: podman  \u2502  \u2502 Use: K8s pod \u2502\n\u2502 push from    \u2502  \u2502 push from    \u2502  \u2502 image pull   \u2502\n\u2502 desktop      \u2502  \u2502 desktop      \u2502  \u2502              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Rules:</p> <ol> <li>Build scripts push to <code>registry.ollyscale.test:49443</code> (external endpoint)</li> <li>Helm values reference <code>docker-registry.registry.svc.cluster.local:5000</code> (internal endpoint)</li> <li>Never use <code>registry.ollyscale.test:49443</code> in pod image specs - cluster can't resolve it!</li> </ol> <p>Example (<code>values-local-dev.yaml</code>):</p> <pre><code># \u2705 CORRECT - internal endpoint for cluster\nui:\n  image:\n    repository: docker-registry.registry.svc.cluster.local:5000/ollyscale/ollyscale\n    tag: v2.1.x-feature\n\n# \u274c WRONG - external endpoint, pods can't pull\nui:\n  image:\n    repository: registry.ollyscale.test:49443/ollyscale/ollyscale\n    tag: v2.1.x-feature\n</code></pre>"},{"location":"build-system/#production-registry-ghcr","title":"Production Registry (GHCR)","text":"<p>Endpoint: <code>ghcr.io/ryanfaircloth/ollyscale/*</code> Access: Public (read), authenticated (write) Usage: Production releases only</p>"},{"location":"build-system/#complete-dependency-matrix","title":"Complete Dependency Matrix","text":""},{"location":"build-system/#what-triggers-what","title":"What Triggers What?","text":"Change Type Requires Rebuild Deployment Action Source Code <code>apps/ollyscale/app/</code> <code>ollyscale:VERSION</code> image Rebuild image \u2192 Update chart \u2192 Deploy <code>apps/ollyscale/receiver/</code> <code>ollyscale:VERSION</code> image Rebuild image \u2192 Update chart \u2192 Deploy <code>apps/ollyscale/requirements.txt</code> <code>ollyscale:VERSION</code> image Rebuild image \u2192 Update chart \u2192 Deploy <code>apps/ollyscale-ui/src/</code> <code>webui:VERSION</code> image Rebuild image \u2192 Update chart \u2192 Deploy <code>apps/ollyscale-ui/package.json</code> <code>webui:VERSION</code> image Rebuild image \u2192 Update chart \u2192 Deploy <code>apps/opamp-server/</code> <code>opamp-server:VERSION</code> image Rebuild image \u2192 Update chart \u2192 Deploy <code>apps/demo/frontend.py</code> <code>demo:VERSION</code> image Rebuild image \u2192 Update demos chart \u2192 Deploy <code>apps/demo/backend.py</code> <code>demo:VERSION</code> image Rebuild image \u2192 Update demos chart \u2192 Deploy Dockerfiles <code>apps/ollyscale/Dockerfile</code> <code>ollyscale:VERSION</code> image Rebuild image \u2192 Update chart \u2192 Deploy <code>apps/ollyscale-ui/Dockerfile</code> <code>webui:VERSION</code> image Rebuild image \u2192 Update chart \u2192 Deploy <code>apps/opamp-server/Dockerfile</code> <code>opamp-server:VERSION</code> image Rebuild image \u2192 Update chart \u2192 Deploy <code>apps/demo/Dockerfile</code> <code>demo:VERSION</code> image Rebuild image \u2192 Update demos chart \u2192 Deploy Helm Charts <code>charts/ollyscale/templates/</code> <code>ollyscale</code> chart Package chart \u2192 Deploy <code>charts/ollyscale/values.yaml</code> <code>ollyscale</code> chart Package chart \u2192 Deploy <code>charts/ollyscale/Chart.yaml</code> <code>ollyscale</code> chart Package chart \u2192 Deploy <code>charts/ollyscale-demos/templates/</code> <code>ollyscale-demos</code> chart Package chart \u2192 Deploy Deployment <code>.kind/modules/main/argocd-applications/</code> None (config only) <code>terraform apply</code>"},{"location":"build-system/#build-optimization-opportunities","title":"Build Optimization Opportunities","text":""},{"location":"build-system/#current-issues","title":"Current Issues","text":"<ol> <li>No shared base image: <code>ollyscale</code> image rebuilds all Python deps on every build</li> <li>No build caching: Production builds use <code>--no-cache</code> flag</li> <li>Dockerfile redundancy: Demo images duplicate patterns from main image</li> <li>Version coordination: Chart version and image tags updated manually</li> </ol>"},{"location":"build-system/#improvement-proposals","title":"Improvement Proposals","text":""},{"location":"build-system/#1-create-python-base-image","title":"1. Create Python Base Image","text":"<pre><code># NEW: Dockerfile.ollyscale-python-base\nFROM python:3.14-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n# Tag: ollyscale/python-base:v2.1\n</code></pre> <p>Then update <code>Dockerfile.ollyscale</code>:</p> <pre><code>FROM ollyscale/python-base:v2.1  # \u2190 Use base\n# Only copy app code, deps already installed\n</code></pre> <p>Benefit: Faster builds when only code changes (not deps)</p>"},{"location":"build-system/#2-enable-build-caching-for-local-builds","title":"2. Enable Build Caching for Local Builds","text":"<pre><code># Remove --no-cache flag from build scripts\ndocker buildx build ... # (no --no-cache)\n</code></pre> <p>Benefit: 5-10x faster iteration on small changes</p>"},{"location":"build-system/#3-single-multi-stage-dockerfile","title":"3. Single Multi-stage Dockerfile","text":"<p>Combine all demo variants into one Dockerfile:</p> <pre><code># Dockerfile.demo - unified\nFROM python:3.12-slim AS base\n# ... shared setup ...\n\nFROM base AS frontend\nCOPY frontend.py .\nCMD [\"python\", \"frontend.py\"]\n\nFROM base AS backend\nCOPY backend.py .\nCMD [\"python\", \"backend.py\"]\n</code></pre> <p>Benefit: DRY, easier to maintain</p>"},{"location":"build-system/#4-automated-version-management","title":"4. Automated Version Management","text":"<pre><code># Extract version from git tag or commit\nVERSION=$(git describe --tags --always)\n# Auto-update Chart.yaml appVersion\nyq eval \".appVersion = \\\"$VERSION\\\"\" -i Chart.yaml\n</code></pre> <p>Benefit: Eliminate manual version sync errors</p>"},{"location":"build-system/#quick-reference-build-commands","title":"Quick Reference: Build Commands","text":""},{"location":"build-system/#local-development-common-case","title":"Local Development (Common Case)","text":"<pre><code># From repo root\ncd charts\n./build-and-push-local.sh v2.1.x-description\n\n# Deploy to cluster\ncd ../.kind\nterraform apply -replace='kubectl_manifest.observability_applications[\"observability/ollyscale.yaml\"]' -auto-approve\n\n# Verify deployment\nkubectl get pods -n ollyscale\nkubectl logs -n ollyscale deployment/ollyscale-ui -f\n\n# Clear cache if needed\nkubectl exec -n ollyscale ollyscale-redis-0 -- redis-cli FLUSHDB\n</code></pre>"},{"location":"build-system/#production-release","title":"Production Release","text":"<pre><code># Build and push images\ncd scripts/build\n./02-build-all.sh v2.1.8\n./03-push-all.sh v2.1.8\n\n# Package and publish charts\ncd ../../helm\n./package.sh\n./push-oci.sh\n\n# Tag release\ngit tag -a v2.1.8 -m \"Release v2.1.8\"\ngit push origin v2.1.8\n</code></pre>"},{"location":"build-system/#quick-image-only-rebuild-local","title":"Quick Image-Only Rebuild (Local)","text":"<pre><code># When you ONLY changed ollyscale source code\ncd /repo/root/docker\npodman build -f dockerfiles/Dockerfile.ollyscale \\\n  -t registry.ollyscale.test:49443/ollyscale/ollyscale:v2.1.x-hotfix .\npodman push --tls-verify=false \\\n  registry.ollyscale.test:49443/ollyscale/ollyscale:v2.1.x-hotfix\n\n# Update just the image tag in ArgoCD\nkubectl -n argocd patch application ollyscale --type merge \\\n  -p '{\"spec\":{\"source\":{\"helm\":{\"valuesObject\":{\"ui\":{\"image\":{\"tag\":\"v2.1.x-hotfix\"}}}}}}}'\n</code></pre>"},{"location":"build-system/#summary-the-build-system-in-one-diagram","title":"Summary: The Build System in One Diagram","text":"<pre><code>SOURCE CODE                    BUILD ARTIFACTS                 REGISTRIES\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500              \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndocker/apps/\nollyscale/          \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   ollyscale/ollyscale    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 ghcr.io/...\n  (Python)                     (OCI image)                    (production)\n\ndocker/apps/                                                  registry.\nollyscale-opamp-    \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   ollyscale/opamp-      \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 ollyscale.test\nserver/ (Go)                   server (OCI image)             (local dev)\n\napps/demo/       \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   ollyscale/demo        \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\n  (Python)                     (OCI image)\n                                      \u2502\n                                      \u2502 (referenced by)\n                                      \u25bc\ncharts/ollyscale/     \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   ollyscale             \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 OCI registry\n  (K8s manifests)              (Helm chart .tgz)              /charts\n\ncharts/ollyscale-     \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6   ollyscale-demos       \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6 OCI registry\ndemos/                         (Helm chart .tgz)              /charts\n\n\nTOOLS USED:\n- podman/docker buildx  \u2192  Build OCI images\n- helm package          \u2192  Package charts\n- helm push            \u2192  Publish to OCI registry\n- ArgoCD + Terraform   \u2192  Deploy to Kubernetes\n</code></pre> <p>Key Insight: We ship 5 artifacts total:</p> <ul> <li>3 container images (ollyscale, opamp-server, demo)</li> <li>2 Helm charts (ollyscale, ollyscale-demos)</li> </ul>"},{"location":"build-system/#document-maintenance","title":"Document Maintenance","text":"<p>This document should be updated when:</p> <ul> <li>New container images are added</li> <li>New Helm charts are created</li> <li>Build scripts are added/removed/renamed</li> <li>Deployment workflow changes</li> <li>Registry endpoints change</li> <li>New dependencies are introduced</li> </ul> <p>Owner: Infrastructure Team Review Frequency: On major releases or build system changes</p>"},{"location":"database-connection-pooling/","title":"Database Connection Pooling and Star Schema ETL Pattern","text":""},{"location":"database-connection-pooling/#overview","title":"Overview","text":"<p>ollyScale uses SQLAlchemy with asyncpg for high-performance async PostgreSQL access with a star schema dimensional model. This document covers connection pooling, transaction management, and the ETL pattern used to avoid deadlocks in multi-process environments.</p>"},{"location":"database-connection-pooling/#star-schema-architecture","title":"Star Schema Architecture","text":"<p>ollyScale implements a dimensional model with:</p> <ul> <li>Dimension tables: <code>namespace_dim</code>, <code>service_dim</code>, <code>operation_dim</code>, <code>resource_dim</code></li> <li>Fact tables: <code>spans_fact</code>, <code>logs_fact</code>, <code>metrics_fact</code></li> </ul>"},{"location":"database-connection-pooling/#multi-process-transaction-strategy","title":"Multi-Process Transaction Strategy","text":"<p>In multi-process/multi-threaded environments, traditional single-transaction upserts cause deadlocks when multiple processes try to insert the same dimensions simultaneously.</p> <p>Solution - Two-Phase ETL Pattern:</p> <p>Phase 1 - Dimension Upserts (Independent, Auto-Commit):</p> <ul> <li>Each dimension upsert commits immediately after INSERT</li> <li>Idempotent operations (<code>INSERT ON CONFLICT DO UPDATE</code>)</li> <li>No locks held between dimension operations</li> <li>If batch fails and retries, dimensions already exist (safe)</li> <li>Works from outer edge inward: namespace \u2192 service \u2192 operation</li> </ul> <p>Phase 2 - Fact Inserts (Single Transaction):</p> <ul> <li>After ALL dimensions committed, insert facts in one transaction</li> <li>Facts reference dimensions via foreign keys</li> <li>Dimensions guaranteed to exist from Phase 1</li> </ul> <p>Benefits:</p> <ul> <li>No deadlocks from concurrent dimension upserts</li> <li>Idempotent retries (dimensions survive batch failures)</li> <li>Better concurrency (no long-held locks)</li> <li>Connection pool efficiency maintained</li> </ul>"},{"location":"database-connection-pooling/#testing-implications","title":"Testing Implications","text":"<p>Important: Dimension auto-commits affect test isolation.</p> <p>Problem: Standard test patterns using transaction rollback don't work:</p> <ul> <li>Dimensions commit immediately (not rolled back)</li> <li>Test data leaks between tests</li> <li>Non-deterministic dimension IDs</li> </ul> <p>Solution: <code>clean_database</code> fixture must truncate dimensions AND facts:</p> <pre><code>@pytest_asyncio.fixture\nasync def clean_database(postgres_session):\n    \"\"\"Clean dimensions AND facts for test isolation.\"\"\"\n    await postgres_session.execute(\n        \"TRUNCATE TABLE spans_fact, logs_fact, metrics_fact, \"\n        \"namespace_dim, service_dim, operation_dim, resource_dim \"\n        \"CASCADE\"\n    )\n    await postgres_session.commit()\n</code></pre> <p>See <code>tests/conftest.py</code> for complete fixture implementation.</p>"},{"location":"database-connection-pooling/#connection-pool-configuration","title":"Connection Pool Configuration","text":"<p>Connection pool settings in <code>app/db/session.py</code>:</p> <pre><code>self.engine = create_async_engine(\n    url,\n    pool_size=10,              # Base pool size (concurrent connections)\n    max_overflow=20,           # Additional connections beyond pool_size\n    pool_pre_ping=True,        # Verify connections before use\n    pool_recycle=3600,         # Recycle connections after 1 hour\n)\n</code></pre> <p>Environment variables:</p> <ul> <li><code>DATABASE_POOL_SIZE</code> - Base pool size (default: 10)</li> <li><code>DATABASE_MAX_OVERFLOW</code> - Additional overflow connections (default: 20)</li> </ul>"},{"location":"database-connection-pooling/#session-management-pattern","title":"Session Management Pattern","text":""},{"location":"database-connection-pooling/#correct-star-schema-two-phase-pattern","title":"\u2705 CORRECT: Star Schema Two-Phase Pattern","text":"<pre><code>async def store_traces(self, resource_spans: list[dict]) -&gt; int:\n    \"\"\"Store OTLP traces - star schema ETL pattern.\"\"\"\n    async with AsyncSession(self.engine) as session:\n        # Phase 1: Dimension upserts (each commits immediately)\n        namespace_id = await self._upsert_namespace(session, namespace)  # COMMITS\n        service_id = await self._upsert_service(session, name, namespace_id)  # COMMITS\n        operation_id = await self._upsert_operation(session, service_id, op_name, kind)  # COMMITS\n\n        # Phase 2: Fact inserts (single transaction after dimensions exist)\n        session.add_all(spans_to_insert)\n        await session.commit()  # Final commit for facts\n</code></pre> <p>Benefits:</p> <ul> <li>\u2705 No deadlocks in multi-process environments</li> <li>\u2705 Idempotent dimension upserts survive retries</li> <li>\u2705 Clean traces with minimal transaction control spans</li> <li>\u2705 Better performance (fewer round-trips)</li> </ul>"},{"location":"database-connection-pooling/#incorrect-multiple-commits-per-request","title":"\u274c INCORRECT: Multiple Commits Per Request","text":"<pre><code>async def _upsert_service(self, session: AsyncSession, name: str) -&gt; int:\n    \"\"\"BAD: Committing inside helper method.\"\"\"\n    stmt = insert(ServiceDim).values(name=name)\n    await session.execute(stmt)\n    await session.commit()  # \u274c WRONG - breaks connection reuse\n    return service_id\n</code></pre> <p>Problems:</p> <ul> <li>\u274c Connection returned to pool after each commit</li> <li>\u274c Next operation may get different connection</li> <li>\u274c Multiple BEGIN/COMMIT/ROLLBACK cycles</li> <li>\u274c Auto-instrumentation creates span for each transaction control</li> <li>\u274c Trace noise: 20+ BEGIN/COMMIT/ROLLBACK spans instead of 1-2</li> </ul>"},{"location":"database-connection-pooling/#trace-impact","title":"Trace Impact","text":""},{"location":"database-connection-pooling/#before-fix-multiple-commits","title":"Before Fix (Multiple Commits)","text":"<pre><code>gRPC Export (215ms)\n\u251c\u2500 connect #1 (5ms)\n\u2502  \u251c\u2500 BEGIN\n\u2502  \u251c\u2500 SELECT tenant_dim\n\u2502  \u2514\u2500 COMMIT\n\u251c\u2500 connect #2 (5ms)\n\u2502  \u251c\u2500 BEGIN\n\u2502  \u251c\u2500 INSERT namespace_dim\n\u2502  \u251c\u2500 COMMIT\n\u2502  \u251c\u2500 BEGIN\n\u2502  \u251c\u2500 SELECT namespace_dim\n\u2502  \u2514\u2500 ROLLBACK\n\u251c\u2500 connect #3 (5ms)\n\u2502  \u251c\u2500 BEGIN\n\u2502  \u251c\u2500 INSERT service_dim\n\u2502  \u251c\u2500 COMMIT\n\u2502  \u251c\u2500 BEGIN\n\u2502  \u2514\u2500 ROLLBACK\n... (8 total connections) ...\n\u2514\u2500 connect #8 (5ms)\n   \u251c\u2500 BEGIN\n   \u251c\u2500 INSERT logs_fact (24 records)\n   \u2514\u2500 COMMIT\n</code></pre> <p>Issues:</p> <ul> <li>8 separate connections in single request</li> <li>20+ transaction control spans (BEGIN, COMMIT, ROLLBACK)</li> <li>Connection pool thrashing</li> <li>Excessive trace noise</li> </ul>"},{"location":"database-connection-pooling/#after-fix-single-transaction","title":"After Fix (Single Transaction)","text":"<pre><code>gRPC Export (180ms) - 35ms faster!\n\u2514\u2500 connect (5ms)\n   \u251c\u2500 BEGIN\n   \u251c\u2500 SELECT tenant_dim\n   \u251c\u2500 INSERT namespace_dim\n   \u251c\u2500 SELECT namespace_dim\n   \u251c\u2500 INSERT service_dim\n   \u251c\u2500 SELECT service_dim\n   \u251c\u2500 INSERT operation_dim\n   \u251c\u2500 SELECT operation_dim\n   \u251c\u2500 INSERT logs_fact (24 records)\n   \u2514\u2500 COMMIT\n</code></pre> <p>Benefits:</p> <ul> <li>\u2705 1 connection per request</li> <li>\u2705 2 transaction control spans (BEGIN, COMMIT)</li> <li>\u2705 Clean, readable trace</li> <li>\u2705 17% performance improvement</li> </ul>"},{"location":"database-connection-pooling/#opentelemetry-auto-instrumentation","title":"OpenTelemetry Auto-Instrumentation","text":""},{"location":"database-connection-pooling/#sqlalchemy-instrumentation-configuration","title":"SQLAlchemy Instrumentation Configuration","text":"<p>In <code>charts/ollyscale/values.yaml</code>:</p> <pre><code>instrumentation:\n  python:\n    env:\n      # Disable SQL comment injection (reduces overhead)\n      - name: OTEL_INSTRUMENTATION_SQLALCHEMY_ENABLE_COMMENTER\n        value: 'false'\n\n      # Instrument all engines automatically\n      - name: OTEL_PYTHON_SQLALCHEMY_INSTRUMENT_ALL_ENGINES\n        value: 'true'\n</code></pre>"},{"location":"database-connection-pooling/#trace-span-reduction-strategies","title":"Trace Span Reduction Strategies","text":"<ol> <li>Remove intermediate commits - Covered above \u2705</li> <li>Use batch inserts - <code>session.add_all()</code> instead of loops</li> <li>Disable SQL commenter - Reduces per-query overhead</li> <li>Connection pooling - Reuse connections across requests</li> </ol>"},{"location":"database-connection-pooling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"database-connection-pooling/#symptom-many-connect-spans-per-request","title":"Symptom: Many \"connect\" spans per request","text":"<p>Diagnosis: Check for intermediate <code>session.commit()</code> calls in helper methods.</p> <p>Fix: Remove commits from <code>_upsert_*</code> methods, let caller handle single commit.</p>"},{"location":"database-connection-pooling/#symptom-begincommitrollback-span-noise","title":"Symptom: BEGIN/COMMIT/ROLLBACK span noise","text":"<p>Diagnosis: Multiple transactions per request due to intermediate commits.</p> <p>Fix: Same as above - one transaction per request.</p>"},{"location":"database-connection-pooling/#symptom-connection-already-being-used-errors","title":"Symptom: \"connection already being used\" errors","text":"<p>Diagnosis: Session being used from multiple coroutines (not thread-safe).</p> <p>Fix: Each async task needs its own session. Don't share sessions across <code>asyncio.create_task()</code> calls.</p>"},{"location":"database-connection-pooling/#symptom-connection-pool-exhausted-warnings","title":"Symptom: \"connection pool exhausted\" warnings","text":"<p>Diagnosis: Connection leaks or insufficient pool size.</p> <p>Fix:</p> <ol> <li>Verify all sessions use <code>async with</code> context manager</li> <li>Increase <code>DATABASE_POOL_SIZE</code> or <code>DATABASE_MAX_OVERFLOW</code></li> <li>Check for long-running transactions blocking pool</li> </ol>"},{"location":"database-connection-pooling/#references","title":"References","text":"<ul> <li>SQLAlchemy Async Documentation</li> <li>asyncpg Pool Configuration</li> <li>OpenTelemetry SQLAlchemy Instrumentation</li> </ul>"},{"location":"database-connection-pooling/#related","title":"Related","text":"<ul> <li>Technical Architecture</li> <li>PostgreSQL Infrastructure</li> <li>Pre-commit Configuration</li> </ul>"},{"location":"ebpf/","title":"eBPF Zero-Code Tracing Demo","text":"<p>This demo showcases OpenTelemetry eBPF Instrumentation (OBI) - automatic trace capture at the kernel level without any code changes to your application.</p>"},{"location":"ebpf/#platform-requirements","title":"\u26a0\ufe0f Platform Requirements","text":"<p>Important: eBPF instrumentation requires a real Linux kernel and will NOT work in virtualized or emulated environments:</p> <p>\u2705 Supported Platforms:</p> <ul> <li>Native Linux Kubernetes clusters (GKE, EKS, AKS, bare-metal)</li> <li>Linux kernel 5.11 or newer</li> <li>Real hardware or KVM-based VMs</li> </ul> <p>\u274c Unsupported Platforms (will fail with memlock errors):</p> <ul> <li>KIND clusters on macOS/Windows</li> <li>Docker Desktop on macOS/Windows</li> <li>Podman on macOS</li> <li>Minikube with Docker driver on macOS/Windows</li> <li>Any Docker-in-Docker or VM-based Kubernetes</li> </ul> <p>For Local Development: Use the OpenTelemetry Operator auto-instrumentation feature instead, which works on all platforms by injecting SDK instrumentation at runtime.</p>"},{"location":"ebpf/#what-is-obi","title":"What is OBI?","text":"<p>OpenTelemetry eBPF Instrumentation (formerly Grafana Beyla) uses eBPF to automatically capture HTTP/gRPC traces by inspecting system calls and network traffic at the Linux kernel level.</p> <p>Key Benefits:</p> <ul> <li>Zero code changes - no SDK, no agent, no restarts</li> <li>Language agnostic - works with Python, Go, Java, Node.js, Rust, C, PHP, and more</li> <li>Protocol-level instrumentation - captures any HTTP/gRPC traffic</li> </ul>"},{"location":"ebpf/#quick-start","title":"Quick Start","text":""},{"location":"ebpf/#docker","title":"Docker","text":"<pre><code># Start ollyScale core first\ncd docker\n./01-start-core.sh\n\n# Deploy eBPF demo (pulls pre-built images from Docker Hub)\ncd ../apps/demo-ebpf\n./01-deploy-ebpf-demo.sh\n</code></pre> <p>Access the UI at <code>http://localhost:5005</code></p>"},{"location":"ebpf/#kubernetes","title":"Kubernetes","text":"<pre><code># Start ollyScale core first\nminikube start\ncd charts\n./install.sh\n\n# Deploy eBPF demo (pulls pre-built images from Docker Hub)\ncd ../k8s-demo-ebpf\n./02-deploy.sh\n</code></pre> <p>Run <code>minikube tunnel</code> in a separate terminal, then access the UI at <code>http://localhost:5002</code></p>"},{"location":"ebpf/#docker-hub-images","title":"Docker Hub Images","text":"<p>The eBPF demo uses pre-built images from Docker Hub:</p> <ul> <li><code>ghcr.io/ryanfaircloth/ebpf-frontend:latest</code> - Frontend with OTel SDK for metrics/logs</li> <li><code>ghcr.io/ryanfaircloth/ebpf-backend:latest</code> - Pure Flask backend (no OTel SDK)</li> </ul> <p>For local development, use the build scripts in each demo folder.</p>"},{"location":"ebpf/#whats-different-from-sdk-instrumentation","title":"What's Different from SDK Instrumentation?","text":""},{"location":"ebpf/#traces","title":"Traces","text":"Aspect SDK Instrumentation eBPF Instrumentation Span names Route names (<code>GET /hello</code>, <code>POST /api/users</code>) Generic (<code>in queue</code>, <code>CONNECT</code>, <code>HTTP</code>) Span attributes Rich application context (user IDs, request params) Network-level only (host, port, method) Distributed tracing Full trace propagation via headers Limited - eBPF sees connections, not header context Setup Code changes or auto-instrumentation wrapper Deploy eBPF agent alongside app <p>Example - SDK trace:</p> <pre><code>{\n  \"trace_id\": \"abc123...\",\n  \"span_name\": \"GET /process-order\",\n  \"attributes\": {\n    \"http.method\": \"GET\",\n    \"http.route\": \"/process-order\",\n    \"http.status_code\": 200,\n    \"order.id\": \"12345\",\n    \"customer.id\": \"678\"\n  }\n}\n</code></pre> <p>Example - eBPF trace:</p> <pre><code>{\n  \"trace_id\": \"def456...\",\n  \"span_name\": \"in queue\",\n  \"attributes\": {\n    \"net.host.name\": \"ebpf-frontend\",\n    \"net.host.port\": 5000\n  }\n}\n</code></pre>"},{"location":"ebpf/#logs","title":"Logs","text":"<p>With SDK instrumentation, logs include trace context (<code>trace_id</code>, <code>span_id</code>) for correlation:</p> <pre><code>{\n  \"message\": \"Processing order 12345\",\n  \"trace_id\": \"abc123...\",\n  \"span_id\": \"xyz789...\"\n}\n</code></pre> <p>With eBPF instrumentation, logs have no trace context because there's no tracing SDK to inject it:</p> <pre><code>{\n  \"message\": \"Processing order 12345\",\n  \"trace_id\": \"\",\n  \"span_id\": \"\"\n}\n</code></pre> <p>This is expected behavior - eBPF operates at the kernel level and cannot inject context into application logs.</p>"},{"location":"ebpf/#metrics","title":"Metrics","text":"<p>Metrics work the same way in both approaches - they're exported via the OTel SDK regardless of how traces are captured.</p>"},{"location":"ebpf/#components","title":"Components","text":""},{"location":"ebpf/#frontend-ebpf-frontend","title":"Frontend (<code>ebpf-frontend</code>)","text":"<ul> <li>Flask application with auto-traffic generation</li> <li>Metrics: Exported via OTel SDK (<code>OTLPMetricExporter</code>)</li> <li>Logs: Exported via OTel SDK (<code>OTLPLogExporter</code>)</li> <li>Traces: None from SDK - captured by eBPF agent</li> </ul>"},{"location":"ebpf/#backend-ebpf-backend","title":"Backend (<code>ebpf-backend</code>)","text":"<ul> <li>Pure Flask application - no OTel SDK at all</li> <li>Demonstrates that eBPF can trace completely uninstrumented apps</li> <li>Logs go to stdout only (not exported to OTel)</li> </ul>"},{"location":"ebpf/#ebpf-agent-otel-ebpf-agent","title":"eBPF Agent (<code>otel-ebpf-agent</code>)","text":"<ul> <li>Runs with <code>privileged: true</code> and <code>pid: host</code></li> <li>Monitors port 5000 for HTTP traffic</li> <li>Sends traces to OTel Collector</li> </ul>"},{"location":"ebpf/#when-to-use-ebpf-vs-sdk","title":"When to Use eBPF vs SDK","text":"<p>Use eBPF when:</p> <ul> <li>You can't modify application code (legacy apps, third-party binaries)</li> <li>You want basic HTTP observability with zero effort</li> <li>You're instrumenting many polyglot services quickly</li> </ul> <p>Use SDK when:</p> <ul> <li>You need rich application-level context in traces</li> <li>You need log-trace correlation</li> <li>You need custom spans for business logic</li> <li>You need full distributed tracing with context propagation</li> </ul> <p>Hybrid approach (this demo):</p> <ul> <li>Use eBPF for traces (zero-code)</li> <li>Use SDK for metrics and logs (richer data)</li> </ul>"},{"location":"ebpf/#configuration","title":"Configuration","text":""},{"location":"ebpf/#docker_1","title":"Docker","text":"<p>The eBPF agent is configured via environment variables in <code>docker-compose.yml</code>:</p> <pre><code>otel-ebpf-agent:\n  image: docker.io/otel/ebpf-instrument:main\n  privileged: true\n  pid: host\n  environment:\n    - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317\n    - OTEL_EBPF_OPEN_PORT=5000\n  volumes:\n    - /sys/kernel/debug:/sys/kernel/debug:rw\n</code></pre>"},{"location":"ebpf/#kubernetes_1","title":"Kubernetes","text":""},{"location":"ebpf/#using-the-ollyscale-helm-chart-recommended","title":"Using the ollyScale Helm Chart (Recommended)","text":"<p>The easiest way to deploy the eBPF agent in Kubernetes is using the ollyScale Helm chart:</p> <pre><code># Install ollyScale with eBPF agent enabled\nhelm install ollyscale ./charts/ollyscale \\\n  --namespace ollyscale \\\n  --create-namespace \\\n  --set ebpfAgent.enabled=true \\\n  --set ebpfAgent.config.openPorts=\"5000,8080,3000\"\n</code></pre> <p>Or create a custom <code>values.yaml</code>:</p> <pre><code># values-with-ebpf.yaml\nebpfAgent:\n  enabled: true\n  config:\n    # Ports to instrument (comma-separated)\n    openPorts: \"5000,8080,3000\"\n    # Service name prefix\n    serviceName: \"my-app\"\n    # Collector endpoint\n    otlpEndpoint: \"http://gateway-collector.ollyscale.svc.cluster.local:4317\"\n  resources:\n    limits:\n      memory: 512Mi\n    requests:\n      cpu: 100m\n      memory: 256Mi\n</code></pre> <p>Then install:</p> <pre><code>helm install ollyscale ./charts/ollyscale \\\n  --namespace ollyscale \\\n  --create-namespace \\\n  --values values-with-ebpf.yaml\n</code></pre> <p>The Helm chart automatically creates:</p> <ul> <li>DaemonSet for eBPF agent (one pod per node)</li> <li>ServiceAccount with proper permissions</li> <li>ClusterRole/ClusterRoleBinding for node and pod access</li> <li>Integration with ollyScale's gateway collector</li> </ul>"},{"location":"ebpf/#manual-kubernetes-deployment","title":"Manual Kubernetes Deployment","text":"<p>In Kubernetes, the eBPF agent runs as a DaemonSet to instrument all pods on each node:</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: otel-ebpf-agent\nspec:\n  template:\n    spec:\n      hostPID: true\n      containers:\n        - name: ebpf-agent\n          image: docker.io/otel/ebpf-instrument:main\n          securityContext:\n            privileged: true\n          env:\n            - name: OTEL_EXPORTER_OTLP_ENDPOINT\n              value: \"http://otel-collector:4317\"\n            - name: OTEL_EBPF_OPEN_PORT\n              value: \"5000\"\n          volumeMounts:\n            - name: sys-kernel-debug\n              mountPath: /sys/kernel/debug\n      volumes:\n        - name: sys-kernel-debug\n          hostPath:\n            path: /sys/kernel/debug\n</code></pre>"},{"location":"ebpf/#key-settings","title":"Key Settings","text":"<ul> <li><code>OTEL_EBPF_OPEN_PORT</code>: Which port to monitor (5000 = Flask default)</li> <li><code>privileged: true</code>: Required for eBPF kernel access</li> <li><code>hostPID: true</code> / <code>pid: host</code>: Required to see processes in other containers/pods</li> </ul>"},{"location":"ebpf/#troubleshooting","title":"Troubleshooting","text":"<p>No traces appearing?</p> <ul> <li>Ensure ollyScale core is running (<code>docker ps | grep otel-collector</code>)</li> <li>Check eBPF agent logs: <code>docker logs otel-ebpf-instrumentation</code></li> <li>Verify the agent can access <code>/sys/kernel/debug</code></li> </ul> <p>Traces have wrong service name?</p> <ul> <li>OBI discovers service names from process info</li> <li>Set <code>OTEL_EBPF_SERVICE_NAME</code> for explicit naming</li> </ul> <p>eBPF agent won't start?</p> <ul> <li>Requires Linux kernel 4.4+ with eBPF support</li> <li>On macOS, runs inside Docker's Linux VM (should work)</li> <li>Check Docker has sufficient privileges</li> </ul>"},{"location":"ebpf/#learn-more","title":"Learn More","text":"<ul> <li>OpenTelemetry eBPF Instrumentation Docs</li> <li>OBI GitHub Repository</li> <li>OBI Docker Setup Guide</li> </ul>"},{"location":"kubernetes/","title":"Kubernetes Deployment","text":"<p>Deploy ollyScale on Kubernetes (Minikube) for local development!</p> <p>Service map showing microservices running on Kubernetes</p> <p>All examples are launched from the repo - clone it first or download the current GitHub release archive:</p> <pre><code>git clone https://github.com/ryanfaircloth/ollyscale\n</code></pre>"},{"location":"kubernetes/#prerequisites","title":"Prerequisites","text":"<ul> <li>Minikube</li> <li>kubectl</li> </ul>"},{"location":"kubernetes/#1-deploy-ollyscale-core","title":"1. Deploy ollyScale Core","text":"<ol> <li> <p>Start Minikube:</p> <pre><code>minikube start\n</code></pre> </li> <li> <p>Deploy ollyScale:</p> <pre><code>Deploy using Helm (images will be pulled from Docker Hub automatically):\n\n```bash\ncd charts\n./install.sh\n```\n\n!!! note \"Local Development Build (Optional)\"\nTo build and deploy custom images for local development:\n`bash\n</code></pre> <p>cd charts ./build-and-push-local.sh  ` <li> <p>Access the UI:</p> <p>To access the ollyScale UI (Service Type: LoadBalancer) on macOS with Minikube, you need to use <code>minikube tunnel</code>.</p> <p>Open a new terminal window and run:</p> <pre><code>minikube tunnel\n</code></pre> <p>You may be asked for your password. Keep this terminal open.</p> <p>Now you can access the ollyScale UI at: http://localhost:5002</p> <p>OpenTelemetry Collector + OpAMP Config Page: Navigate to the \"OpenTelemetry Collector + OpAMP Config\" tab in the UI to view and manage collector configurations remotely. See the OpAMP Configuration section below for setup instructions.</p> </li> <li> <p>Send Telemetry from Host Apps:</p> <p>To send telemetry from applications running on your host machine (outside Kubernetes), use <code>kubectl port-forward</code> to expose the OTel Collector ports:</p> <p>Open a new terminal window and run:</p> <pre><code>kubectl port-forward service/otel-collector 4317:4317 4318:4318\n</code></pre> <p>Keep this terminal open. Now point your application's OpenTelemetry exporter to: - gRPC: <code>http://localhost:4317</code> - HTTP: <code>http://localhost:4318</code></p> <p>Example environment variables:</p> <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318\n</code></pre> <p>For apps running inside the Kubernetes cluster: Use the Kubernetes service name: - gRPC: <code>http://otel-collector:4317</code> - HTTP: <code>http://otel-collector:4318</code></p> </li> <li> <p>Clean Up:</p> <p>Uninstall ollyScale using Helm:</p> <pre><code>helm uninstall ollyscale -n ollyscale\nkubectl delete namespace ollyscale\n</code></pre> <p>Shut down Minikube:</p> <pre><code>minikube stop\n</code></pre> <p>Minikube may be more stable if you delete it:</p> <pre><code>minikube delete\n</code></pre> </li>"},{"location":"kubernetes/#2-demo-applications-optional","title":"2. Demo Applications (Optional)","text":"<p>To see ollyScale in action with instrumented microservices:</p> <pre><code>cd k8s-demo\n./02-deploy.sh\n</code></pre> <p>The deploy script pulls demo images from Docker Hub by default. For local development, you can build images locally when prompted.</p> <p>To clean up the demo:</p> <pre><code>./03-cleanup.sh\n</code></pre> <p>The demo includes two microservices that automatically generate traffic, showcasing distributed tracing across service boundaries.</p>"},{"location":"kubernetes/#3-opentelemetry-demo-20-services-optional","title":"3. OpenTelemetry Demo (~20 Services - Optional)","text":"<p>To deploy the full OpenTelemetry Demo with ~20 microservices:</p> <p>Prerequisites:</p> <ul> <li>ollyScale must be deployed first (see Setup above)</li> <li>Helm installed</li> <li>Sufficient cluster resources (demo is resource-intensive)</li> </ul> <p>Deploy:</p> <pre><code>cd k8s-otel-demo\n./01-deploy-otel-demo-helm.sh\n</code></pre> <p>This deploys all OpenTelemetry Demo services configured to send telemetry to ollyScale's collector via HTTP on port 4318. Built-in observability tools (Jaeger, Grafana, Prometheus) are disabled.</p> <p>Cleanup:</p> <pre><code>cd k8s-otel-demo\n./02-cleanup-otel-demo-helm.sh\n</code></pre> <p>This removes the OpenTelemetry Demo but leaves ollyScale running.</p>"},{"location":"kubernetes/#4-ollyscale-core-only-deployment-use-your-own-kubernetes-opentelemetry-collector","title":"4. ollyScale Core-Only Deployment: Use Your Own Kubernetes OpenTelemetry Collector","text":"<p>To deploy ollyScale without the bundled OTel Collector (e.g., if you have an existing collector daemonset). Includes OpAMP server for optional remote collector configuration management:</p> <ol> <li>Deploy Core:</li> </ol> <pre><code>cd k8s-core-only\n./01-deploy.sh\n</code></pre> <ol> <li> <p>Access UI:    Run <code>minikube tunnel</code> and access <code>http://localhost:5002</code>.</p> </li> <li> <p>Cleanup:</p> </li> </ol> <pre><code>./02-cleanup.sh\n</code></pre>"},{"location":"kubernetes/#use-ollyscale-with-any-opentelemetry-collector","title":"Use ollyScale with Any OpenTelemetry Collector","text":"<p>Swap out the included Otel Collector for any distro of Otel Collector.</p> <p>Point your OpenTelemetry exporters to ollyscale-otlp-receiver:4343: i.e.</p> <pre><code>exporters:\n  debug:\n    verbosity: detailed\n\n  otlp:\n    endpoint: \"ollyscale-otlp-receiver:4343\"\n    tls:\n      insecure: true\n\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug, otlp, spanmetrics]\n\n    metrics:\n      receivers: [otlp, spanmetrics]\n      processors: [batch]\n      exporters: [debug, otlp]\n\n    logs:\n      receivers: [otlp]\n      processors: [batch]\n      exporters: [debug, otlp]\n</code></pre> <p>The Otel Collector will forward everything to ollyScale's OTLP receiver, which process telemetry and stores it in Redis in OTEL format for the backend and UI to access.</p>"},{"location":"kubernetes/#opamp-configuration-optional","title":"OpAMP Configuration (Optional)","text":"<p>The OpenTelemetry Collector + OpAMP Config page in the ollyScale UI allows you to view and manage collector configurations remotely. To enable this feature, add the OpAMP extension to your collector config:</p> <pre><code>extensions:\n  opamp:\n    server:\n      ws:\n        endpoint: ws://ollyscale-opamp-server:4320/v1/opamp\n\nservice:\n  extensions: [opamp]\n</code></pre> <p>The default configuration template (included as a ConfigMap in <code>k8s-core-only/ollyscale-opamp-server.yaml</code>) shows a complete example with OTLP receivers, OpAMP extension, batch processing, and spanmetrics connector. Your collector will connect to the OpAMP server and receive configuration updates through the ollyScale UI.</p>"},{"location":"kubernetes/#building-images","title":"Building Images","text":"<p>By default, deployment scripts pull pre-built images from GitHub Container Registry (GHCR). For building images locally (Minikube) or publishing to GHCR, see build/README.md.</p>"},{"location":"licensing/","title":"Licensing","text":"<p>ollyScale is released under the BSD 3-Clause License. The full license text is available in the LICENSE file. This license permits free use, modification, and redistribution for individuals, researchers, and small organizations.</p> <p>This project is forked from the BSD 3-Clause license and maintained by Ryan Faircloth. Eventually I will update the logo and naming to avoid confusion with the original project.</p>"},{"location":"metrics/","title":"Metrics &amp; Cardinality explorer","text":"<p>ollyScale provides a powerful interface for analyzing OpenTelemetry metrics, with specific tools designed to help you understand the shape and cardinality of your telemetry data.</p>"},{"location":"metrics/#metrics-table","title":"Metrics Table","text":"<p>The metrics table offers a dense, high-information view of all ingested metrics.</p> <p></p> <p>Click the Chart button to visualize metric data over time:</p> <p></p>"},{"location":"metrics/#key-columns","title":"Key Columns","text":"<ul> <li>Name: The full OTel metric name (e.g., <code>http.server.response.size</code>).</li> <li>Unit: The unit of measurement (e.g., <code>By</code>, <code>ms</code>).</li> <li>Type: The metric type (Histogram, Sum, Gauge, etc.).</li> <li>Resources: Click to view the unique resource combinations associated with this metric.</li> <li>Cardinality: Shows the number of label dimensions vs the total number of unique time series (e.g., <code>8 labels / 185 series</code>).</li> </ul>"},{"location":"metrics/#cardinality-explorer","title":"Cardinality Explorer","text":"<p>Clicking on the blue Cardinality link for any metric opens the Cardinality Explorer. This tool is essential for understanding \"high cardinality\" issues and exploring your data's dimensions.</p>"},{"location":"metrics/#1-header-stats","title":"1. Header Stats","text":"<ul> <li>Total Series (Historic): The total number of unique time series seen for this metric since startup (persisted in Redis).</li> <li>Active Series (1h): The count of series seen in the last hour.</li> <li>Label Dimensions: The number of unique label keys (e.g., <code>http.method</code>, <code>http.status_code</code>).</li> </ul>"},{"location":"metrics/#2-label-analysis-table","title":"2. Label Analysis Table","text":"<p>This table helps you identify which labels are contributing most to your cardinality.</p> <ul> <li>Label Name: The key of the label.</li> <li>Cardinality: The number of unique values for this label.</li> <li>Values (Top 5): A preview of the most common values.</li> <li>If there are more than 5 values, a clickable <code>...</code> link expands the list to show all values inline.</li> </ul>"},{"location":"metrics/#3-raw-active-series","title":"3. Raw Active Series","text":"<p>A scrollable view of all active series in a PromQL-like syntax:</p> <pre><code>{container.id=\"...\", http.method=\"GET\", http.route=\"/api/traces\", http.status_code=\"200\", service.name=\"ollyscale-ui\"}\n{container.id=\"...\", http.method=\"GET\", http.route=\"/health\", http.status_code=\"200\", service.name=\"ollyscale-ui\"}\n</code></pre>"},{"location":"metrics/#export-actions","title":"Export Actions","text":"<p>Use the buttons in the \"Raw Active Series\" section to export data for offline analysis:</p> <ul> <li>Copy PromQL: Copies the visible series list to your clipboard.</li> <li>Download JSON: Downloads the full series object as a JSON file.</li> </ul>"},{"location":"metrics/#cardinality-protection","title":"Cardinality Protection","text":"<p>ollyScale includes built-in protection against cardinality explosions to prevent memory exhaustion during local development.</p> <ul> <li>Hard Limit: 1000 unique metric names (configurable).</li> <li>Visual Warnings:</li> <li>\u26a0\ufe0f Yellow: &gt; 70% capacity</li> <li>\ud83d\udd34 Red: &gt; 90% capacity</li> <li>Behavior: Metrics exceeding the limit are dropped, and a system alert is triggered.</li> </ul> <p>See Cardinality Protection for more details.</p>"},{"location":"ollyscale-v2-postgres/","title":"ollyScale v2 - Postgres Backend Architecture","text":""},{"location":"ollyscale-v2-postgres/#overview","title":"Overview","text":"<p>ollyScale v2 introduces a PostgreSQL-backed storage layer to replace the Redis-based ephemeral storage, providing persistent, queryable observability data with OTEL-aligned schema design.</p>"},{"location":"ollyscale-v2-postgres/#architecture-principles","title":"Architecture Principles","text":""},{"location":"ollyscale-v2-postgres/#core-tenets","title":"Core Tenets","text":"<ul> <li>OTEL-Native: Schema aligned with OpenTelemetry data model and semantic conventions</li> <li>Postgres-First: Leverage PostgreSQL features (JSONB, partitioning, indexes, pg_cron)</li> <li>Separation of Concerns: New <code>apps/frontend</code> app is independent from legacy <code>apps/ollyscale</code></li> <li>Clean Install: No migration/ETL from Redis required</li> <li>Kubernetes Native: Managed via Zalando Postgres Operator</li> </ul>"},{"location":"ollyscale-v2-postgres/#technology-stack","title":"Technology Stack","text":"<ul> <li>Database: PostgreSQL 18+ via Zalando Postgres Operator</li> <li>App Framework: FastAPI with async SQLAlchemy</li> <li>Migration Tool: Alembic</li> <li>Exposure: Kubernetes Gateway API (HTTPRoutes)</li> <li>Deployment: Helm charts with K8s Jobs for migrations</li> </ul>"},{"location":"ollyscale-v2-postgres/#data-model","title":"Data Model","text":""},{"location":"ollyscale-v2-postgres/#signals","title":"Signals","text":"<p>Three primary telemetry signals with OTEL alignment:</p> <ol> <li>Traces (<code>spans_fact</code> table)</li> <li>Logs (<code>logs_fact</code> table)</li> <li>Metrics (<code>metrics_fact</code> table)</li> </ol>"},{"location":"ollyscale-v2-postgres/#fact-tables","title":"Fact Tables","text":""},{"location":"ollyscale-v2-postgres/#spans_fact","title":"spans_fact","text":"<p>Stores distributed trace spans with OTEL span model:</p> <pre><code>CREATE TABLE spans_fact (\n    id BIGSERIAL PRIMARY KEY,\n    tenant_id VARCHAR(255) NOT NULL DEFAULT 'default',\n    connection_id VARCHAR(255),\n    trace_id VARCHAR(32) NOT NULL,\n    span_id VARCHAR(16) NOT NULL,\n    parent_span_id VARCHAR(16),\n\n    -- Core OTEL fields\n    name VARCHAR(1024) NOT NULL,\n    kind SMALLINT NOT NULL,  -- OTEL SpanKind enum\n    status_code SMALLINT,     -- OTEL StatusCode enum\n    status_message TEXT,\n\n    -- Timing\n    start_time_unix_nano BIGINT NOT NULL,\n    end_time_unix_nano BIGINT NOT NULL,\n    duration BIGINT GENERATED ALWAYS AS (end_time_unix_nano - start_time_unix_nano) STORED,\n\n    -- References\n    service_id INTEGER REFERENCES service_dim(id),\n    operation_id INTEGER REFERENCES operation_dim(id),\n    resource_id INTEGER REFERENCES resource_dim(id),\n\n    -- OTEL structures as JSONB\n    attributes JSONB,\n    events JSONB,\n    links JSONB,\n    resource JSONB,\n    scope JSONB,\n\n    -- Flags\n    flags INTEGER DEFAULT 0,\n    dropped_attributes_count INTEGER DEFAULT 0,\n    dropped_events_count INTEGER DEFAULT 0,\n    dropped_links_count INTEGER DEFAULT 0,\n\n    -- Metadata\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n) PARTITION BY RANGE (start_time_unix_nano);\n\nCREATE INDEX idx_spans_trace_id ON spans_fact(trace_id);\nCREATE INDEX idx_spans_service ON spans_fact(service_id);\nCREATE INDEX idx_spans_time ON spans_fact(start_time_unix_nano);\nCREATE INDEX idx_spans_attributes ON spans_fact USING GIN (attributes);\n</code></pre>"},{"location":"ollyscale-v2-postgres/#logs_fact","title":"logs_fact","text":"<p>Stores log entries with OTEL log data model:</p> <pre><code>CREATE TABLE logs_fact (\n    id BIGSERIAL PRIMARY KEY,\n    tenant_id VARCHAR(255) NOT NULL DEFAULT 'default',\n    connection_id VARCHAR(255),\n\n    -- OTEL correlation\n    trace_id VARCHAR(32),\n    span_id VARCHAR(16),\n\n    -- Timing\n    time_unix_nano BIGINT NOT NULL,\n    observed_time_unix_nano BIGINT,\n\n    -- Severity\n    severity_number SMALLINT,\n    severity_text VARCHAR(64),\n\n    -- Content\n    body JSONB,\n\n    -- OTEL structures\n    attributes JSONB,\n    resource JSONB,\n    scope JSONB,\n\n    -- Flags\n    flags INTEGER DEFAULT 0,\n    dropped_attributes_count INTEGER DEFAULT 0,\n\n    -- Metadata\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n) PARTITION BY RANGE (time_unix_nano);\n\nCREATE INDEX idx_logs_trace_id ON logs_fact(trace_id);\nCREATE INDEX idx_logs_time ON logs_fact(time_unix_nano);\nCREATE INDEX idx_logs_severity ON logs_fact(severity_number);\nCREATE INDEX idx_logs_attributes ON logs_fact USING GIN (attributes);\n</code></pre>"},{"location":"ollyscale-v2-postgres/#metrics_fact","title":"metrics_fact","text":"<p>Stores metrics with OTEL metrics data model:</p> <pre><code>CREATE TABLE metrics_fact (\n    id BIGSERIAL PRIMARY KEY,\n    tenant_id VARCHAR(255) NOT NULL DEFAULT 'default',\n    connection_id VARCHAR(255),\n\n    -- Metric identity\n    metric_name VARCHAR(1024) NOT NULL,\n    metric_type VARCHAR(32) NOT NULL,  -- gauge, sum, histogram, summary, exponential_histogram\n    unit VARCHAR(64),\n    description TEXT,\n\n    -- Timing\n    time_unix_nano BIGINT NOT NULL,\n    start_time_unix_nano BIGINT,\n\n    -- OTEL structures\n    resource JSONB,\n    scope JSONB,\n    attributes JSONB,\n    data_points JSONB,  -- Array of data point objects\n\n    -- Aggregation temporality (for sum/histogram)\n    temporality VARCHAR(32),\n\n    -- Monotonicity (for sum)\n    is_monotonic BOOLEAN,\n\n    -- Metadata\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n) PARTITION BY RANGE (time_unix_nano);\n\nCREATE INDEX idx_metrics_name ON metrics_fact(metric_name);\nCREATE INDEX idx_metrics_time ON metrics_fact(time_unix_nano);\nCREATE INDEX idx_metrics_attributes ON metrics_fact USING GIN (attributes);\n</code></pre>"},{"location":"ollyscale-v2-postgres/#dimension-tables","title":"Dimension Tables","text":""},{"location":"ollyscale-v2-postgres/#service_dim","title":"service_dim","text":"<p>Service catalog:</p> <pre><code>CREATE TABLE service_dim (\n    id SERIAL PRIMARY KEY,\n    tenant_id VARCHAR(255) NOT NULL DEFAULT 'default',\n    name VARCHAR(255) NOT NULL,\n    namespace VARCHAR(255) NOT NULL DEFAULT '',\n    version VARCHAR(255),\n    attributes JSONB,\n    first_seen TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    last_seen TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    UNIQUE(tenant_id, name, namespace)\n);\n</code></pre> <p>Note on namespace: Per OTEL spec, <code>service.namespace</code> is optional. We store empty string <code>''</code> when not provided (rather than NULL) so the UNIQUE constraint works correctly. OTEL spec states: \"If service.namespace is not specified then service.name is expected to be unique for all services that have no explicit namespace defined (so the empty/unspecified namespace is simply one more valid namespace).\"</p>"},{"location":"ollyscale-v2-postgres/#operation_dim","title":"operation_dim","text":"<p>Operation (span name) catalog:</p> <pre><code>CREATE TABLE operation_dim (\n    id SERIAL PRIMARY KEY,\n    tenant_id VARCHAR(255) NOT NULL DEFAULT 'default',\n    service_id INTEGER REFERENCES service_dim(id),\n    name VARCHAR(1024) NOT NULL,\n    span_kind SMALLINT,\n    first_seen TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    last_seen TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    UNIQUE(tenant_id, service_id, name, span_kind)\n);\n</code></pre>"},{"location":"ollyscale-v2-postgres/#resource_dim","title":"resource_dim","text":"<p>Resource attributes catalog:</p> <pre><code>CREATE TABLE resource_dim (\n    id SERIAL PRIMARY KEY,\n    tenant_id VARCHAR(255) NOT NULL DEFAULT 'default',\n    resource_hash VARCHAR(64) NOT NULL,\n    attributes JSONB NOT NULL,\n    first_seen TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    last_seen TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    UNIQUE(tenant_id, resource_hash)\n);\n\nCREATE INDEX idx_resource_attributes ON resource_dim USING GIN (attributes);\n</code></pre>"},{"location":"ollyscale-v2-postgres/#retention-partitioning","title":"Retention &amp; Partitioning","text":""},{"location":"ollyscale-v2-postgres/#retention-policy-table","title":"Retention Policy Table","text":"<pre><code>CREATE TABLE retention_policy (\n    id SERIAL PRIMARY KEY,\n    tenant_id VARCHAR(255) NOT NULL DEFAULT 'default',\n    signal_type VARCHAR(32) NOT NULL,  -- 'traces', 'logs', 'metrics'\n    retention_days INTEGER NOT NULL DEFAULT 7,\n    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n    UNIQUE(tenant_id, signal_type)\n);\n\n-- Default retention policies\nINSERT INTO retention_policy (tenant_id, signal_type, retention_days) VALUES\n    ('default', 'traces', 7),\n    ('default', 'logs', 3),\n    ('default', 'metrics', 30);\n</code></pre>"},{"location":"ollyscale-v2-postgres/#partitioning-strategy","title":"Partitioning Strategy","text":"<p>Approach: Native PostgreSQL table partitioning by time range</p> <p>Interval: Daily partitions for spans_fact and logs_fact, weekly for metrics_fact</p> <p>Rationale:</p> <ul> <li>Daily spans/logs for fine-grained retention control</li> <li>Weekly metrics for reduced partition overhead (metrics have higher cardinality)</li> <li>Partition pruning optimizes time-range queries</li> <li>Easy partition dropping for retention enforcement</li> </ul> <p>Partition Naming:</p> <ul> <li><code>spans_fact_YYYYMMDD</code> (e.g., <code>spans_fact_20260121</code>)</li> <li><code>logs_fact_YYYYMMDD</code></li> <li><code>metrics_fact_YYYYWW</code> (e.g., <code>metrics_fact_202603</code> for week 3)</li> </ul>"},{"location":"ollyscale-v2-postgres/#retention-enforcement","title":"Retention Enforcement","text":"<p>Mechanism: Kubernetes CronJob running daily at 02:00 UTC</p> <p>Process:</p> <ol> <li>Query <code>retention_policy</code> table for each signal type</li> <li>Calculate partition names older than retention window</li> <li>Drop expired partitions: <code>DROP TABLE IF EXISTS spans_fact_YYYYMMDD</code></li> <li>Log dropped partitions to stdout (captured by K8s logs)</li> </ol> <p>Job Specification: See <code>charts/ollyscale/templates/cronjob-retention.yaml</code></p>"},{"location":"ollyscale-v2-postgres/#api-design","title":"API Design","text":""},{"location":"ollyscale-v2-postgres/#endpoints","title":"Endpoints","text":""},{"location":"ollyscale-v2-postgres/#ingest","title":"Ingest","text":"<ul> <li><code>POST /api/traces</code> - Ingest OTLP traces</li> <li><code>POST /api/logs</code> - Ingest OTLP logs</li> <li><code>POST /api/metrics</code> - Ingest OTLP metrics</li> </ul>"},{"location":"ollyscale-v2-postgres/#query","title":"Query","text":"<ul> <li><code>GET /api/traces/search</code> - Search traces with filters</li> <li><code>GET /api/traces/{trace_id}</code> - Get trace by ID</li> <li><code>GET /api/logs/search</code> - Search logs with filters</li> <li><code>GET /api/metrics/search</code> - Query metrics</li> <li><code>GET /api/service-map</code> - Get service dependency graph</li> <li><code>GET /api/services</code> - List services with RED metrics</li> </ul>"},{"location":"ollyscale-v2-postgres/#query-semantics","title":"Query Semantics","text":"<p>Time Range (Required):</p> <ul> <li>All queries require <code>start_time</code> and <code>end_time</code> parameters (Unix nanoseconds)</li> <li>Leverages partition pruning for performance</li> </ul> <p>Filtering:</p> <ul> <li>Structured filters: <code>{field: string, operator: string, value: any}[]</code></li> <li>Supported operators: <code>eq</code>, <code>ne</code>, <code>gt</code>, <code>lt</code>, <code>gte</code>, <code>lte</code>, <code>contains</code>, <code>regex</code></li> <li>Common fields: <code>service.name</code>, <code>http.method</code>, <code>status.code</code>, <code>severity</code>, <code>trace_id</code></li> </ul> <p>Pagination:</p> <ul> <li>Cursor-based pagination (opaque cursor encoding time + id)</li> <li><code>limit</code> parameter (default: 100, max: 1000)</li> <li>Response includes <code>has_more</code> boolean and <code>next_cursor</code> string</li> </ul> <p>Limits:</p> <ul> <li>Hard maximum of 1000 results per request (server-side enforcement)</li> <li>Query timeout: 30 seconds</li> <li>Large result warnings at 500+ rows</li> </ul>"},{"location":"ollyscale-v2-postgres/#deployment","title":"Deployment","text":""},{"location":"ollyscale-v2-postgres/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes cluster with Gateway API CRDs installed</li> <li>Zalando Postgres Operator deployed</li> <li>Sufficient storage for Postgres PVCs (default: 30GB)</li> </ul>"},{"location":"ollyscale-v2-postgres/#components","title":"Components","text":"<ol> <li>Postgres Cluster (<code>apps/frontend/postgres-cluster</code>)</li> <li>Zalando <code>postgresql</code> CRD resource</li> <li>Single-node, no HA (upgradeable to Patroni later)</li> <li> <p>Operator-managed credentials in Secret</p> </li> <li> <p>Migration Job (<code>charts/ollyscale/templates/job-migration.yaml</code>)</p> </li> <li>Runs Alembic <code>upgrade head</code> before app deployment</li> <li>Uses init container pattern with shared volume</li> <li> <p>Fails deployment if migrations fail</p> </li> <li> <p>Frontend App (<code>charts/ollyscale/templates/deployment-frontend.yaml</code>)</p> </li> <li>FastAPI app with async SQLAlchemy connection pool</li> <li>Reads DB credentials from operator-managed Secret</li> <li> <p>Readiness probe checks DB connectivity + migration status</p> </li> <li> <p>Gateway API (<code>charts/ollyscale/templates/httproute-frontend.yaml</code>)</p> </li> <li>HTTPRoute exposing <code>/api/*</code> paths</li> <li>Matches Gateway in <code>ollyscale</code> namespace</li> </ol>"},{"location":"ollyscale-v2-postgres/#helm-values","title":"Helm Values","text":"<pre><code>postgres:\n  enabled: true\n  operator:\n    namespace: postgres-operator\n  cluster:\n    name: ollyscale-db\n    size: 30Gi\n    storageClass: standard\n  credentials:\n    secretName: ollyscale-db-credentials\n\nfrontend:\n  replicas: 2\n  image:\n    repository: ghcr.io/ryanfaircloth/ollyscale/frontend\n    tag: \"v2.0.0\"\n  resources:\n    requests:\n      memory: 512Mi\n      cpu: 250m\n    limits:\n      memory: 1Gi\n      cpu: 1000m\n  database:\n    poolSize: 20\n    maxOverflow: 10\n    queryTimeout: 30\n\nmigration:\n  enabled: true\n  backoffLimit: 3\n  ttlSecondsAfterFinished: 86400\n</code></pre>"},{"location":"ollyscale-v2-postgres/#credentials-flow","title":"Credentials Flow","text":"<ol> <li>Zalando Operator creates Secret: <code>&lt;cluster-name&gt;.&lt;user&gt;.&lt;secret-suffix&gt;</code></li> <li>Format: <code>ollyscale-db.frontend.credentials</code></li> <li> <p>Keys: <code>username</code>, <code>password</code>, <code>hostname</code>, <code>port</code>, <code>database</code></p> </li> <li> <p>Migration Job mounts Secret as environment variables:</p> </li> </ol> <pre><code>env:\n  - name: DB_HOST\n    valueFrom:\n      secretKeyRef:\n        name: ollyscale-db.frontend.credentials\n        key: hostname\n</code></pre> <ol> <li>Frontend App constructs connection string:</li> </ol> <pre><code>DB_URL = f\"postgresql+asyncpg://{username}:{password}@{hostname}:{port}/{database}\"\n</code></pre>"},{"location":"ollyscale-v2-postgres/#migration-management","title":"Migration Management","text":""},{"location":"ollyscale-v2-postgres/#alembic-configuration","title":"Alembic Configuration","text":"<p>Location: <code>apps/frontend/migrations/</code></p> <p>Key Files:</p> <ul> <li><code>alembic.ini</code> - Alembic configuration</li> <li><code>env.py</code> - Environment setup with async support</li> <li><code>versions/</code> - Migration scripts</li> </ul> <p>Connection String:</p> <pre><code># env.py\nconfig.set_main_option(\n    \"sqlalchemy.url\",\n    os.getenv(\"DATABASE_URL\", \"postgresql+asyncpg://localhost/ollyscale\")\n)\n</code></pre>"},{"location":"ollyscale-v2-postgres/#creating-migrations","title":"Creating Migrations","text":"<pre><code>cd apps/frontend\npoetry run alembic revision -m \"add spans_fact partitioning\"\n# Edit generated file in migrations/versions/\npoetry run alembic upgrade head  # Test locally\n</code></pre>"},{"location":"ollyscale-v2-postgres/#migration-job-flow","title":"Migration Job Flow","text":"<ol> <li>Init container runs <code>alembic upgrade head</code></li> <li>Logs migration output to stdout</li> <li>Exits with code 0 on success, non-zero on failure</li> <li>Main app container only starts if init container succeeds</li> <li>Kubernetes backoff handles transient failures</li> </ol>"},{"location":"ollyscale-v2-postgres/#rollback-strategy","title":"Rollback Strategy","text":"<p>Automatic Rollback: Not supported (forward-only migrations)</p> <p>Manual Rollback:</p> <pre><code>kubectl exec -it &lt;migration-job-pod&gt; -- alembic downgrade -1\n</code></pre> <p>Backup Strategy (future):</p> <ul> <li>pg_dump before major schema changes</li> <li>Stored in S3-compatible object storage</li> </ul>"},{"location":"ollyscale-v2-postgres/#health-endpoints","title":"Health Endpoints","text":""},{"location":"ollyscale-v2-postgres/#health-overall-status","title":"<code>/health</code> (Overall Status)","text":"<p>Returns 200 if app is healthy, 503 if degraded:</p> <pre><code>{\n  \"status\": \"healthy\",\n  \"database\": \"connected\",\n  \"migrations\": \"current\",\n  \"version\": \"v2.0.0\"\n}\n</code></pre>"},{"location":"ollyscale-v2-postgres/#healthdb-database-status","title":"<code>/health/db</code> (Database Status)","text":"<p>Returns DB connectivity and migration status:</p> <pre><code>{\n  \"connected\": true,\n  \"pool_size\": 20,\n  \"pool_active\": 5,\n  \"migrations_applied\": 42,\n  \"latest_migration\": \"20260121_add_metrics_partitioning\"\n}\n</code></pre>"},{"location":"ollyscale-v2-postgres/#observability-performance","title":"Observability &amp; Performance","text":""},{"location":"ollyscale-v2-postgres/#instrumentation","title":"Instrumentation","text":"<p>OTEL Tracing:</p> <ul> <li>FastAPI automatic instrumentation via <code>opentelemetry-instrumentation-fastapi</code></li> <li>SQLAlchemy query tracing via <code>opentelemetry-instrumentation-sqlalchemy</code></li> <li>Sampling: 10% for normal requests, 100% for errors</li> </ul> <p>Metrics:</p> <ul> <li>HTTP request duration (histogram)</li> <li>Database connection pool stats (gauge)</li> <li>Query execution time by endpoint (histogram)</li> <li>Ingest throughput (counter)</li> </ul> <p>Logging:</p> <ul> <li>Structured JSON logs to stdout</li> <li>Correlation via <code>trace_id</code> and <code>span_id</code> fields</li> <li>Log levels: INFO (normal), DEBUG (query details), ERROR (failures)</li> </ul>"},{"location":"ollyscale-v2-postgres/#performance-optimization","title":"Performance Optimization","text":"<p>Indexes:</p> <ul> <li>B-tree on time columns (partition key)</li> <li>B-tree on trace_id, span_id (exact match queries)</li> <li>GIN on JSONB attributes (attribute filtering)</li> <li>Composite indexes on (service_id, time) for service-filtered queries</li> </ul> <p>Query Patterns:</p> <ul> <li>Always include time range in WHERE clause (partition pruning)</li> <li>Use <code>attributes @&gt; '{\"key\": \"value\"}'</code> for JSONB filtering</li> <li>Prefer <code>EXISTS</code> over <code>IN</code> for subqueries</li> <li>Use <code>EXPLAIN ANALYZE</code> for slow query debugging</li> </ul> <p>Connection Pooling:</p> <ul> <li>SQLAlchemy async pool: 20 connections, max_overflow 10</li> <li>Connection timeout: 5 seconds</li> <li>Pool pre-ping enabled for stale connection detection</li> </ul> <p>Caching (future):</p> <ul> <li>Service map cached for 30 seconds (Redis)</li> <li>Service catalog cached for 5 minutes</li> <li>Query result caching for identical time ranges (optional)</li> </ul>"},{"location":"ollyscale-v2-postgres/#troubleshooting","title":"Troubleshooting","text":""},{"location":"ollyscale-v2-postgres/#migration-failures","title":"Migration Failures","text":"<p>Symptom: Migration Job fails, pods stuck in Init</p> <p>Diagnosis:</p> <pre><code>kubectl logs -n ollyscale job/ollyscale-migration\n</code></pre> <p>Common Causes:</p> <ul> <li>DB credentials incorrect (check Secret)</li> <li>DB not reachable (check network policies)</li> <li>Syntax error in migration script</li> <li>Concurrent migration attempts (lock timeout)</li> </ul> <p>Resolution:</p> <ul> <li>Fix underlying issue (credentials, network, SQL)</li> <li>Delete failed Job: <code>kubectl delete job -n ollyscale ollyscale-migration</code></li> <li>Helm upgrade will recreate Job</li> </ul>"},{"location":"ollyscale-v2-postgres/#slow-queries","title":"Slow Queries","text":"<p>Symptom: API timeouts, high DB load</p> <p>Diagnosis:</p> <pre><code>-- Check slow queries\nSELECT * FROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 10;\n\n-- Check missing indexes\nSELECT schemaname, tablename, attname, n_distinct, correlation\nFROM pg_stats\nWHERE schemaname = 'public'\nAND correlation &lt; 0.5;\n</code></pre> <p>Resolution:</p> <ul> <li>Add missing indexes based on query patterns</li> <li>Reduce query time range</li> <li>Increase <code>limit</code> parameter instead of broad queries</li> <li>Consider query result caching</li> </ul>"},{"location":"ollyscale-v2-postgres/#high-cardinality","title":"High Cardinality","text":"<p>Symptom: Metrics table growing rapidly, slow queries</p> <p>Diagnosis:</p> <pre><code>SELECT metric_name, COUNT(DISTINCT attributes) as cardinality\nFROM metrics_fact\nWHERE time_unix_nano &gt; extract(epoch from now() - interval '1 hour') * 1000000000\nGROUP BY metric_name\nORDER BY cardinality DESC\nLIMIT 20;\n</code></pre> <p>Resolution:</p> <ul> <li>Identify high-cardinality metrics</li> <li>Drop unnecessary attributes at ingestion (OTEL Processor)</li> <li>Implement attribute allowlist/blocklist</li> <li>Consider aggregating high-cardinality metrics</li> </ul>"},{"location":"ollyscale-v2-postgres/#partition-management","title":"Partition Management","text":"<p>Symptom: Retention Job fails, old partitions not dropping</p> <p>Diagnosis:</p> <pre><code>kubectl logs -n ollyscale cronjob/ollyscale-retention\n</code></pre> <p>Resolution:</p> <ul> <li>Check CronJob schedule and last run time</li> <li>Verify <code>retention_policy</code> table values</li> <li>Manually drop partitions if needed:</li> </ul> <pre><code>DROP TABLE IF EXISTS spans_fact_20260101;\n</code></pre>"},{"location":"ollyscale-v2-postgres/#future-enhancements","title":"Future Enhancements","text":"<p>High Availability:</p> <ul> <li>Enable Patroni for Postgres HA (Zalando Operator supports this)</li> <li>Multi-replica frontend app (already supported)</li> <li>Cross-AZ deployment</li> </ul> <p>Backups:</p> <ul> <li>Automated pg_basebackup to S3</li> <li>Point-in-time recovery (PITR) via WAL archiving</li> <li>Backup retention policy (30 days)</li> </ul> <p>Multi-Tenancy:</p> <ul> <li>Row-level security (RLS) for tenant isolation</li> <li>Separate schemas per tenant</li> <li>Tenant-specific retention policies</li> </ul> <p>Query Optimization:</p> <ul> <li>Materialized views for service catalog</li> <li>Pre-aggregated RED metrics tables</li> <li>Query result caching (Redis)</li> </ul> <p>Advanced Features:</p> <ul> <li>Trace exemplars linked to metrics</li> <li>Log pattern detection</li> <li>Anomaly detection on metrics</li> <li>Alerting on query results</li> </ul>"},{"location":"ollyscale-v2-postgres/#references","title":"References","text":"<ul> <li>OpenTelemetry Specification</li> <li>Zalando Postgres Operator</li> <li>PostgreSQL Partitioning</li> <li>Alembic Documentation</li> <li>FastAPI Async SQL</li> </ul>"},{"location":"otel-collector/","title":"OpenTelemetry Collector","text":"<p>OpenTelemetry Collector configuration management via OpAMP</p> <p>ollyScale uses the OpenTelemetry Collector as the telemetry ingestion and shipping layer. The collector receives telemetry from your applications and forwards it to ollyScale's OTLP receiver.</p>"},{"location":"otel-collector/#opentelemetry-collector-opamp-config-page","title":"OpenTelemetry Collector + OpAMP Config Page","text":"<p>ollyScale includes a web interface for managing OpenTelemetry Collector configurations via the OpAMP (Open Agent Management Protocol). Access this page through the \"OpenTelemetry Collector + OpAMP Config\" tab in the ollyScale UI.</p> <p>Features:</p> <ul> <li>View current configuration from connected collectors</li> <li>Apply configuration changes with real-time validation</li> <li>Browse configuration templates for common use cases</li> <li>Monitor OpAMP server status and connected agents</li> <li>Preview configuration diffs before applying</li> </ul> <p>Requirements:</p> <ul> <li>Your collector must be configured with the OpAMP extension (see OpAMP Configuration below)</li> <li>Collector must be connected to the OpAMP server</li> </ul>"},{"location":"otel-collector/#configuration","title":"Configuration","text":"<p>ollyScale includes a sample collector configuration that you can customize for your needs. The configuration files are located at:</p> <ul> <li>Docker: <code>docker/otelcol-configs/config.yaml</code></li> <li>Kubernetes (Helm): Configured via Helm values - see <code>charts/ollyscale/values.yaml</code> for OTel Collector settings</li> </ul>"},{"location":"otel-collector/#default-configuration","title":"Default Configuration","text":"<p>The default configuration includes:</p> <ul> <li>OTLP Receivers: Accepts telemetry on ports 4317 (gRPC) and 4318 (HTTP)</li> <li>OpAMP Extension: Enables remote configuration management via ollyScale UI</li> <li>Span Metrics Connector: Automatically generates RED metrics from traces</li> <li>Batch Processor: Batches telemetry for efficient processing</li> <li>OTLP Exporter: Forwards all telemetry to ollyScale's OTLP receiver</li> </ul>"},{"location":"otel-collector/#customization-examples","title":"Customization Examples","text":"<p>You can extend the collector configuration to add additional capabilities. The collector uses the <code>otel/opentelemetry-collector-contrib</code> image which includes:</p> <ul> <li>Receivers: OTLP, Prometheus, Jaeger, Zipkin, and many more</li> <li>Processors: Batch, Memory Limiter, Resource Detection, Tail Sampling, Filtering</li> <li>Connectors: Span Metrics, Service Graph</li> <li>Exporters: OTLP, Prometheus, Logging, and many more</li> </ul> <p>For a complete list of available components, see the OpenTelemetry Collector Contrib documentation.</p>"},{"location":"otel-collector/#applying-changes","title":"Applying Changes","text":""},{"location":"otel-collector/#docker","title":"Docker","text":"<p>After modifying <code>docker/otelcol-configs/config.yaml</code> rebuild/restart using:</p> <pre><code>cd docker\n./01-start-core.sh\n</code></pre>"},{"location":"otel-collector/#kubernetes","title":"Kubernetes","text":"<p>When deployed via Helm, the OTel Collector is managed by the OpenTelemetry Operator. To customize the configuration:</p> <ol> <li>Edit <code>charts/ollyscale/values.yaml</code> under the <code>otelCollector</code> section</li> <li>Apply changes:</li> </ol> <pre><code>cd charts\nhelm upgrade ollyscale ./ollyscale -n ollyscale\n</code></pre> <p>Alternatively, patch the OpenTelemetryCollector custom resource directly:</p> <pre><code>kubectl edit opentelemetrycollector ollyscale-otel-collector -n ollyscale\n</code></pre>"},{"location":"otel-collector/#using-your-own-collector","title":"Using Your Own Collector","text":"<p>You can use your own OpenTelemetry Collector instance instead of the one bundled with ollyScale. This is useful if you have an existing collector setup or want to test specific collector configurations.</p> <p>To do this, deploy the Core-Only version of ollyScale (see Docker Deployment or Kubernetes Deployment).</p> <p>Then, configure your collector's OTLP exporter to send data to the ollyScale Receiver:</p> <ul> <li>Endpoint: <code>ollyscale-otlp-receiver:4343</code> (or <code>localhost:4343</code> from host)</li> <li>Protocol: gRPC</li> <li>TLS: Insecure (or configured as needed)</li> </ul> <p>Example Exporter Configuration:</p> <pre><code>exporters:\n  otlp:\n    endpoint: \"ollyscale-otlp-receiver:4343\"\n    tls:\n      insecure: true\n</code></pre>"},{"location":"otel-collector/#opamp-configuration-optional","title":"OpAMP Configuration (Optional)","text":"<p>The OpenTelemetry Collector + OpAMP Config page in the ollyScale UI allows you to view and manage collector configurations remotely. To enable this feature, add the OpAMP extension to your collector config:</p> <pre><code>extensions:\n  opamp:\n    server:\n      ws:\n        endpoint: ws://localhost:4320/v1/opamp\n\nservice:\n  extensions: [opamp]\n</code></pre> <p>The default configuration template (located at <code>docker/otelcol-configs/config.yaml</code>) shows a complete example with OTLP receivers, OpAMP extension, batch processing, and spanmetrics connector. Your collector will connect to the OpAMP server and receive configuration updates through the ollyScale UI.</p>"},{"location":"partition-management/","title":"Partition Management","text":""},{"location":"partition-management/#overview","title":"Overview","text":"<p>The ollyScale v2 backend uses PostgreSQL native table partitioning to efficiently manage time-series observability data. This document describes the automated partition management system.</p>"},{"location":"partition-management/#why-partitioning","title":"Why Partitioning?","text":"<p>Time-series data (traces, logs, metrics) accumulates rapidly and has a natural time-based access pattern:</p> <ul> <li>Recent data is hot - Most queries target last few hours/days</li> <li>Old data is cold - Rarely accessed after retention period</li> <li>Predictable lifecycle - Data expires based on retention policy</li> </ul> <p>Partitioning provides:</p> <ul> <li>Query performance - Partition pruning skips irrelevant data</li> <li>Efficient deletion - Drop entire partitions instead of DELETE (much faster)</li> <li>Storage optimization - Old partitions can be compressed or moved to cold storage</li> <li>Maintenance windows - Operations on individual partitions don't lock entire table</li> </ul>"},{"location":"partition-management/#partitioning-strategy","title":"Partitioning Strategy","text":""},{"location":"partition-management/#tables","title":"Tables","text":"<p>All fact tables use range partitioning on their timestamp column:</p> Table Partition Key Retention (default) <code>spans_fact</code> <code>start_time_ns</code> 7 days <code>logs_fact</code> <code>timestamp_ns</code> 3 days <code>metrics_fact</code> <code>timestamp_ns</code> 30 days"},{"location":"partition-management/#partition-naming-convention","title":"Partition Naming Convention","text":"<p>Partitions are named: <code>{table_name}_y{YYYY}_m{MM}_d{DD}</code></p> <p>Examples:</p> <ul> <li><code>spans_fact_y2026_m01_d21</code> - Spans for January 21, 2026</li> <li><code>logs_fact_y2026_m01_d20</code> - Logs for January 20, 2026</li> <li><code>metrics_fact_y2026_m02_d05</code> - Metrics for February 5, 2026</li> </ul>"},{"location":"partition-management/#partition-granularity","title":"Partition Granularity","text":"<ul> <li>Daily partitions (24-hour intervals)</li> <li>Aligned to UTC day boundaries</li> <li>One partition per table per day</li> </ul>"},{"location":"partition-management/#automated-management","title":"Automated Management","text":""},{"location":"partition-management/#cronjob","title":"CronJob","text":"<p>The <code>partition-maintenance</code> CronJob runs daily at 2 AM UTC:</p> <pre><code>partitionMaintenance:\n  enabled: true\n  schedule: \"0 2 * * *\"  # Daily at 2 AM UTC\n  daysAhead: 7            # Create partitions 7 days ahead\n</code></pre> <p>Actions performed:</p> <ol> <li>Create future partitions - Ensures partitions exist for upcoming data</li> <li>Drop old partitions - Removes partitions older than retention policy</li> </ol>"},{"location":"partition-management/#functions","title":"Functions","text":""},{"location":"partition-management/#create_partitions_for_range","title":"<code>create_partitions_for_range()</code>","text":"<p>Creates partitions for a time range.</p> <p>Signature:</p> <pre><code>create_partitions_for_range(\n    table_name TEXT,\n    start_time BIGINT,\n    end_time BIGINT,\n    interval_hours INTEGER DEFAULT 24\n) RETURNS INTEGER\n</code></pre> <p>Parameters:</p> <ul> <li><code>table_name</code> - Target fact table (<code>spans_fact</code>, <code>logs_fact</code>, <code>metrics_fact</code>)</li> <li><code>start_time</code> - Start time in nanoseconds (Unix epoch)</li> <li><code>end_time</code> - End time in nanoseconds</li> <li><code>interval_hours</code> - Partition size in hours (default: 24)</li> </ul> <p>Returns: Number of partitions created</p> <p>Example:</p> <pre><code>-- Create daily partitions for spans_fact for next 7 days\nSELECT create_partitions_for_range(\n    'spans_fact',\n    EXTRACT(EPOCH FROM NOW())::BIGINT * 1000000000,\n    EXTRACT(EPOCH FROM NOW() + INTERVAL '7 days')::BIGINT * 1000000000,\n    24\n);\n</code></pre>"},{"location":"partition-management/#drop_old_partitions","title":"<code>drop_old_partitions()</code>","text":"<p>Drops partitions older than retention policy.</p> <p>Signature:</p> <pre><code>drop_old_partitions(\n    table_name TEXT,\n    signal_type TEXT,\n    tenant_id_param TEXT DEFAULT 'default'\n) RETURNS INTEGER\n</code></pre> <p>Parameters:</p> <ul> <li><code>table_name</code> - Target fact table</li> <li><code>signal_type</code> - Signal type (<code>traces</code>, <code>logs</code>, <code>metrics</code>)</li> <li><code>tenant_id_param</code> - Tenant ID (default: <code>'default'</code>)</li> </ul> <p>Returns: Number of partitions dropped</p> <p>Example:</p> <pre><code>-- Drop old trace partitions\nSELECT drop_old_partitions('spans_fact', 'traces', 'default');\n</code></pre>"},{"location":"partition-management/#maintenance_create_partitions","title":"<code>maintenance_create_partitions()</code>","text":"<p>Wrapper that creates future partitions for all fact tables.</p> <p>Signature:</p> <pre><code>maintenance_create_partitions(\n    days_ahead INTEGER DEFAULT 7\n) RETURNS TABLE(table_name TEXT, partitions_created INTEGER)\n</code></pre> <p>Parameters:</p> <ul> <li><code>days_ahead</code> - Number of days ahead to create partitions (default: 7)</li> </ul> <p>Returns: Table with results per fact table</p> <p>Example:</p> <pre><code>-- Create partitions 14 days ahead\nSELECT * FROM maintenance_create_partitions(14);\n</code></pre> <p>Output:</p> <pre><code>  table_name   | partitions_created\n---------------+-------------------\n spans_fact    |                 7\n logs_fact     |                 7\n metrics_fact  |                 7\n</code></pre>"},{"location":"partition-management/#maintenance_drop_partitions","title":"<code>maintenance_drop_partitions()</code>","text":"<p>Wrapper that drops old partitions for all fact tables.</p> <p>Signature:</p> <pre><code>maintenance_drop_partitions()\nRETURNS TABLE(table_name TEXT, signal_type TEXT, partitions_dropped INTEGER)\n</code></pre> <p>Returns: Table with results per fact table</p> <p>Example:</p> <pre><code>SELECT * FROM maintenance_drop_partitions();\n</code></pre> <p>Output:</p> <pre><code>  table_name   | signal_type | partitions_dropped\n---------------+-------------+-------------------\n spans_fact    | traces      |                 2\n logs_fact     | logs        |                 5\n metrics_fact  | metrics     |                 0\n</code></pre>"},{"location":"partition-management/#monitoring","title":"Monitoring","text":""},{"location":"partition-management/#view-all-partitions","title":"View All Partitions","text":"<pre><code>SELECT\n    parent.relname AS table_name,\n    child.relname AS partition_name,\n    pg_size_pretty(pg_total_relation_size(child.oid)) AS partition_size,\n    pg_get_expr(child.relpartbound, child.oid) AS partition_range\nFROM pg_class parent\nJOIN pg_inherits i ON i.inhparent = parent.oid\nJOIN pg_class child ON child.oid = i.inhrelid\nWHERE parent.relname IN ('spans_fact', 'logs_fact', 'metrics_fact')\nORDER BY parent.relname, child.relname;\n</code></pre>"},{"location":"partition-management/#check-partition-coverage","title":"Check Partition Coverage","text":"<pre><code>-- Check if partitions exist for today and tomorrow\nWITH dates AS (\n    SELECT\n        t.table_name,\n        CURRENT_DATE + i AS check_date\n    FROM generate_series(0, 1) AS i\n    CROSS JOIN (VALUES ('spans_fact'), ('logs_fact'), ('metrics_fact')) AS t(table_name)\n)\nSELECT\n    d.table_name,\n    d.check_date,\n    CASE\n        WHEN c.relname IS NOT NULL THEN 'EXISTS'\n        ELSE 'MISSING'\n    END AS status\nFROM dates d\nLEFT JOIN pg_class c ON c.relname = d.table_name || '_y' ||\n    TO_CHAR(d.check_date, 'YYYY') || '_m' ||\n    TO_CHAR(d.check_date, 'MM') || '_d' ||\n    TO_CHAR(d.check_date, 'DD')\nORDER BY d.table_name, d.check_date;\n</code></pre>"},{"location":"partition-management/#partition-sizes","title":"Partition Sizes","text":"<pre><code>SELECT\n    parent.relname AS table_name,\n    COUNT(*) AS partition_count,\n    pg_size_pretty(SUM(pg_total_relation_size(child.oid))) AS total_size,\n    pg_size_pretty(AVG(pg_total_relation_size(child.oid))::BIGINT) AS avg_partition_size\nFROM pg_class parent\nJOIN pg_inherits i ON i.inhparent = parent.oid\nJOIN pg_class child ON child.oid = i.inhrelid\nWHERE parent.relname IN ('spans_fact', 'logs_fact', 'metrics_fact')\nGROUP BY parent.relname\nORDER BY parent.relname;\n</code></pre>"},{"location":"partition-management/#troubleshooting","title":"Troubleshooting","text":""},{"location":"partition-management/#missing-partitions","title":"Missing Partitions","text":"<p>Symptom: Ingestion fails with \"no partition of relation found for row\"</p> <p>Diagnosis:</p> <pre><code>-- Check if partition exists for specific timestamp\nSELECT\n    table_name || '_y' ||\n    TO_CHAR(TO_TIMESTAMP(timestamp_ns / 1000000000.0), 'YYYY') || '_m' ||\n    TO_CHAR(TO_TIMESTAMP(timestamp_ns / 1000000000.0), 'MM') || '_d' ||\n    TO_CHAR(TO_TIMESTAMP(timestamp_ns / 1000000000.0), 'DD') AS expected_partition,\n    EXISTS (\n        SELECT 1 FROM pg_class\n        WHERE relname = table_name || '_y' ||\n            TO_CHAR(TO_TIMESTAMP(timestamp_ns / 1000000000.0), 'YYYY') || '_m' ||\n            TO_CHAR(TO_TIMESTAMP(timestamp_ns / 1000000000.0), 'MM') || '_d' ||\n            TO_CHAR(TO_TIMESTAMP(timestamp_ns / 1000000000.0), 'DD')\n    ) AS partition_exists\nFROM (\n    SELECT 'spans_fact' AS table_name, 1737478800000000000::BIGINT AS timestamp_ns\n) AS test;\n</code></pre> <p>Fix: Create missing partition manually</p> <pre><code>-- Create partition for specific date\nSELECT create_partitions_for_range(\n    'spans_fact',\n    EXTRACT(EPOCH FROM '2026-01-21'::TIMESTAMP)::BIGINT * 1000000000,\n    EXTRACT(EPOCH FROM '2026-01-22'::TIMESTAMP)::BIGINT * 1000000000,\n    24\n);\n</code></pre>"},{"location":"partition-management/#cronjob-not-running","title":"CronJob Not Running","text":"<p>Diagnosis:</p> <pre><code># Check CronJob status\nkubectl get cronjob -n ollyscale partition-maintenance\n\n# Check recent jobs\nkubectl get jobs -n ollyscale -l app.kubernetes.io/component=partition-maintenance\n\n# Check logs\nkubectl logs -n ollyscale -l app.kubernetes.io/component=partition-maintenance\n</code></pre> <p>Fix: Trigger manual job</p> <pre><code>kubectl create job -n ollyscale \\\n    partition-maintenance-manual \\\n    --from=cronjob/ollyscale-partition-maintenance\n</code></pre>"},{"location":"partition-management/#partition-cleanup-not-working","title":"Partition Cleanup Not Working","text":"<p>Symptom: Old partitions not being dropped</p> <p>Diagnosis:</p> <pre><code>-- Check retention policy\nSELECT * FROM retention_policy;\n\n-- Manually test drop function\nSELECT * FROM drop_old_partitions('spans_fact', 'traces', 'default');\n</code></pre> <p>Fix: Verify retention policy is correct</p> <pre><code>-- Update retention policy if needed\nUPDATE retention_policy\nSET retention_days_value = 7\nWHERE signal_type_value = 'traces' AND tenant_id = 'default';\n</code></pre>"},{"location":"partition-management/#manual-operations","title":"Manual Operations","text":""},{"location":"partition-management/#create-partitions-for-specific-date-range","title":"Create Partitions for Specific Date Range","text":"<pre><code>-- Create partitions for January 2026\nSELECT create_partitions_for_range(\n    'spans_fact',\n    EXTRACT(EPOCH FROM '2026-01-01'::TIMESTAMP)::BIGINT * 1000000000,\n    EXTRACT(EPOCH FROM '2026-02-01'::TIMESTAMP)::BIGINT * 1000000000,\n    24\n);\n</code></pre>"},{"location":"partition-management/#drop-specific-partition","title":"Drop Specific Partition","text":"<pre><code>-- Drop partition for specific date\nDROP TABLE IF EXISTS spans_fact_y2026_m01_d15;\n</code></pre>"},{"location":"partition-management/#disable-automatic-maintenance","title":"Disable Automatic Maintenance","text":"<pre><code># Edit values.yaml\nhelm upgrade ollyscale ./charts/ollyscale \\\n    --namespace ollyscale \\\n    --set partitionMaintenance.enabled=false\n</code></pre>"},{"location":"partition-management/#best-practices","title":"Best Practices","text":"<ol> <li>Monitor partition coverage - Ensure partitions exist for current + N days ahead</li> <li>Check CronJob logs - Review daily maintenance logs for errors</li> <li>Set appropriate retention - Balance storage costs vs. query needs</li> <li>Test manually first - Run maintenance functions manually before relying on CronJob</li> <li>Plan for time zones - Partitions use UTC, coordinate with query patterns</li> <li>Pre-create partitions - Always maintain buffer of future partitions (7+ days)</li> </ol>"},{"location":"partition-management/#integration-with-other-components","title":"Integration with Other Components","text":""},{"location":"partition-management/#migration-job","title":"Migration Job","text":"<p>The migration job creates partition management functions during deployment:</p> <ul> <li>Runs <code>alembic upgrade head</code> as Helm pre-upgrade/pre-install hook</li> <li>Creates functions: <code>create_partitions_for_range()</code>, <code>drop_old_partitions()</code>, etc.</li> <li>Creates initial 7 days of partitions</li> </ul>"},{"location":"partition-management/#ingestion-pipeline","title":"Ingestion Pipeline","text":"<p>The OTLP receiver and frontend API insert data into fact tables:</p> <ul> <li>PostgreSQL automatically routes rows to correct partition based on timestamp</li> <li>If no partition exists, insert fails with error</li> <li>Maintenance CronJob ensures partitions always exist</li> </ul>"},{"location":"partition-management/#retention-policy","title":"Retention Policy","text":"<p>Partition cleanup respects the <code>retention_policy</code> table:</p> tenant_id signal_type_value retention_days_value default traces 7 default logs 3 default metrics 30 <p>See ollyscale-v2-postgres.md for details.</p>"},{"location":"partition-management/#references","title":"References","text":"<ul> <li>PostgreSQL Table Partitioning</li> <li>ollyscale v2 Architecture</li> <li>Postgres Infrastructure</li> <li>Migration Strategy</li> </ul>"},{"location":"postgres-infrastructure/","title":"PostgreSQL Infrastructure for ollyScale v2","text":"<p>This document describes the PostgreSQL infrastructure setup for ollyScale v2.</p>"},{"location":"postgres-infrastructure/#architecture","title":"Architecture","text":""},{"location":"postgres-infrastructure/#components","title":"Components","text":"<ol> <li>Zalando Postgres Operator - Deploys and manages PostgreSQL clusters</li> <li>Location: <code>.kind/modules/main/argocd-applications/middleware/postgres-operator.yaml</code></li> <li>Namespace: <code>postgres-operator</code></li> <li>Sync Wave: 30 (after Strimzi at wave 20, before Redis at wave 35)</li> <li>Chart: <code>https://opensource.zalando.com/postgres-operator/charts/postgres-operator</code></li> <li> <p>Version: 1.14.0</p> </li> <li> <p>PostgreSQL Database Cluster - HA database for ollyScale</p> </li> <li>Location: <code>charts/ollyscale/templates/postgresql.yaml</code></li> <li>Namespace: <code>ollyscale</code></li> <li>Managed by: Zalando Postgres Operator</li> <li>Features:<ul> <li>2 replicas with streaming replication</li> <li>Synchronous replication (patroni)</li> <li>Connection pooler (PgBouncer) for efficient connection management</li> <li>Automatic failover via Patroni</li> </ul> </li> </ol>"},{"location":"postgres-infrastructure/#deployment-order","title":"Deployment Order","text":"<ol> <li>postgres-operator (sync-wave: 30) - Installs CRDs and operator</li> <li>ollyscale chart - Deploys Postgresql CR, creates database cluster</li> </ol>"},{"location":"postgres-infrastructure/#configuration","title":"Configuration","text":""},{"location":"postgres-infrastructure/#operator-configuration","title":"Operator Configuration","text":"<pre><code># .kind/modules/main/argocd-applications/middleware/postgres-operator.yaml\nconfigKubernetes:\n  enable_pod_antiaffinity: true          # Spread replicas across nodes\n  enable_pod_disruption_budget: true     # Protect from disruptions\n  watched_namespace: \"*\"                  # Watch all namespaces\n</code></pre>"},{"location":"postgres-infrastructure/#database-configuration","title":"Database Configuration","text":"<pre><code># charts/ollyscale/values.yaml\npostgresql:\n  teamId: ollyscale\n  numberOfInstances: 2                    # Primary + 1 replica\n  volume:\n    size: 10Gi\n    storageClass: ''                      # Use default StorageClass\n  databases:\n    ollyscale: ollyscale                  # database: owner\n  users:\n    ollyscale:\n      - superuser\n      - createdb\n  postgresql:\n    version: '18'\n  resources:\n    requests:\n      cpu: 100m\n      memory: 256Mi\n    limits:\n      cpu: 1000m\n      memory: 1Gi\n  patroni:\n    synchronous_mode: true                # Sync replication for durability\n    synchronous_mode_strict: false        # Allow async if replica unavailable\n</code></pre>"},{"location":"postgres-infrastructure/#connection-details","title":"Connection Details","text":""},{"location":"postgres-infrastructure/#from-frontend-application","title":"From Frontend Application","text":"<p>Both the frontend API and OTLP receiver automatically receive these environment variables:</p> <pre><code>DATABASE_HOST=ollyscale-db-pooler        # PgBouncer connection pooler\nDATABASE_PORT=5432\nDATABASE_NAME=ollyscale\nDATABASE_USER=&lt;from secret&gt;              # Auto-generated by operator\nDATABASE_PASSWORD=&lt;from secret&gt;          # Auto-generated by operator\n</code></pre>"},{"location":"postgres-infrastructure/#secret-name-pattern","title":"Secret Name Pattern","text":"<p>The operator creates a secret with credentials:</p> <pre><code>ollyscale.&lt;chart-name&gt;-db.credentials.postgresql.acid.zalan.do\n</code></pre> <p>Keys:</p> <ul> <li><code>username</code> - Database username (ollyscale)</li> <li><code>password</code> - Auto-generated password</li> </ul>"},{"location":"postgres-infrastructure/#connection-string","title":"Connection String","text":"<pre><code># SQLAlchemy connection string\nDATABASE_URL = (\n    f\"postgresql+asyncpg://{DATABASE_USER}:{DATABASE_PASSWORD}@\"\n    f\"{DATABASE_HOST}:{DATABASE_PORT}/{DATABASE_NAME}\"\n)\n</code></pre>"},{"location":"postgres-infrastructure/#services-created","title":"Services Created","text":"<ol> <li>ollyscale-db - Direct PostgreSQL service (for admin)</li> <li>Port: 5432</li> <li> <p>Endpoints: Primary and replica pods</p> </li> <li> <p>ollyscale-db-pooler - PgBouncer connection pooler (for apps)</p> </li> <li>Port: 5432</li> <li>Mode: transaction</li> <li> <p>Replicas: 2</p> </li> <li> <p>ollyscale-db-replica - Read-only replica service</p> </li> <li>Port: 5432</li> <li>Endpoints: Replica pods only</li> </ol>"},{"location":"postgres-infrastructure/#deployment","title":"Deployment","text":""},{"location":"postgres-infrastructure/#deploy-operator","title":"Deploy Operator","text":"<pre><code>cd .kind\nterraform apply -target='kubectl_manifest.middleware_applications[\"middleware/postgres-operator.yaml\"]'\n</code></pre>"},{"location":"postgres-infrastructure/#deploy-database","title":"Deploy Database","text":"<pre><code>cd charts\nhelm upgrade --install ollyscale ./ollyscale \\\n  --namespace ollyscale \\\n  --create-namespace\n</code></pre> <p>Migration Strategy: Database schema migrations run automatically as a Kubernetes Job:</p> <ul> <li>Helm hook: <code>pre-upgrade,pre-install</code> (runs before deployment updates)</li> <li>Hook weight: <code>-5</code> (runs early in the upgrade sequence)</li> <li>Uses same image as frontend but executes <code>alembic upgrade head</code></li> <li>Runs exactly once per deployment (no race conditions between replicas)</li> <li>Failed migrations block the rollout (safety mechanism)</li> <li>Auto-cleanup after 5 minutes on success</li> </ul>"},{"location":"postgres-infrastructure/#verify-deployment","title":"Verify Deployment","text":"<pre><code># Check operator\nkubectl get pods -n postgres-operator\n\n# Check migration job (only exists during upgrade)\nkubectl get jobs -n ollyscale | grep migration\n\n# Check database cluster\nkubectl get postgresql -n ollyscale\nkubectl get pods -n ollyscale -l application=spilo\n\n# Check services\nkubectl get svc -n ollyscale | grep db\n\n# Get credentials\nkubectl get secret -n ollyscale \\\n  ollyscale.ollyscale-db.credentials.postgresql.acid.zalan.do \\\n  -o jsonpath='{.data.username}' | base64 -d\nkubectl get secret -n ollyscale \\\n  ollyscale.ollyscale-db.credentials.postgresql.acid.zalan.do \\\n  -o jsonpath='{.data.password}' | base64 -d\n</code></pre>"},{"location":"postgres-infrastructure/#testing-connection","title":"Testing Connection","text":"<pre><code># Port-forward to pooler\nkubectl port-forward -n ollyscale svc/ollyscale-db-pooler 5432:5432\n\n# Connect with psql\npsql -h localhost -U ollyscale -d ollyscale\n</code></pre>"},{"location":"postgres-infrastructure/#high-availability","title":"High Availability","text":""},{"location":"postgres-infrastructure/#automatic-failover","title":"Automatic Failover","text":"<p>Patroni manages automatic failover:</p> <ol> <li>Health checks detect primary failure</li> <li>Replica promoted to primary (&lt; 30 seconds)</li> <li>Services updated to point to new primary</li> <li>Failed pod restarted as replica</li> </ol>"},{"location":"postgres-infrastructure/#synchronous-replication","title":"Synchronous Replication","text":"<ul> <li><code>synchronous_mode: true</code> - Writes wait for replica ACK</li> <li><code>synchronous_mode_strict: false</code> - Allow async if replica down (availability over consistency)</li> </ul>"},{"location":"postgres-infrastructure/#connection-pooling","title":"Connection Pooling","text":"<p>PgBouncer provides:</p> <ul> <li>Connection reuse (reduces overhead)</li> <li>Transaction-level pooling</li> <li>2 pooler instances for HA</li> <li>Automatic load balancing</li> </ul>"},{"location":"postgres-infrastructure/#monitoring","title":"Monitoring","text":""},{"location":"postgres-infrastructure/#check-cluster-status","title":"Check Cluster Status","text":"<pre><code># Patroni cluster info\nkubectl exec -n ollyscale ollyscale-db-0 -- \\\n  patronictl list\n\n# Replication status\nkubectl exec -n ollyscale ollyscale-db-0 -- \\\n  psql -U postgres -c \"SELECT * FROM pg_stat_replication;\"\n</code></pre>"},{"location":"postgres-infrastructure/#common-issues","title":"Common Issues","text":"<p>Issue: Pods stuck in Pending</p> <ul> <li>Check: <code>kubectl get events -n ollyscale</code></li> <li>Likely: Insufficient resources or missing StorageClass</li> </ul> <p>Issue: Database not created</p> <ul> <li>Check operator logs: <code>kubectl logs -n postgres-operator deployment/postgres-operator</code></li> <li>Check postgresql status: <code>kubectl describe postgresql -n ollyscale ollyscale-db</code></li> </ul> <p>Issue: Can't connect from frontend</p> <ul> <li>Verify secret exists: <code>kubectl get secret -n ollyscale | grep credentials</code></li> <li>Check pooler service: <code>kubectl get svc -n ollyscale ollyscale-db-pooler</code></li> <li>Check frontend logs: <code>kubectl logs -n ollyscale deployment/ollyscale-frontend</code></li> </ul>"},{"location":"postgres-infrastructure/#resource-management","title":"Resource Management","text":""},{"location":"postgres-infrastructure/#disk-usage","title":"Disk Usage","text":"<pre><code># Check PVC usage\nkubectl exec -n ollyscale ollyscale-db-0 -- df -h /home/postgres/pgdata\n</code></pre>"},{"location":"postgres-infrastructure/#scaling","title":"Scaling","text":"<pre><code># Scale to 3 replicas\nhelm upgrade ollyscale ./ollyscale \\\n  --namespace ollyscale \\\n  --set postgresql.numberOfInstances=3 \\\n  --reuse-values\n</code></pre>"},{"location":"postgres-infrastructure/#backup-and-restore","title":"Backup and Restore","text":""},{"location":"postgres-infrastructure/#manual-backup","title":"Manual Backup","text":"<pre><code># Backup via pg_dump\nkubectl exec -n ollyscale ollyscale-db-0 -- \\\n  pg_dump -U ollyscale ollyscale &gt; backup.sql\n</code></pre>"},{"location":"postgres-infrastructure/#automated-backups-future","title":"Automated Backups (Future)","text":"<p>The operator supports WAL-E/WAL-G for continuous archiving:</p> <pre><code>postgresql:\n  backup:\n    enabled: true\n    s3_bucket: ollyscale-backups\n    schedule: \"0 2 * * *\"\n</code></pre>"},{"location":"postgres-infrastructure/#security","title":"Security","text":""},{"location":"postgres-infrastructure/#network-policies-recommended","title":"Network Policies (Recommended)","text":"<pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: ollyscale-db-policy\nspec:\n  podSelector:\n    matchLabels:\n      application: spilo\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app.kubernetes.io/component: frontend\n    ports:\n    - protocol: TCP\n      port: 5432\n</code></pre>"},{"location":"postgres-infrastructure/#tls-encryption-future","title":"TLS Encryption (Future)","text":"<pre><code>postgresql:\n  tls:\n    secretName: ollyscale-db-tls\n    caSecret: ollyscale-ca\n</code></pre>"},{"location":"postgres-infrastructure/#references","title":"References","text":"<ul> <li>Zalando Postgres Operator Documentation</li> <li>Patroni Documentation</li> <li>PgBouncer Documentation</li> <li>PostgreSQL High Availability</li> </ul>"},{"location":"precommit/","title":"Pre-commit Configuration for ollyScale","text":"<p>This project uses pre-commit to maintain code quality and consistency across all languages and tools.</p>"},{"location":"precommit/#quick-start","title":"Quick Start","text":"<pre><code># Install and setup hooks\nmake precommit-setup\n\n# Or manually\npip install pre-commit\npre-commit install\n</code></pre>"},{"location":"precommit/#what-gets-checked","title":"What Gets Checked","text":""},{"location":"precommit/#python","title":"Python","text":"<ul> <li>ruff: Linting and formatting (replaces black, isort, flake8, pylint)</li> <li>Auto-fixes import sorting, formatting, and common issues</li> <li>Config: <code>pyproject.toml</code></li> </ul>"},{"location":"precommit/#yaml","title":"YAML","text":"<ul> <li>yamlfmt: Formatting and style consistency</li> <li>check-yaml: Syntax validation</li> <li>Config: <code>.yamlfmt</code></li> </ul>"},{"location":"precommit/#json","title":"JSON","text":"<ul> <li>prettier: Formatting</li> <li>check-json: Syntax validation</li> </ul>"},{"location":"precommit/#shell-scripts","title":"Shell Scripts","text":"<ul> <li>shellcheck: Linting for bash/sh scripts</li> <li>Catches common bugs and anti-patterns</li> </ul>"},{"location":"precommit/#docker","title":"Docker","text":"<ul> <li>hadolint: Dockerfile linting</li> <li>Enforces best practices (layer caching, security, maintainability)</li> <li>Config: <code>.hadolint.yaml</code></li> </ul>"},{"location":"precommit/#helm","title":"Helm","text":"<ul> <li>helmlint: Validates Helm chart structure and templates</li> <li>Runs on all charts in <code>charts/</code></li> </ul>"},{"location":"precommit/#go","title":"Go","text":"<ul> <li>golangci-lint: Comprehensive Go linting</li> <li>Auto-fixes when possible</li> </ul>"},{"location":"precommit/#markdown","title":"Markdown","text":"<ul> <li>markdownlint: Style and syntax checking</li> <li>Config: <code>.markdownlint.yaml</code></li> </ul>"},{"location":"precommit/#terraform","title":"Terraform","text":"<ul> <li>terraform fmt: Format HCL files</li> <li>terraform validate: Validate configuration</li> </ul>"},{"location":"precommit/#general","title":"General","text":"<ul> <li>Trailing whitespace removal</li> <li>End-of-file fixer</li> <li>Large file detection (&gt;1MB)</li> <li>Private key detection</li> <li>Merge conflict detection</li> </ul>"},{"location":"precommit/#usage","title":"Usage","text":""},{"location":"precommit/#automatic-recommended","title":"Automatic (Recommended)","text":"<p>Hooks run automatically on <code>git commit</code>. If checks fail, the commit is blocked:</p> <pre><code>git add .\ngit commit -m \"feat: add new feature\"\n# Pre-commit runs, auto-fixes issues, then commits\n</code></pre>"},{"location":"precommit/#manual","title":"Manual","text":"<pre><code># Run all checks\nmake lint\n# Or: pre-commit run --all-files\n\n# Run specific hook\npre-commit run ruff --all-files\npre-commit run hadolint-docker --all-files\n\n# Run on specific files\npre-commit run --files apps/frontend/main.py apps/demo/backend.py\n\n# Update hooks to latest versions\npre-commit autoupdate\n</code></pre>"},{"location":"precommit/#skip-hooks-not-recommended","title":"Skip Hooks (Not Recommended)","text":"<pre><code># Skip all hooks (emergency only)\ngit commit --no-verify -m \"hotfix\"\n\n# Skip specific files by adding to exclude in .pre-commit-config.yaml\n</code></pre>"},{"location":"precommit/#configuration-files","title":"Configuration Files","text":"<ul> <li><code>.pre-commit-config.yaml</code> - Main hook configuration</li> <li><code>pyproject.toml</code> - Python/ruff settings</li> <li><code>.hadolint.yaml</code> - Docker linting rules</li> <li><code>.markdownlint.yaml</code> - Markdown style rules</li> <li><code>.yamlfmt</code> - YAML formatting rules</li> </ul>"},{"location":"precommit/#troubleshooting","title":"Troubleshooting","text":""},{"location":"precommit/#hooks-fail-on-first-run","title":"Hooks fail on first run","text":"<p>This is normal. Pre-commit will auto-fix many issues. Run again:</p> <pre><code>git add .\ngit commit -m \"fix: apply pre-commit auto-fixes\"\n</code></pre>"},{"location":"precommit/#command-not-found-pre-commit","title":"\"command not found: pre-commit\"","text":"<pre><code>pip install pre-commit\npre-commit install\n</code></pre>"},{"location":"precommit/#hook-installation-fails","title":"Hook installation fails","text":"<pre><code># Clear cache and reinstall\npre-commit clean\npre-commit install --install-hooks\n</code></pre>"},{"location":"precommit/#specific-hook-keeps-failing","title":"Specific hook keeps failing","text":"<pre><code># Run in verbose mode for debugging\npre-commit run &lt;hook-id&gt; --all-files --verbose\n\n# Temporarily skip hook (not recommended)\nSKIP=&lt;hook-id&gt; git commit -m \"message\"\n</code></pre>"},{"location":"precommit/#adding-new-hooks","title":"Adding New Hooks","text":"<ol> <li>Find the hook at pre-commit.com hooks</li> <li>Add to <code>.pre-commit-config.yaml</code>:</li> </ol> <pre><code>- repo: https://github.com/org/repo\n  rev: v1.0.0\n  hooks:\n    - id: hook-name\n      args: [--option]\n</code></pre> <ol> <li>Install: <code>pre-commit install --install-hooks</code></li> <li>Test: <code>pre-commit run hook-name --all-files</code></li> </ol>"},{"location":"precommit/#ci-integration","title":"CI Integration","text":"<p>Pre-commit checks run in GitHub Actions workflows. See <code>.github/workflows/</code>.</p>"},{"location":"precommit/#performance","title":"Performance","text":"<ul> <li>First run: Slower (downloads and caches tools)</li> <li>Subsequent runs: Fast (only changed files)</li> <li>Full run: <code>pre-commit run --all-files</code> takes ~2-5 minutes</li> </ul>"},{"location":"precommit/#best-practices","title":"Best Practices","text":"<ol> <li>Run before pushing: <code>make lint</code> or <code>pre-commit run --all-files</code></li> <li>Commit auto-fixes separately: Let pre-commit fix, then review changes</li> <li>Keep hooks updated: <code>pre-commit autoupdate</code> monthly</li> <li>Don't skip hooks: They catch real issues before code review</li> <li>Fix root causes: If a hook fails repeatedly, fix the underlying issue</li> </ol>"},{"location":"precommit/#ruff-configuration-highlights","title":"Ruff Configuration Highlights","text":"<ul> <li>Target: Python 3.11+</li> <li>Line length: 120 characters</li> <li>Enabled checks:</li> <li>Code quality (pyflakes, pycodestyle)</li> <li>Import sorting (isort)</li> <li>Security (bandit subset)</li> <li>Complexity (pylint subset)</li> <li>Modern Python (pyupgrade)</li> <li>Auto-fix: Most issues fixed automatically</li> <li>Config: <code>pyproject.toml</code></li> </ul>"},{"location":"precommit/#docker-linting-highlights","title":"Docker Linting Highlights","text":"<ul> <li>Ignored rules:</li> <li>DL3008: Pin apt versions (handled by base images)</li> <li>DL3013: Pin pip versions (requirements.txt handles this)</li> <li>Trusted registries: docker.io, ghcr.io, gcr.io, registry.k8s.io</li> <li>Config: <code>.hadolint.yaml</code></li> </ul>"},{"location":"precommit/#support","title":"Support","text":"<ul> <li>Pre-commit docs: https://pre-commit.com</li> <li>Ruff docs: https://docs.astral.sh/ruff</li> <li>Hadolint docs: https://github.com/hadolint/hadolint</li> <li>Project issues: https://github.com/ryanfaircloth/ollyscale/issues</li> </ul>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Get ollyScale running in under 5 minutes!</p>"},{"location":"quickstart/#what-youll-get","title":"What You'll Get","text":"<ul> <li>ollyScale UI at <code>http://localhost:5005</code></li> <li>OpenTelemetry Collector listening on ports 4317 (gRPC) and 4318 (HTTP)</li> <li>OpAMP Server for remote collector configuration management</li> <li>Demo microservices generating automatic telemetry</li> </ul>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker Desktop installed and running</li> <li>Git (to clone the repository)</li> <li>5 minutes of your time</li> </ul>"},{"location":"quickstart/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/ryanfaircloth/ollyscale\ncd ollyscale\n</code></pre>"},{"location":"quickstart/#step-2-start-ollyscale-core","title":"Step 2: Start ollyScale Core","text":"<pre><code>cd docker\n./01-start-core.sh\n</code></pre> <p>This pulls pre-built images from Docker Hub and starts:</p> <ul> <li>OpenTelemetry Collector (ports 4317/4318)</li> <li>OpAMP Server (ports 4320/4321)</li> <li>ollyScale OTLP Receiver (internal)</li> <li>ollyScale UI (port 5005)</li> <li>Redis storage (internal)</li> </ul> <p>Deployment time: ~30 seconds (pulls from Docker Hub)</p> <p>For local development: Use <code>./01-start-core-local.sh</code> to build images locally.</p>"},{"location":"quickstart/#step-3-deploy-demo-apps-optional-but-recommended","title":"Step 3: Deploy Demo Apps (Optional but Recommended)","text":"<p>In a new terminal:</p> <pre><code>cd apps/demo\n./01-deploy-demo.sh\n</code></pre> <p>This pulls demo images from Docker Hub and deploys two Flask microservices that automatically generate traffic.</p> <p>Wait 30 seconds for telemetry to appear!</p> <p>For local development: Use <code>./01-deploy-demo-local.sh</code> to build images locally.</p>"},{"location":"quickstart/#step-4-open-the-ui","title":"Step 4: Open the UI","text":"<p>Open your browser to: <code>http://localhost:5005</code></p> <p>You should see:</p> <p>Trace waterfall with correlated logs and span timing</p>"},{"location":"quickstart/#step-5-explore-the-features","title":"Step 5: Explore the Features","text":""},{"location":"quickstart/#traces-tab","title":"Traces Tab","text":"<p>View distributed traces across microservices with timing waterfall.</p> <p>Span waterfall showing request timing breakdown with correlated logs</p> <p>Click on a span to view detailed JSON data:</p> <p>Span detail view with full OpenTelemetry attributes</p>"},{"location":"quickstart/#logs-tab","title":"Logs Tab","text":"<p>Browse logs with trace/span correlation. Filter by severity (Error, Warn, Info, Debug).</p> <p>Real-time logs with trace and span correlation</p> <p>Click on a log entry to view full details:</p> <p>Log detail view with full attributes and resource info</p> <p>Filter to show only errors:</p> <p>Filtered error logs with trace correlation</p>"},{"location":"quickstart/#metrics-tab","title":"Metrics Tab","text":"<p>Visualize metrics with automatic charting.</p> <p>Time-series metrics visualization with rate charts</p>"},{"location":"quickstart/#service-catalog","title":"Service Catalog","text":"<p>View all services with RED metrics (Rate, Errors, Duration).</p> <p>Service catalog with RED metrics for all services</p>"},{"location":"quickstart/#service-map","title":"Service Map","text":"<p>Visualize service dependencies with an interactive graph.</p> <p>Interactive service dependency map with latency information</p>"},{"location":"quickstart/#opentelemetry-collector-opamp-config","title":"OpenTelemetry Collector + OpAMP Config","text":"<p>View and manage your OpenTelemetry Collector configuration remotely via the OpAMP protocol.</p> <p>OpenTelemetry Collector configuration management via OpAMP</p> <p>This page allows you to:</p> <ul> <li>View current configuration from connected collectors</li> <li>Apply configuration changes with validation and diff preview</li> <li>Use configuration templates for common scenarios (default, prometheus-remote-write, etc.)</li> <li>Check OpAMP server status and see connected collector agents</li> <li>Validate configurations before applying to catch errors early</li> </ul> <p>To use this feature, your OpenTelemetry Collector must be configured with the OpAMP extension (see Docker Deployment or OpenTelemetry Collector documentation).</p>"},{"location":"quickstart/#step-6-use-your-own-application","title":"Step 6: Use Your Own Application","text":"<p>Point your application's OpenTelemetry exporter to:</p> <p>For apps running on your host machine (outside Docker):</p> <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318\n</code></pre> <p>For apps running inside Docker:</p> <pre><code>export OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318\n</code></pre> <p>ollyScale will automatically capture and display your telemetry!</p>"},{"location":"quickstart/#cleanup","title":"Cleanup","text":"<p>Stop demo apps (keeps ollyScale running):</p> <pre><code>cd apps/demo\n./02-cleanup-demo.sh\n</code></pre> <p>Stop everything:</p> <pre><code>cd docker\n./02-stop-core.sh\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Configure your own OpenTelemetry Collector</li> <li>Explore the REST API at <code>http://localhost:5005/docs</code></li> <li>Deploy on Kubernetes</li> <li>Learn about the architecture</li> </ul>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"quickstart/#ui-shows-no-traceslogsmetrics","title":"UI shows \"No traces/logs/metrics\"","text":"<ul> <li>Wait 30 seconds after starting demo apps</li> <li>Check containers are running: <code>docker ps</code></li> <li>Check demo app logs: <code>docker compose -f apps/demo/docker-compose-demo.yml logs</code></li> </ul>"},{"location":"quickstart/#port-conflicts","title":"Port conflicts","text":"<ul> <li>ollyScale uses ports 4317, 4318, 4320, 4321, 4343, 5005, 6379, 19291</li> <li>Stop conflicting services or modify ports in <code>docker-compose-ollyscale-core.yml</code></li> </ul>"},{"location":"quickstart/#demo-apps-not-generating-traffic","title":"Demo apps not generating traffic","text":"<ul> <li>Restart demo: <code>cd apps/demo &amp;&amp; ./02-cleanup-demo.sh &amp;&amp; ./01-deploy-demo.sh</code></li> <li>Check logs: <code>docker compose -f apps/demo/docker-compose-demo.yml logs demo-frontend</code></li> </ul> <p>For more help, open an issue on GitHub.</p>"},{"location":"release-migration/","title":"Migration from semantic-release to release-please","text":"<p>This document explains the migration from semantic-release to release-please and what changed.</p>"},{"location":"release-migration/#why-migrate","title":"Why Migrate?","text":"<p>The previous semantic-release system had challenges with:</p> <ul> <li>Complex multi-package coordination requiring custom plugins</li> <li>All components released together even when only one changed</li> <li>Difficulty managing cross-component dependencies</li> <li>Complex configuration spread across multiple <code>.releaserc.json</code> files</li> </ul> <p>The new release-please system provides:</p> <ul> <li>Native multi-package support - no special plugins needed</li> <li>Separate releases - each component releases independently</li> <li>bumpDependents feature - automatic version bumping when dependencies change</li> <li>Simpler configuration - single <code>release-please-config.json</code> file</li> <li>Better visibility - separate PRs for each component release</li> </ul>"},{"location":"release-migration/#what-changed","title":"What Changed","text":""},{"location":"release-migration/#configuration-files","title":"Configuration Files","text":"<p>Removed:</p> <ul> <li>Individual <code>.releaserc.json</code> files in <code>apps/*/</code> and <code>charts/*/</code> \u2705 COMPLETED</li> <li>Root <code>.releaserc.json</code> \u2705 COMPLETED</li> <li><code>.multi-releaserc.json</code> \u2705 COMPLETED</li> <li>All semantic-release dependencies from <code>package.json</code> \u2705 COMPLETED</li> <li><code>.github/workflows/semantic-release.yml</code> \u2705 COMPLETED</li> </ul> <p>Added:</p> <ul> <li><code>release-please-config.json</code> - Main configuration for all components</li> <li><code>.release-please-manifest.json</code> - Version tracking manifest</li> </ul>"},{"location":"release-migration/#github-workflows","title":"GitHub Workflows","text":"<p>Removed:</p> <ul> <li><code>.github/workflows/semantic-release.yml</code> - Completely removed \u2705 COMPLETED</li> </ul> <p>Added:</p> <ul> <li><code>.github/workflows/release-please.yml</code> - New release workflow supporting multiple branches</li> </ul>"},{"location":"release-migration/#version-management","title":"Version Management","text":"<p>Before (semantic-release):</p> <pre><code># All components released together when main is updated\ngit push origin main\n# semantic-release runs and releases everything with changes\n</code></pre> <p>After (release-please):</p> <pre><code># Commit with conventional commit format\ngit commit -m \"feat(ollyscale): add new API endpoint\"\ngit push origin main\n\n# release-please creates/updates PR for ollyscale component\n# Merge the PR to trigger the release\n</code></pre>"},{"location":"release-migration/#dependency-management","title":"Dependency Management","text":"<p>Before (semantic-release):</p> <ul> <li>Each component's <code>.releaserc.json</code> had <code>semantic-release-yaml</code> plugin</li> <li>Manually specified which values.yaml fields to update</li> <li>No automatic chart version bumping</li> </ul> <p>After (release-please):</p> <ul> <li>App components update their image tags in values.yaml via <code>extra-files</code></li> <li>Chart has <code>bumpDependents: true</code> entries that watch for these changes</li> <li>Chart version automatically bumps when any dependency changes</li> </ul>"},{"location":"release-migration/#migration-steps","title":"Migration Steps","text":""},{"location":"release-migration/#for-developers","title":"For Developers","text":"<ol> <li>Commit message format stays the same - Continue using Conventional Commits</li> <li>Review release PRs - release-please creates PRs that need to be reviewed and merged</li> <li>Watch for dependency bumps - Chart releases may happen automatically due to bumpDependents</li> </ol>"},{"location":"release-migration/#for-maintainers","title":"For Maintainers","text":"<ol> <li>Bootstrap versions - Initial versions set in <code>.release-please-manifest.json</code></li> <li>Monitor first releases - Verify the workflow works correctly</li> <li>Update documentation - Point to new release system docs</li> <li>Remove old configs - After successful migration, remove semantic-release configs</li> </ol>"},{"location":"release-migration/#configuration-details","title":"Configuration Details","text":""},{"location":"release-migration/#component-configuration","title":"Component Configuration","text":"<p>Each component in <code>release-please-config.json</code> has:</p> <pre><code>{\n  \"apps/ollyscale\": {\n    \"component\": \"ollyscale\",           // Tag prefix: ollyscale-v1.0.0\n    \"release-type\": \"python\",           // Handles Python version files\n    \"package-name\": \"@ollyscale/ollyscale\",\n    \"extra-files\": [                    // Additional files to update\n      {\n        \"type\": \"yaml\",\n        \"path\": \"charts/ollyscale/values.yaml\",\n        \"jsonpath\": \"$.frontend.image.tag\"\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"release-migration/#dependency-management_1","title":"Dependency Management","text":"<p>The chart configuration uses <code>bumpDependents</code>:</p> <pre><code>{\n  \"charts/ollyscale\": {\n    \"component\": \"chart-ollyscale\",\n    \"release-type\": \"helm\",\n    \"extra-files\": [\n      {\n        \"type\": \"yaml\",\n        \"path\": \"charts/ollyscale/values.yaml\",\n        \"jsonpath\": \"$.frontend.image.tag\",\n        \"component\": \"ollyscale\",       // Watch this component\n        \"bumpDependents\": true          // Bump chart when ollyscale changes\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"release-migration/#release-flow-comparison","title":"Release Flow Comparison","text":""},{"location":"release-migration/#semantic-release-flow","title":"semantic-release Flow","text":"<pre><code>1. Push to main\n2. semantic-release runs for all workspaces\n3. Each workspace checks for changes\n4. Versions bumped, CHANGELOGs updated\n5. Docker images built and pushed\n6. Git tags created\n7. GitHub releases created\n</code></pre> <p>Issues:</p> <ul> <li>All workspaces processed even if unchanged</li> <li>Single workflow must handle all build types</li> <li>Complex plugin configuration per workspace</li> <li>No dependency tracking between components</li> </ul>"},{"location":"release-migration/#release-please-flow","title":"release-please Flow","text":"<pre><code>1. Push to main\n2. release-please creates/updates PR per component with changes\n3. Review and merge PR\n4. release-please creates release tag\n5. Workflow triggered by release tag\n6. Build and publish component (Docker/Helm)\n7. GitHub release created with notes\n</code></pre> <p>Benefits:</p> <ul> <li>Only changed components get PRs</li> <li>Review releases before they happen</li> <li>Matrix strategy handles builds cleanly</li> <li>bumpDependents tracks component relationships</li> </ul>"},{"location":"release-migration/#example-scenarios","title":"Example Scenarios","text":""},{"location":"release-migration/#scenario-1-app-change","title":"Scenario 1: App Change","text":"<pre><code># Make a change to ollyscale backend\ngit commit -m \"feat(ollyscale): add trace filtering\"\ngit push origin main\n\n# Results:\n# 1. release-please creates PR for apps/ollyscale\n# 2. PR updates version to 2.2.0 in package.json\n# 3. PR updates frontend.image.tag in values.yaml to v2.2.0\n# 4. bumpDependents detects this change\n# 5. release-please also creates PR for charts/ollyscale\n# 6. Chart version bumps to 0.2.0\n\n# Merge both PRs to release\n</code></pre>"},{"location":"release-migration/#scenario-2-chart-only-change","title":"Scenario 2: Chart-Only Change","text":"<pre><code># Update chart templates\ngit commit -m \"feat(chart-ollyscale): add podDisruptionBudget\"\ngit push origin main\n\n# Results:\n# 1. release-please creates PR for charts/ollyscale only\n# 2. Chart version bumps to 0.2.0\n# No app component changes needed\n</code></pre>"},{"location":"release-migration/#scenario-3-multiple-app-changes","title":"Scenario 3: Multiple App Changes","text":"<pre><code># Make changes to UI and backend\ngit commit -m \"feat(ollyscale-ui): improve timeline view\"\ngit commit -m \"fix(ollyscale): handle null span attributes\"\ngit push origin main\n\n# Results:\n# 1. release-please creates PR for apps/ollyscale-ui\n# 2. release-please creates PR for apps/ollyscale\n# 3. release-please creates PR for charts/ollyscale (due to bumpDependents)\n# All can be reviewed and merged independently\n</code></pre>"},{"location":"release-migration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"release-migration/#release-pr-not-created","title":"Release PR not created","text":"<p>Check:</p> <ol> <li>Commits follow Conventional Commit format</li> <li>Commits are in correct component directory</li> <li>Commits pushed to <code>main</code> branch</li> <li><code>.release-please-manifest.json</code> has entry for component</li> </ol>"},{"location":"release-migration/#chart-not-bumped-when-app-changes","title":"Chart not bumped when app changes","text":"<p>Check:</p> <ol> <li>App component has <code>extra-files</code> entry updating <code>values.yaml</code></li> <li>Chart has matching <code>bumpDependents: true</code> entry</li> <li>Component name matches exactly in both places</li> </ol>"},{"location":"release-migration/#build-fails-after-release","title":"Build fails after release","text":"<p>Check:</p> <ol> <li>Dockerfile exists and is valid</li> <li>GitHub token has packages:write permission</li> <li>Image name in workflow matches ghcr.io registry</li> </ol>"},{"location":"release-migration/#rollback-plan","title":"Rollback Plan","text":"<p>If the migration needs to be rolled back:</p> <ol> <li>Re-enable semantic-release workflow:</li> </ol> <pre><code># Remove \"if: false\" from .github/workflows/semantic-release.yml\n</code></pre> <ol> <li>Restore <code>.releaserc.json</code> files from git history:</li> </ol> <pre><code>git checkout &lt;previous-commit&gt; -- apps/*/.releaserc.json charts/*/.releaserc.json\n</code></pre> <ol> <li>Remove release-please configuration:</li> </ol> <pre><code>git rm release-please-config.json .release-please-manifest.json\ngit rm .github/workflows/release-please.yml\n</code></pre>"},{"location":"release-migration/#questions","title":"Questions?","text":"<p>See docs/release-system.md for complete documentation or open an issue on GitHub.</p>"},{"location":"release-process/","title":"ollyScale Release Process","text":"<p>ollyScale uses release-please for automated releases in our monorepo.</p>"},{"location":"release-process/#how-it-works","title":"How It Works","text":"<ol> <li>Make changes using Conventional Commits</li> <li>Merge to main - release-please creates/updates a release PR automatically</li> <li>Review release PR - check versions, changelogs, and changes</li> <li>Merge release PR - triggers automated builds and releases</li> </ol>"},{"location":"release-process/#conventional-commit-format","title":"Conventional Commit Format","text":"<pre><code>&lt;type&gt;(&lt;scope&gt;): &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre>"},{"location":"release-process/#types-determine-version-bump","title":"Types (determine version bump)","text":"<ul> <li><code>feat:</code> - New feature (minor version bump)</li> <li><code>fix:</code> - Bug fix (patch version bump)</li> <li><code>perf:</code> - Performance improvement (patch version bump)</li> <li><code>docs:</code> - Documentation only (no release)</li> <li><code>style:</code> - Code style changes (no release)</li> <li><code>refactor:</code> - Code refactoring (patch version bump if in scope)</li> <li><code>test:</code> - Test changes (no release)</li> <li><code>build:</code> - Build system changes (no release)</li> <li><code>ci:</code> - CI configuration changes (no release)</li> <li><code>chore:</code> - Other changes (no release)</li> </ul> <p>Breaking Changes: Add <code>!</code> after type or <code>BREAKING CHANGE:</code> in footer for major version bump</p>"},{"location":"release-process/#scopes-optional-but-recommended","title":"Scopes (optional but recommended)","text":"<p>Container Components:</p> <ul> <li><code>ollyscale</code> - Core platform (apps/frontend)</li> <li><code>opamp</code> - OpAMP server (apps/opamp-server)</li> <li><code>demo</code> - Demo application (apps/demo)</li> <li><code>ai-agent</code> - AI agent demo (apps/ai-agent-demo)</li> </ul> <p>Helm Charts:</p> <ul> <li><code>helm/ollyscale</code> - Main platform chart</li> <li><code>helm/demos</code> - Demos chart</li> <li><code>helm/ai-agent</code> - AI agent chart</li> </ul> <p>General:</p> <ul> <li><code>deps</code> - Dependency updates</li> <li><code>*</code> - Multiple components (use sparingly)</li> </ul>"},{"location":"release-process/#examples","title":"Examples","text":""},{"location":"release-process/#container-changes","title":"Container Changes","text":"<pre><code># New feature in ollyscale (minor bump: 2.2.2 \u2192 2.3.0)\nfeat(ollyscale): add GenAI span filtering\n\n# Bug fix in opamp-server (patch bump: 1.0.0 \u2192 1.0.1)\nfix(opamp): handle nil pointer in config validation\n\n# Performance improvement (patch bump)\nperf(demo): optimize database query for metrics\n\n# Breaking change (major bump: 0.3.0 \u2192 1.0.0)\nfeat(ai-agent)!: migrate to Ollama 2.0 API\n\nBREAKING CHANGE: Ollama 1.x no longer supported\n</code></pre>"},{"location":"release-process/#helm-chart-changes","title":"Helm Chart Changes","text":"<pre><code># New chart feature (minor bump)\nfeat(helm/ollyscale): add PVC for persistent storage\n\n# Chart bug fix (patch bump)\nfix(helm/demos): correct service port configuration\n\n# Chart breaking change (major bump)\nfeat(helm/ollyscale)!: require Kubernetes 1.28+\n\nBREAKING CHANGE: Dropped support for Kubernetes &lt; 1.28\n</code></pre>"},{"location":"release-process/#multi-component-changes","title":"Multi-Component Changes","text":"<pre><code># Affects both containers and triggers chart update\nfeat(ollyscale,opamp): add mutual TLS authentication\n\n# Multiple charts\ndocs(helm/ollyscale,helm/demos): update README with examples\n</code></pre>"},{"location":"release-process/#no-release-examples","title":"No Release Examples","text":"<pre><code># Documentation only\ndocs: update API documentation\n\n# CI changes\nci: add workflow for dependency updates\n\n# Test changes\ntest(ollyscale): add integration tests for filtering\n\n# Build changes\nbuild: update base Python image to 3.12\n</code></pre>"},{"location":"release-process/#release-pr-workflow","title":"Release PR Workflow","text":""},{"location":"release-process/#when-changes-are-pushed-to-main","title":"When Changes Are Pushed to Main","text":"<ol> <li>Release-please analyzes commits since last release</li> <li>Determines which components need version bumps</li> <li>Creates/updates a single release PR with:</li> <li>Updated <code>.release-please-manifest.json</code> (single source of truth)</li> <li>Updated CHANGELOG.md for each component</li> <li>Updated Chart.yaml versions for Helm charts</li> <li>All changes in one PR</li> </ol>"},{"location":"release-process/#example-release-pr","title":"Example Release PR","text":"<pre><code>Title: chore: release main\n\nComponents to be released:\n- ollyscale: 2.2.2 \u2192 2.3.0\n- opamp-server: 1.0.0 \u2192 1.0.1\n- helm-ollyscale: 0.1.1 \u2192 0.2.0\n\nChanges:\n- Updated apps/frontend/VERSION\n- Updated apps/frontend/CHANGELOG.md\n- Updated apps/opamp-server/VERSION\n- Updated apps/opamp-server/CHANGELOG.md\n- Updated charts/ollyscale/Chart.yaml\n- Updated charts/ollyscale/CHANGELOG.md\n</code></pre>"},{"location":"release-process/#when-release-pr-is-merged","title":"When Release PR Is Merged","text":"<ol> <li>GitHub releases created for each component with tags:</li> <li><code>ollyscale-v2.3.0</code></li> <li><code>opamp-server-v1.0.1</code></li> <li> <p><code>helm-ollyscale-v0.2.0</code></p> </li> <li> <p>Container images built and pushed:</p> </li> <li><code>ghcr.io/ollyscale/ollyscale:v2.3.0</code> (+ <code>:latest</code>)</li> <li> <p><code>ghcr.io/ollyscale/opamp-server:v1.0.1</code> (+ <code>:latest</code>)</p> </li> <li> <p>Helm charts packaged and pushed:</p> </li> <li><code>oci://ghcr.io/ollyscale/charts/ollyscale:0.2.0</code></li> <li>Chart's <code>values.yaml</code> updated with new image versions</li> </ol>"},{"location":"release-process/#version-strategy","title":"Version Strategy","text":""},{"location":"release-process/#container-images","title":"Container Images","text":"<p>Each container maintains independent semantic versioning:</p> <pre><code>ollyscale:         v2.3.0\nopamp-server:     v1.0.1\ndemo:             v0.5.2\nai-agent-demo:    v0.3.1\n</code></pre>"},{"location":"release-process/#helm-charts","title":"Helm Charts","text":"<p>Charts have two version fields:</p> <ul> <li><code>version</code>: Chart's semantic version (no 'v' prefix)</li> <li><code>appVersion</code>: Version of primary application</li> </ul> <p>Release triggers:</p> <ul> <li>Direct changes to chart files</li> <li>Dependency container version bumps</li> <li>Manual version bump in manifest</li> </ul>"},{"location":"release-process/#local-development","title":"Local Development","text":"<p>Local development builds bypass release-please:</p> <pre><code># Build for local KIND cluster (no version tracking)\ncd charts\n./build-and-push-local.sh v2.3.0-my-feature\n\n# Or use existing build scripts\ncd scripts/build\n./02-build-all.sh local-test\n</code></pre> <p>Local builds:</p> <ul> <li>Use custom version tags (e.g., <code>local-</code>, feature names)</li> <li>Push only to local registry</li> <li>Don't update manifest (versions managed by release-please only)</li> <li>Don't trigger releases</li> </ul>"},{"location":"release-process/#manual-version-bumps","title":"Manual Version Bumps","text":"<p>If you need to force a version bump without code changes:</p> <pre><code># Update manifest file\nvim .release-please-manifest.json\n\n# Change version for component:\n{\n  \"apps/frontend\": \"2.3.0\",  # Bump this\n  ...\n}\n\n# Commit with chore type (won't trigger another bump)\ngit add .release-please-manifest.json\ngit commit -m \"chore: bump ollyscale to v2.3.0\"\ngit push origin main\n</code></pre>"},{"location":"release-process/#coordinated-releases","title":"Coordinated Releases","text":""},{"location":"release-process/#scenario-both-ollyscale-and-opamp-server-updated","title":"Scenario: Both ollyscale and opamp-server updated","text":"<pre><code># Commit 1\nfeat(ollyscale): add TLS support\n\n# Commit 2\nfeat(opamp): add TLS configuration endpoint\n\n# Result: Single release PR with:\n# - ollyscale: 2.2.2 \u2192 2.3.0\n# - opamp-server: 1.0.0 \u2192 1.1.0\n# - helm-ollyscale: 0.1.1 \u2192 0.2.0 (dependency bump)\n</code></pre> <p>Release-please automatically:</p> <ul> <li>Bumps both containers</li> <li>Bumps helm chart once (not twice)</li> <li>Updates chart's values.yaml with both new image versions</li> </ul>"},{"location":"release-process/#troubleshooting","title":"Troubleshooting","text":""},{"location":"release-process/#release-pr-not-created","title":"Release PR not created","text":"<ul> <li>Check commit messages follow conventional commits</li> <li>Verify commits have releasable types (<code>feat</code>, <code>fix</code>, <code>perf</code>)</li> <li>Check <code>.release-please-manifest.json</code> has correct initial versions</li> </ul>"},{"location":"release-process/#wrong-version-bump","title":"Wrong version bump","text":"<ul> <li>Check commit type (<code>feat</code> = minor, <code>fix</code> = patch)</li> <li>Verify <code>BREAKING CHANGE</code> in footer for major bumps</li> <li>Look at release PR description for bump reasoning</li> </ul>"},{"location":"release-process/#container-not-building-after-release","title":"Container not building after release","text":"<ul> <li>Check workflow logs in Actions tab</li> <li>Verify Dockerfile exists and is correct</li> <li>Check GHCR permissions</li> </ul>"},{"location":"release-process/#helm-chart-has-wrong-image-versions","title":"Helm chart has wrong image versions","text":"<ul> <li>Check <code>.release-please-manifest.json</code> for correct versions</li> <li>Verify <code>values.yaml</code> structure matches workflow expectations</li> <li>Ensure workflow's <code>yq</code> commands reference manifest correctly</li> </ul>"},{"location":"release-process/#best-practices","title":"Best Practices","text":"<ol> <li>One logical change per commit - easier to review and revert</li> <li>Use descriptive scopes - helps track which components changed</li> <li>Write clear descriptions - these become CHANGELOG entries</li> <li>Test before merging to main - releases are automatic</li> <li>Review release PR carefully - check all versions and changelogs</li> <li>Keep breaking changes to majors - minimize disruption</li> </ol>"},{"location":"release-process/#tools","title":"Tools","text":""},{"location":"release-process/#commitlint-optional","title":"Commitlint (optional)","text":"<p>Enforce conventional commits with pre-commit hooks:</p> <pre><code>npm install -g @commitlint/cli @commitlint/config-conventional\necho \"module.exports = {extends: ['@commitlint/config-conventional']}\" &gt; commitlint.config.js\n\n# Add to .git/hooks/commit-msg or use husky\n</code></pre>"},{"location":"release-process/#commit-message-templates","title":"Commit Message Templates","text":"<pre><code># Set git template\ngit config commit.template .gitmessage\n\n# .gitmessage content:\n# &lt;type&gt;(&lt;scope&gt;): &lt;subject&gt;\n#\n# &lt;body&gt;\n#\n# &lt;footer&gt;\n#\n# Types: feat, fix, docs, style, refactor, perf, test, build, ci, chore\n# Scopes: ollyscale, opamp, demo, ai-agent, helm/ollyscale, helm/demos, helm/ai-agent\n</code></pre>"},{"location":"release-process/#migration-from-current-process","title":"Migration from Current Process","text":"<ol> <li>\u2705 Release-please configured</li> <li>\u2705 VERSION files removed (manifest is single source of truth)</li> <li>\u2705 Manifest initialized with current versions</li> <li>\u2705 GitHub Actions workflow created</li> <li>\u2705 Team documentation updated</li> <li>\u2705 Conventional commits in use</li> <li>\u2705 Multiple successful releases completed</li> <li>\u2705 Old release workflow deprecated</li> </ol>"},{"location":"release-process/#references","title":"References","text":"<ul> <li>Release Please Documentation</li> <li>Conventional Commits Spec</li> <li>Semantic Versioning</li> <li>Keep a Changelog</li> </ul>"},{"location":"release-system/","title":"Release System Documentation","text":""},{"location":"release-system/#overview","title":"Overview","text":"<p>This project uses release-please with a custom fork that supports the <code>bumpDependents</code> feature for managing cross-component version dependencies.</p>"},{"location":"release-system/#architecture","title":"Architecture","text":""},{"location":"release-system/#components","title":"Components","text":"<p>The repository contains multiple components that are released independently:</p> <p>Applications (Container Images):</p> <ul> <li><code>apps/frontend</code> - Python backend \u2192 <code>ghcr.io/ryanfaircloth/ollyscale/ollyscale</code></li> <li><code>apps/ollyscale-ui</code> - TypeScript frontend \u2192 <code>ghcr.io/ryanfaircloth/ollyscale/ollyscale-ui</code></li> <li><code>apps/opamp-server</code> - Go OpAMP server \u2192 <code>ghcr.io/ryanfaircloth/ollyscale/opamp-server</code></li> <li><code>apps/demo</code> - Demo application \u2192 <code>ghcr.io/ryanfaircloth/ollyscale/demo</code></li> <li><code>apps/demo-otel-agent</code> - Demo OTel agent \u2192 <code>ghcr.io/ryanfaircloth/ollyscale/demo-otel-agent</code></li> </ul> <p>Helm Charts:</p> <ul> <li><code>charts/ollyscale</code> - Main application chart \u2192 <code>ghcr.io/ryanfaircloth/ollyscale/charts/ollyscale</code></li> <li><code>charts/ollyscale-demos</code> - Demo charts \u2192 <code>ghcr.io/ryanfaircloth/ollyscale/charts/ollyscale-demos</code></li> <li><code>charts/ollyscale-otel-agent</code> - OTel agent chart \u2192 <code>ghcr.io/ryanfaircloth/ollyscale/charts/ollyscale-otel-agent</code></li> </ul>"},{"location":"release-system/#dependency-management-with-bumpdependents","title":"Dependency Management with bumpDependents","text":"<p>The <code>bumpDependents</code> feature automatically bumps the Helm chart version when any of its application dependencies are released. This is configured in <code>release-please-config.json</code>:</p> <pre><code>{\n  \"charts/ollyscale\": {\n    \"extra-files\": [\n      {\n        \"type\": \"yaml\",\n        \"path\": \"charts/ollyscale/values.yaml\",\n        \"jsonpath\": \"$.frontend.image.tag\",\n        \"component\": \"ollyscale\",\n        \"bumpDependents\": true\n      }\n    ]\n  }\n}\n</code></pre> <p>When <code>apps/frontend</code> gets a new release:</p> <ol> <li>The app's version is bumped (e.g., <code>v2.1.9</code> \u2192 <code>v2.1.10</code>)</li> <li>The app updates <code>charts/ollyscale/values.yaml</code> with <code>frontend.image.tag: v2.1.10</code></li> <li>The chart detects this change via <code>bumpDependents: true</code></li> <li>The chart version is automatically bumped (e.g., <code>0.1.0</code> \u2192 <code>0.1.1</code>)</li> </ol>"},{"location":"release-system/#release-workflow","title":"Release Workflow","text":""},{"location":"release-system/#supported-branches","title":"Supported Branches","text":"<p>The release-please workflow triggers on pushes to these branches:</p> <ul> <li><code>main</code> - Stable releases (Docker images tagged with <code>latest</code>)</li> <li><code>develop</code> - Pre-release builds</li> <li><code>next-release</code> - Pre-release builds for minor version testing</li> <li><code>next-release-major</code> - Pre-release builds for major version testing</li> </ul> <p>Pre-release behavior:</p> <ul> <li>All branches except <code>main</code> produce pre-release builds</li> <li>Pre-release Docker images do NOT get the <code>latest</code> tag</li> <li>GitHub releases marked as pre-releases</li> <li>Images labeled with <code>org.opencontainers.image.prerelease=true</code></li> </ul>"},{"location":"release-system/#automatic-releases","title":"Automatic Releases","text":"<ol> <li>Commit with Conventional Commit messages to any supported branch:</li> <li><code>feat:</code> - triggers a minor version bump (0.x.0)</li> <li><code>fix:</code> - triggers a patch version bump (0.0.x)</li> <li> <p><code>feat!:</code> or <code>BREAKING CHANGE:</code> - triggers a major version bump (x.0.0)</p> </li> <li> <p>release-please creates/updates PRs for each component that has unreleased changes:</p> </li> <li>One PR per component (separate-pull-requests: true)</li> <li>Includes CHANGELOG updates</li> <li> <p>Updates version in package files</p> </li> <li> <p>Merge the release PR to trigger the release:</p> </li> <li>Builds and pushes Docker images (multi-platform: amd64/arm64)</li> <li>Packages and publishes Helm charts to OCI registry</li> <li>Creates GitHub releases with notes</li> </ol>"},{"location":"release-system/#manual-pre-releases","title":"Manual Pre-releases","text":"<p>For testing releases before production:</p> <pre><code># Trigger via GitHub UI or gh CLI on any branch\ngh workflow run release-please.yml -f prerelease=true\n</code></pre> <p>Pre-releases:</p> <ul> <li>Tagged as pre-release in GitHub</li> <li>Docker images do NOT get the <code>latest</code> tag</li> <li>Useful for testing in staging environments</li> </ul>"},{"location":"release-system/#configuration-files","title":"Configuration Files","text":""},{"location":"release-system/#release-please-configjson","title":"release-please-config.json","text":"<p>Main configuration file defining:</p> <ul> <li>Component locations (<code>packages</code>)</li> <li>Release types (python, node, helm, simple)</li> <li>Extra files to update (version files, image tags)</li> <li>Dependency relationships (<code>bumpDependents</code>)</li> </ul>"},{"location":"release-system/#release-please-manifestjson","title":".release-please-manifest.json","text":"<p>Tracks the current version of each component. This file is automatically updated by release-please when releases are created.</p> <pre><code>{\n  \"apps/frontend\": \"2.1.9\",\n  \"apps/ollyscale-ui\": \"0.0.0\",\n  \"charts/ollyscale\": \"0.1.0\"\n}\n</code></pre>"},{"location":"release-system/#conventional-commit-format","title":"Conventional Commit Format","text":"<p>Follow Conventional Commits specification:</p> <pre><code>&lt;type&gt;[optional scope]: &lt;description&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre> <p>Types:</p> <ul> <li><code>feat</code>: New feature (minor bump)</li> <li><code>fix</code>: Bug fix (patch bump)</li> <li><code>docs</code>: Documentation changes (no bump)</li> <li><code>chore</code>: Maintenance tasks (no bump)</li> <li><code>refactor</code>: Code refactoring (no bump)</li> <li><code>test</code>: Test updates (no bump)</li> <li><code>ci</code>: CI/CD changes (no bump)</li> </ul> <p>Scopes (optional):</p> <ul> <li>Component names: <code>ollyscale</code>, <code>ollyscale-ui</code>, <code>opamp-server</code>, <code>chart-ollyscale</code></li> <li>Functional areas: <code>api</code>, <code>ui</code>, <code>storage</code>, <code>ingestion</code></li> </ul> <p>Examples:</p> <pre><code># Feature in ollyscale backend\ngit commit -m \"feat(ollyscale): add span filtering API\"\n\n# Fix in UI\ngit commit -m \"fix(ollyscale-ui): correct trace timeline rendering\"\n\n# Breaking change\ngit commit -m \"feat(api)!: redesign OTLP ingestion endpoint\n\nBREAKING CHANGE: The OTLP endpoint now requires authentication headers.\"\n\n# Chart update\ngit commit -m \"feat(chart-ollyscale): add support for external Redis\"\n</code></pre>"},{"location":"release-system/#release-tags","title":"Release Tags","text":"<p>Components are tagged with their component name prefix:</p> <ul> <li><code>ollyscale-v2.1.9</code> - ollyscale backend</li> <li><code>ollyscale-ui-v1.0.0</code> - ollyscale UI</li> <li><code>opamp-server-v1.0.1</code> - OpAMP server</li> <li><code>chart-ollyscale-v0.1.0</code> - ollyscale Helm chart</li> <li><code>demo-v0.1.0</code> - Demo application</li> <li><code>demo-otel-agent-v0.1.0</code> - Demo OTel agent</li> </ul>"},{"location":"release-system/#docker-image-tags","title":"Docker Image Tags","text":"<p>Each release creates multiple Docker image tags:</p> <pre><code>ghcr.io/ryanfaircloth/ollyscale/ollyscale:2.1.9      # Exact version\nghcr.io/ryanfaircloth/ollyscale/ollyscale:2.1        # Minor version\nghcr.io/ryanfaircloth/ollyscale/ollyscale:2          # Major version\nghcr.io/ryanfaircloth/ollyscale/ollyscale:latest     # Latest (production only)\n</code></pre>"},{"location":"release-system/#helm-chart-publishing","title":"Helm Chart Publishing","text":"<p>Charts are published to OCI registry:</p> <pre><code># Add the chart repository\nhelm repo add ollyscale oci://ghcr.io/ryanfaircloth/ollyscale/charts\n\n# Install a specific version\nhelm install ollyscale oci://ghcr.io/ryanfaircloth/ollyscale/charts/ollyscale --version 0.1.0\n\n# Install latest version\nhelm install ollyscale oci://ghcr.io/ryanfaircloth/ollyscale/charts/ollyscale\n</code></pre>"},{"location":"release-system/#troubleshooting","title":"Troubleshooting","text":""},{"location":"release-system/#release-pr-not-created","title":"Release PR not created","text":"<p>Check that:</p> <ol> <li>Commits follow Conventional Commit format</li> <li>Changes are in the component's directory</li> <li>Commits were pushed to <code>main</code> branch</li> </ol>"},{"location":"release-system/#chart-not-bumped-when-app-changes","title":"Chart not bumped when app changes","text":"<p>Verify in <code>release-please-config.json</code>:</p> <ol> <li>The app component updates the chart's <code>values.yaml</code> in its <code>extra-files</code></li> <li>The chart has corresponding entry with <code>bumpDependents: true</code></li> <li>The <code>component</code> name matches exactly</li> </ol>"},{"location":"release-system/#build-failures","title":"Build failures","text":"<p>Check GitHub Actions logs:</p> <ol> <li>Docker build step - may need to fix Dockerfile issues</li> <li>Helm package step - may need to fix Chart.yaml issues</li> <li>Registry push step - check credentials and permissions</li> </ol>"},{"location":"release-system/#development-workflow","title":"Development Workflow","text":""},{"location":"release-system/#testing-changes-locally","title":"Testing Changes Locally","text":"<pre><code># Build locally\ncd apps/frontend\ndocker build -t ollyscale:dev -f Dockerfile .\n\n# Test Helm chart\ncd charts/ollyscale\nhelm install ollyscale-dev . --values values-local-dev.yaml\n</code></pre>"},{"location":"release-system/#validating-release-configuration","title":"Validating Release Configuration","text":"<p>Run the validation checks locally:</p> <pre><code># Validate JSON files\njq empty release-please-config.json\njq empty .release-please-manifest.json\n\n# Check for duplicate components\njq -r '.packages[].component' release-please-config.json | sort | uniq -d\n\n# Verify extra-files exist\njq -r '.packages[] | .[\"extra-files\"][]? | if type == \"string\" then . else .path end' \\\n  release-please-config.json | while read -r file; do\n  [ -f \"$file\" ] &amp;&amp; echo \"\u2705 $file\" || echo \"\u274c $file\"\ndone\n</code></pre>"},{"location":"release-system/#migration-from-semantic-release","title":"Migration from semantic-release","text":"<p>The old semantic-release system has been replaced with release-please. Key differences:</p> <p>semantic-release (old):</p> <ul> <li>Single monolithic release for all components</li> <li>Required <code>@anolilab/multi-semantic-release</code> plugin</li> <li>Complex plugin configuration per component</li> <li>Released everything together</li> </ul> <p>release-please (new):</p> <ul> <li>Independent releases per component</li> <li>Native multi-package support</li> <li>Simpler configuration</li> <li><code>bumpDependents</code> handles cross-component dependencies</li> <li>Separate PRs for better visibility</li> </ul>"},{"location":"release-system/#references","title":"References","text":"<ul> <li>release-please documentation</li> <li>Conventional Commits</li> <li>Forked release-please with bumpDependents</li> <li>GitHub Container Registry</li> <li>Helm OCI Registry</li> </ul>"},{"location":"technical/","title":"Technical Details","text":""},{"location":"technical/#architecture","title":"Architecture","text":""},{"location":"technical/#data-storage","title":"Data Storage","text":"<ul> <li>Format: Full OpenTelemetry (OTEL) format for traces, logs, and metrics</li> <li>Redis: All telemetry stored with 30-minute TTL (compressed with ZSTD + msgpack)</li> <li>Sorted Sets: Time-series data indexed by timestamp</li> <li>Correlation: Native trace-metric-log correlation via trace/span IDs</li> <li>Cardinality Protection: Prevents metric explosion</li> <li>No Persistence: Data vanishes after TTL (ephemeral dev tool)</li> </ul>"},{"location":"technical/#otlp-compatibility","title":"OTLP Compatibility","text":"<p>ollyScale is fully OpenTelemetry-native:</p> <ul> <li>Ingestion: Accepts OTLP/gRPC (primary) and OTLP/HTTP</li> <li>Storage: Stores traces, logs, and metrics in full OTEL format with resources, scopes, and attributes</li> <li>Correlation: Native support for trace/span ID correlation across all telemetry types</li> <li>REST API: Exposes OTEL-formatted JSON for programmatic access</li> <li>Control Plane: OpenTelemetry Collector OpAmp for dynamic configuration</li> </ul>"},{"location":"v2-api-migration-plan/","title":"V2 api migration plan","text":"<p>Below is a revised single plan file with your corrections baked in:</p> <ul> <li>Assumes you create/manage the branch (no branch-creation steps in the plan).</li> <li>The new frontend app is v2; no <code>v2</code> prefixes/markings inside the code.</li> <li>Exposure is via Kubernetes Gateway API (not classic Ingress wording).[1][2][3]</li> <li>Markdown is structured so you can paste it directly into the repo as <code>docs/ollyscale-postgres-plan.md</code>.</li> </ul> <p>All OTEL / Postgres / operator references remain aligned with current practices.[4][5][6][7][8]</p>"},{"location":"v2-api-migration-plan/#ollyscale-postgres-otel-migration-plan","title":"ollyScale Postgres &amp; OTEL Migration Plan","text":""},{"location":"v2-api-migration-plan/#implementation-status","title":"Implementation Status","text":"<p>Branch: <code>storage-improvements</code> Progress: Phases 1-10 Complete (as of January 21, 2026)</p>"},{"location":"v2-api-migration-plan/#completed-phases","title":"\u2705 Completed Phases","text":"Phase Status Commits Notes Phase 1: Frontend Skeleton \u2705 Complete a79d7fc FastAPI structure, StorageBackend, OTEL models Phase 2: PostgreSQL Infrastructure \u2705 Complete 1968d7f Zalando Operator, Postgresql CR (2 replicas + PgBouncer) Phase 3: Alembic OTEL Schema \u2705 Complete 96ee885, 0143588 Async Alembic, partitioned fact tables, dimension tables Phase 4: Partition Management \u2705 Complete d773d6b create_partitions(), drop_old_partitions(), daily CronJob Phase 5: PostgresStorage Backend \u2705 Complete 4a62d83 Database session manager, full StorageBackend implementation Phase 6: Query API \u2705 Complete e289095 Dependency injection, all query endpoints implemented Phase 7: Ingestion API \u2705 Complete af670d3 Ingest endpoints with validation, error handling Phase 8: Migration Job \u2705 Complete 0143588 Helm pre-upgrade Job runs Alembic migrations Phase 9: Testing \u2705 Complete a89e9de, 19e55ef 22 unit tests passing, 6 integration tests (require live DB) Phase 10: Documentation \u2705 Complete 1327624, 19e55ef Status table, testing strategy documented"},{"location":"v2-api-migration-plan/#refactoring-complete","title":"\ud83d\udd27 Refactoring Complete","text":"<ul> <li>InMemoryStorage Removal | a89e9de | Removed dual-mode operation, PostgresStorage required in production</li> <li>Bug Fixes | 19e55ef | Fixed TimeRange attribute names (start_time/end_time), fixed _upsert methods</li> </ul>"},{"location":"v2-api-migration-plan/#remaining-phases","title":"\ud83d\udccb Remaining Phases","text":"<ul> <li>Phase 11: Integration Testing - API contract tests with FastAPI TestClient (see section 11 below)</li> <li>Phase 12: Performance Benchmarking - Load test ingestion/queries, optimize indexes</li> <li>Phase 13: Observability - Add metrics for storage operations, partition status</li> <li>Phase 14: Deployment &amp; Rollback - Deploy to test environment, verify migration</li> </ul>"},{"location":"v2-api-migration-plan/#phase-11-status-update-january-21-2026","title":"Phase 11 Status Update (January 21, 2026)","text":"<p>Integration Testing Framework Added:</p> <ul> <li>Test Fixtures (<code>tests/fixtures.py</code>): Reusable OTLP data generators for ResourceSpans, ResourceLogs, ResourceMetrics</li> <li>Ingestion Tests (<code>tests/test_api_ingest.py</code>): 17 tests covering POST /api/traces, /api/logs, /api/metrics - ALL PASSING \u2705</li> <li>Query Tests (<code>tests/test_api_query.py</code>): 21 tests for search endpoints, pagination, filters (mock data structure fixes needed)</li> <li>Error Tests (<code>tests/test_api_errors.py</code>): 26 tests for error handling, large payloads, concurrent requests</li> <li>Health Tests (<code>tests/test_health.py</code>): 18 comprehensive health endpoint tests - ALL PASSING \u2705</li> </ul> <p>Testing Philosophy:</p> <ul> <li>Unit Tests: Mock database completely, test business logic and error handling</li> <li>Integration Tests: Use FastAPI TestClient with mocked StorageBackend to test API contracts</li> <li>Dependency Override Pattern: <code>app.dependency_overrides[get_storage] = lambda: mock_storage</code></li> <li>No Live DB Required: All tests use AsyncMock for storage, fast execution (~0.5s for 108 tests)</li> </ul> <p>Current Status:</p> <ul> <li>86 tests passing, 22 tests need mock data structure fixes to match Pydantic models</li> <li>Deprecation warning fixed: <code>HTTP_422_UNPROCESSABLE_ENTITY</code> \u2192 <code>HTTP_422_UNPROCESSABLE_CONTENT</code></li> <li>Removed obsolete <code>test_storage.py</code> (tested removed InMemoryStorage)</li> </ul> <p>Next Steps:</p> <ul> <li>Fix mock data structures in query/error tests to match actual Service, Metric, ServiceMapNode models</li> <li>Add test coverage for edge cases (time range validation, pagination cursors)</li> <li>Consider adding property-based testing with Hypothesis for OTLP data validation</li> </ul>"},{"location":"v2-api-migration-plan/#key-technical-decisions","title":"Key Technical Decisions","text":"<p>Storage Architecture:</p> <ul> <li>Single-mode operation: PostgresStorage required (DATABASE_HOST must be set)</li> <li>InMemoryStorage removed for production safety (was not in original plan)</li> <li>Dependency injection pattern with <code>get_storage()</code> for clean separation</li> <li>PostgreSQL 18 with native partitioning (daily intervals on <code>time_unix_nano</code>)</li> </ul> <p>Schema Design:</p> <ul> <li>Partitioned fact tables: <code>spans_fact</code>, <code>logs_fact</code>, <code>metrics_fact</code></li> <li>Dimension tables: <code>service_dim</code>, <code>operation_dim</code>, <code>resource_dim</code></li> <li>JSONB attributes with GIN indexes for flexible querying</li> <li>Foreign keys with indexes for fast joins</li> </ul> <p>Partition Management:</p> <ul> <li>Automated creation of future partitions (7-day lookahead)</li> <li>Automated cleanup of old partitions (30-day retention)</li> <li>CronJob running daily at 2 AM with comprehensive error handling</li> </ul> <p>Testing:</p> <ul> <li>25 unit tests passing (InMemoryStorage)</li> <li>6 integration tests require live PostgreSQL (validated in Kubernetes)</li> <li>Pre-commit hooks ensure code quality (ruff, pytest, yamlfmt, hadolint)</li> </ul> <p>Dependencies:</p> <ul> <li>FastAPI 0.115+, Pydantic 2.10+, SQLAlchemy[asyncio] 2.0.36+</li> <li>asyncpg 0.30+ for high-performance async Postgres</li> <li>Alembic 1.14+ for schema migrations</li> </ul> <p>Goal: Introduce an OTEL-aligned, Postgres-backed version of the ollyScale frontend app (replacing Redis-based storage) using the Zalando Postgres Operator, exposed via Kubernetes Gateway API.[2][5][6][1][4]</p> <p>Constraints:</p> <ul> <li>The new frontend app is the \u201cv2\u201d; no <code>v2</code> prefixes in module names, URLs, or directories inside that app.</li> <li>All new work happens under:</li> <li><code>apps/frontend/app</code></li> <li><code>apps/frontend/tests</code></li> <li><code>apps/ollyscale/</code> is effectively frozen:</li> <li>Only minimal bug/operational fixes are allowed.</li> <li>No new \u201cv2\u201d logic, models, or storage added there.</li> <li>No legacy data migration/ETL is required; all deployments are clean installs.</li> </ul>"},{"location":"v2-api-migration-plan/#0-scope-guardrails","title":"0. Scope &amp; Guardrails","text":"<ul> <li>Replace Redis-backed storage for traces, logs, and metrics with Postgres in the new frontend application, while keeping the legacy ollyscale app intact for this phase.[5][6][9]</li> <li>The new frontend app:</li> <li>Implements OTEL-aligned ingest and query APIs.</li> <li>Is exposed to clients via Gateway API HTTPRoutes.</li> <li>A follow-up phase will:</li> <li>Remove unused Redis and legacy code.</li> <li>Finalize cut-over and decommission Redis.</li> </ul> <p>Guardrail rules for implementation:</p> <ul> <li>Do not rename internal modules to include <code>v2</code> (e.g., avoid <code>routers/v2/*</code>); the whole app is the new version.</li> <li>Before every commit:</li> <li>Verify that <code>apps/ollyscale/</code> has no unintended changes.</li> <li>Ensure all new code is under <code>apps/frontend/</code> paths.</li> </ul>"},{"location":"v2-api-migration-plan/#1-project-setup-docs-structure-only","title":"1. Project Setup (Docs &amp; Structure Only)","text":"<ul> <li>Add plan and architecture docs:</li> <li><code>docs/ollyscale-postgres-plan.md</code> (this file).</li> <li>Optionally <code>docs/ollyscale-postgres-architecture.md</code> for diagrams and schema.</li> <li>Under <code>apps/frontend/app</code>:</li> <li>Ensure there is a clear app entrypoint (e.g., <code>main.py</code>) and standard FastAPI layout.[8][10]</li> <li>Add top-level packages:<ul> <li><code>routers/</code> for HTTP endpoints.</li> <li><code>models/</code> for Pydantic request/response models.</li> <li><code>storage/</code> for storage abstraction and implementations.</li> <li><code>db/</code> for DB session/connection utilities (if needed).</li> </ul> </li> </ul> <p>(Branch creation is assumed to be handled manually by the developer, not by this plan.)</p>"},{"location":"v2-api-migration-plan/#2-postgres-infrastructure-zalando-operator-via-terraform","title":"2. Postgres Infrastructure (Zalando Operator via Terraform)","text":"<ul> <li>Use Terraform to deploy Zalando Postgres Operator and a Postgres cluster:[11][12][13][4]</li> <li>Operator:<ul> <li>Install in namespace like <code>postgres-operator</code>.</li> <li>Ensure required CRDs and RBAC are applied.</li> </ul> </li> <li>Cluster:<ul> <li>Create an <code>acid.zalan.do/v1</code> <code>postgresql</code> manifest for a single-node internal cluster.</li> <li>Parameterize storage size and class (e.g., 30GB default; overridable).</li> <li>Restrict network access so only the frontend app namespace(s) and migration Jobs can reach it.</li> </ul> </li> <li>Credentials and connectivity:</li> <li>Use the operator-managed Secret for DB credentials.</li> <li>Mount credentials into:<ul> <li>Frontend app Deployment/Pod.</li> <li>Alembic migration Job.</li> </ul> </li> <li>Document:<ul> <li>Secret names.</li> <li>Connection URI format.</li> <li>How credentials are mapped into environment variables.</li> </ul> </li> </ul>"},{"location":"v2-api-migration-plan/#3-otel-aligned-data-model-schema","title":"3. OTEL-Aligned Data Model &amp; Schema","text":"<ul> <li>Design schema aligned with OpenTelemetry\u2019s data model and semantic conventions for logs, metrics, and traces:[6][7][14][15][5]</li> <li>Signals:<ul> <li>Traces (spans).</li> <li>Logs.</li> <li>Metrics (time-series points).</li> </ul> </li> <li>Fact tables (examples to adjust as needed):</li> <li><code>spans_fact</code>:<ul> <li>Keys: <code>tenant_id</code>, <code>connection_id</code>, <code>trace_id</code>, <code>span_id</code>, <code>parent_span_id</code>, <code>service_id</code>, <code>operation_id</code>.</li> <li>Time: <code>start_time_unix_nano</code>, <code>end_time_unix_nano</code>, <code>duration</code>.</li> <li>Fields: <code>status_code</code>, <code>kind</code>, <code>resource_id</code>, <code>attributes JSONB</code>, <code>events JSONB</code>, <code>links JSONB</code>.</li> </ul> </li> <li><code>logs_fact</code>:<ul> <li>Keys: <code>id</code>, <code>tenant_id</code>, <code>connection_id</code>.</li> <li>Time: <code>time_unix_nano</code>, <code>observed_time_unix_nano</code>.</li> <li>OTEL: <code>trace_id</code>, <code>span_id</code>, <code>severity_number</code>, <code>severity_text</code>, <code>body JSONB</code>, <code>attributes JSONB</code>, <code>resource JSONB</code>, <code>flags</code>, <code>dropped_attributes_count</code>.</li> </ul> </li> <li><code>metrics_fact</code>:<ul> <li>Keys: <code>id</code>, <code>tenant_id</code>, <code>connection_id</code>.</li> <li>OTEL: <code>resource JSONB</code>, <code>scope JSONB</code>, <code>metric_name</code>, <code>metric_type</code>, <code>unit</code>, <code>description</code>, <code>data_points JSONB</code>, <code>attributes JSONB</code>, <code>time_unix_nano</code>, <code>start_time_unix_nano</code>.[16][5]</li> </ul> </li> <li>Dimensions:</li> <li><code>service_dim</code>, <code>operation_dim</code>, <code>resource_dim</code>, optional <code>time_dim</code>.</li> <li>Conventions:</li> <li>Snake_case for all identifiers.</li> <li>JSONB for flexible attributes with planned GIN indexes for common keys (e.g., service name, environment).[7]</li> <li>Time-based partitioning on fact tables (details next section).</li> <li>Implement via Alembic migrations under <code>apps/frontend/migrations/</code>.</li> </ul>"},{"location":"v2-api-migration-plan/#4-retention-partitioning","title":"4. Retention &amp; Partitioning","text":"<ul> <li>Retention configuration:</li> <li>Create <code>retention_policy</code> table with:<ul> <li>Tenant identifier.</li> <li>Signal type (trace/log/metric).</li> <li>Retention period (e.g., days).</li> </ul> </li> <li>Provide a simple API/CLI path for reading/updating retention policies.</li> <li>Partitioning:</li> <li>Use native Postgres partitioning on time columns (<code>time_unix_nano</code> or truncated timestamp).</li> <li>Choose an initial interval (daily or weekly) and document why.</li> <li>Enforcement:</li> <li>Implement either:<ul> <li><code>pg_cron</code> jobs within Postgres, or</li> <li>K8s CronJobs calling a management script.</li> </ul> </li> <li>Logic:<ul> <li>Drop expired partitions or run <code>DELETE</code> on old rows (for tables not yet partitioned).</li> <li>Log actions for audit.</li> </ul> </li> <li>Document future-proofing and volume assumptions in <code>docs/ollyscale-postgres-architecture.md</code>.</li> </ul>"},{"location":"v2-api-migration-plan/#5-frontend-app-api-design-new-v2-app","title":"5. Frontend App API Design (New \u201cv2\u201d App)","text":"<p>(In this plan, \u201cv2\u201d refers to the entire new frontend app; internal names remain clean.)</p> <ul> <li>Endpoints:</li> <li>Ingest:<ul> <li><code>/api/traces</code></li> <li><code>/api/logs</code></li> <li><code>/api/metrics</code></li> </ul> </li> <li>Query:<ul> <li><code>/api/traces/search</code></li> <li><code>/api/logs/search</code></li> <li><code>/api/metrics/search</code></li> <li><code>/api/service-map</code></li> </ul> </li> <li>Models:</li> <li>Create Pydantic models in <code>apps/frontend/app/models/</code> that:<ul> <li>Are compatible with OTEL structures but simplified for HTTP JSON.</li> <li>Represent ingest payloads and query responses clearly.</li> </ul> </li> <li>Decoupling:</li> <li>The frontend app must not import code from <code>apps/ollyscale/</code>.</li> <li>The legacy app remains separate in deployment and routing.</li> </ul>"},{"location":"v2-api-migration-plan/#6-storage-layer-abstraction","title":"6. Storage Layer Abstraction","text":"<ul> <li>Define a storage abstraction in <code>apps/frontend/app/storage/</code>:</li> <li>Interfaces:<ul> <li><code>store_traces(payload)</code></li> <li><code>store_logs(payload)</code></li> <li><code>store_metrics(payload)</code></li> <li><code>search_traces(query)</code></li> <li><code>search_logs(query)</code></li> <li><code>search_metrics(query)</code></li> <li><code>get_service_map(params)</code></li> </ul> </li> <li>Implementation phases:</li> <li>Phase A (stub):<ul> <li>Implement an in-memory or no-op backend for early unit testing.</li> </ul> </li> <li>Phase B (Postgres):<ul> <li>Implement Postgres-backed storage using an async driver (e.g., async SQLAlchemy or asyncpg) and a pooled connection.[10][17][8]</li> <li>Encapsulate all SQL and query-building logic inside this layer.</li> </ul> </li> </ul>"},{"location":"v2-api-migration-plan/#7-query-semantics-time-filtering-pagination","title":"7. Query Semantics: Time, Filtering, Pagination","text":"<ul> <li>Time:</li> <li>All search endpoints require <code>start_time</code> and <code>end_time</code> parameters.</li> <li>Limits:</li> <li><code>limit</code> parameter with default (e.g., 50\u2013100) and hard maximum.</li> <li>Pagination:</li> <li>Prefer cursor-based pagination (opaque cursor capturing time and id).</li> <li>Accept offset/limit as a fallback if needed, but design for cursor-first.</li> <li>Filtering:</li> <li>Accept structured filters (field, operator, value).</li> <li>Validate filters and construct safe SQL WHERE clauses in the storage layer.</li> <li>Support common fields such as: tenant, service, operation, severity, status, trace_id, environment.</li> <li>Response signals:</li> <li>Include <code>has_more</code> or <code>next_cursor</code> when additional results are available.</li> <li>Enforce the hard cap on rows per query server-side.</li> </ul>"},{"location":"v2-api-migration-plan/#8-exposure-via-kubernetes-gateway-api","title":"8. Exposure via Kubernetes Gateway API","text":"<ul> <li>Use Gateway API to expose the new frontend app:[3][18][1][2]</li> <li>Create a <code>Gateway</code> object (or use an existing one) for HTTP traffic.</li> <li>Create <code>HTTPRoute</code> objects that:<ul> <li>Match hostnames/paths for the new app (e.g., <code>api.ziggiz.ai</code> with <code>/olly</code> prefix, adjust as needed).</li> <li>Forward traffic to the Service backing the new frontend Deployment.</li> </ul> </li> <li>Routing rules:</li> <li>Treat the new frontend app as the authoritative \u201cv2\u201d API for new clients.</li> <li>Keep legacy routes pointing at the existing ollyscale app until cut-over.</li> <li>Document:</li> <li>Which hostnames/paths route to the new frontend app.</li> <li>Any header-based routing or canary/A/B patterns if used.[19][20]</li> </ul>"},{"location":"v2-api-migration-plan/#9-migrations-deployment-integration","title":"9. Migrations &amp; Deployment Integration","text":"<ul> <li>Alembic:</li> <li>Configure Alembic (and SQLAlchemy if used) for Postgres migrations.[8][10]</li> <li>Store migration scripts in <code>apps/frontend/migrations/</code>.</li> <li>Migration Job:</li> <li>Add a dedicated K8s Job that:<ul> <li>Runs <code>alembic upgrade head</code> on deploy/upgrade.</li> <li>Uses DB credentials from the operator Secret.</li> <li>Is idempotent and fails fast on errors.</li> </ul> </li> <li>App readiness:</li> <li>Implement readiness checks so the frontend app:<ul> <li>Verifies DB connectivity.</li> <li>Confirms required migrations are applied (via Alembic version table).</li> </ul> </li> <li>Health endpoints:</li> <li><code>/health</code> and <code>/health/db</code>:<ul> <li>Return 200 when the app is ready and DB is healthy.</li> <li>Return 503 (with structured body) when DB is down or migrations are running.</li> </ul> </li> </ul>"},{"location":"v2-api-migration-plan/#10-unit-testing","title":"10. Unit Testing","text":"<ul> <li>Add unit tests in <code>apps/frontend/tests/</code> for:</li> <li>Pydantic models: validation, defaults, required fields.</li> <li>Routers: status codes, error handling, payload shapes.</li> <li>Storage abstraction:<ul> <li>Stub behavior.</li> <li>Filter parsing and SQL generation (where possible without hitting real DB).</li> </ul> </li> <li>Retention logic (computations and selection of partitions to drop).</li> <li>Ensure the unit test suite can run without a live Postgres instance (stubs/mocks).[21]</li> </ul>"},{"location":"v2-api-migration-plan/#11-integration-testing-final-testing-layer","title":"11. Integration Testing (Final Testing Layer)","text":"<ul> <li>API integration tests:</li> <li>Use FastAPI <code>TestClient</code> with a test Postgres instance (e.g., Dockerized DB) and migrations applied.</li> <li>Cover:<ul> <li>Ingest flows for traces/logs/metrics.</li> <li>Query flows with time range, filters, and pagination.</li> <li>Behavior when DB is unavailable or migrations are incomplete.</li> </ul> </li> <li>UI integration/E2E:</li> <li>Exercise:<ul> <li>Query builder behavior.</li> <li>Pagination/infinite scroll.</li> <li>Error and maintenance banners when <code>/health</code> or <code>/health/db</code> indicate problems.</li> </ul> </li> <li>Integration tests must be run after unit tests are passing and are treated as the last verification layer before commit/merge.[22]</li> </ul>"},{"location":"v2-api-migration-plan/#12-observability-performance-indexing","title":"12. Observability, Performance &amp; Indexing","text":"<ul> <li>Observability:</li> <li>Emit telemetry for the new frontend app using OTEL (traces, metrics, logs) with appropriate sampling.[9][14][23]</li> <li>Monitor:<ul> <li>DB health and resource usage.</li> <li>Migration Job success/failure.</li> <li>API latency, error rates, and throughput.</li> </ul> </li> <li>Indexing:</li> <li>Add indexes on:<ul> <li>Time columns for partition pruning.</li> <li><code>trace_id</code>, <code>span_id</code>.</li> <li>Foreign keys such as <code>service_id</code>, <code>operation_id</code>.</li> <li>JSONB attributes used frequently in filters (GIN indexes).[7]</li> </ul> </li> <li>Performance:</li> <li>Capture slow queries and iteratively tune indexes and query patterns.</li> <li>Adjust partitioning strategy if needed based on actual load.</li> </ul>"},{"location":"v2-api-migration-plan/#13-documentation-follow-up-cleanup-phase","title":"13. Documentation &amp; Follow-Up (Cleanup Phase)","text":"<ul> <li>Documentation:</li> <li>Keep <code>docs/ollyscale-postgres-plan.md</code> and architecture docs up to date as implementation details change.</li> <li>Include:<ul> <li>Schema diagrams, table descriptions.</li> <li>Retention and partitioning details.</li> <li>Deployment &amp; migration steps.</li> <li>Troubleshooting guides.</li> </ul> </li> <li>Future cleanup phase (out of scope for this plan):</li> <li>Identify and remove unused Redis and legacy ollyscale code paths.</li> <li>Plan and execute final cut-over from legacy to new frontend app as the single source of truth.</li> <li>Decommission Redis and any unused infrastructure.</li> </ul>"},{"location":"v2-api-migration-plan/#14-copilot-automation-rules","title":"14. Copilot / Automation Rules","text":"<p>For each incremental change within these phases:</p> <ol> <li>Implement the minimal set of changes needed for that step.</li> <li>Run the full test suite (unit + integration + UI tests if configured).</li> <li>If any tests fail:</li> <li>Fix the issues.</li> <li>Re-run tests until green.</li> <li>Before committing:</li> <li>Ensure no new v2 logic has been added under <code>apps/ollyscale/</code>.</li> <li>Confirm all changes are under <code>apps/frontend/</code> (plus allowed infra/docs tweaks).</li> <li>Only when all tests pass and guardrails are satisfied:</li> <li>Format code as needed.</li> <li>Commit with a descriptive message referencing the phase/step.</li> <li>Continue autonomously to the next step in the plan, maintaining the same discipline.[24][25][22]</li> </ol> <p>Sources [1] Kubernetes Gateway API: Introduction https://gateway-api.sigs.k8s.io [2] Gateway API - Kubernetes https://kubernetes.io/docs/concepts/services-networking/gateway/ [3] What Is the Kubernetes Gateway API? - Tetrate https://tetrate.io/learn/what-is-kubernetes-gateway-api [4] Zalando Postgres operator - The Blue Book https://lyz-code.github.io/blue-book/zalando_postgres_operator/ [5] Metrics Data Model | OpenTelemetry https://opentelemetry.io/docs/specs/otel/metrics/data-model/ [6] Logs Data Model | OpenTelemetry https://opentelemetry.io/docs/specs/otel/logs/data-model/ [7] OpenTelemetry Logging Explained: Concepts and Data Model \u00b7 Dash0 https://www.dash0.com/knowledge/opentelemetry-logging-explained [8] FastAPI with Async SQLAlchemy, SQLModel, and Alembic https://testdriven.io/blog/fastapi-sqlmodel/ [9] OpenTelemetry Signals Overview: Logs vs Metrics vs Traces - Dash0 https://www.dash0.com/knowledge/logs-metrics-and-traces-observability [10] Adding a production-grade database to your FastAPI project https://python.plainenglish.io/adding-a-production-grade-database-to-your-fastapi-project-local-setup-50107b10d539 [11] zalando/postgres-operator - GitHub https://github.com/zalando/postgres-operator [12] Zalando's Postgres Operator https://opensource.zalando.com/postgres-operator/docs/quickstart.html [13] Postgres Operator - Read the Docs https://postgres-operator.readthedocs.io [14] OpenTelemetry Logging https://opentelemetry.io/docs/specs/otel/logs/ [15] Inside OpenTelemetry: Understanding Its Data Model - LinkedIn https://www.linkedin.com/pulse/opentelemetry-series-6-deep-dive-opentelemetrys-data-model-kulkarni-rzmmf [16] Understanding OpenTelemetry Metrics: Types, Model, Collection ... https://edgedelta.com/company/blog/understanding-opentelemetry-metrics [17] FastAPI SQLAlchemy 2, Alembic and PostgreSQL Setup Tutorial 2025 https://www.youtube.com/watch?v=gg7AX1iRnmg [18] API Overview - Kubernetes Gateway API https://gateway-api.sigs.k8s.io/concepts/api-overview/ [19] Understanding Kubernetes Gateway API: A Modern Approach to ... https://www.cncf.io/blog/2025/05/02/understanding-kubernetes-gateway-api-a-modern-approach-to-traffic-management/ [20] Kubernetes Gateway API in action | Containers - AWS https://aws.amazon.com/blogs/containers/kubernetes-gateway-api-in-action/ [21] How to use PostgreSQL test database in async FastAPI tests? https://stackoverflow.com/questions/70752806/how-to-use-postgresql-test-database-in-async-fastapi-tests [22] Automation Pipeline and CI/CD: A Guide to Testing Best Practices https://www.browserstack.com/guide/automation-pipeline [23] What is OpenTelemetry? An open-source standard for logs, metrics ... https://www.dynatrace.com/news/blog/what-is-opentelemetry/ [24] Run Tests Before Commit - JetBrains Guide https://www.jetbrains.com/guide/go/tips/vcs-run-tests-before-commit/ [25] pre-commit vs. CI - Sebastian Witowski https://switowski.com/blog/pre-commit-vs-ci/ [26] Kubernetes Gateway API: What Is It And Why Do You Need It? https://traefik.io/glossary/kubernetes-gateway-api [27] Getting started with Gateway API https://gateway-api.sigs.k8s.io/guides/ [28] Collect logs, metrics, and traces with OpenTelemetry - Open Liberty https://openliberty.io/docs/latest/microprofile-telemetry.html [29] Postgres operator doesn't place the user secrets in the defined ... https://github.com/zalando/postgres-operator/issues/1827</p>"},{"location":"demos/","title":"Demo Applications","text":"<p>ollyScale includes several demo applications to help you explore its observability capabilities. These demos range from simple microservices to complex distributed systems.</p>"},{"location":"demos/#available-demos","title":"Available Demos","text":""},{"location":"demos/#custom-demo-applications","title":"Custom Demo Applications","text":"<p>Simple Python Flask microservices demonstrating core observability features:</p> <ul> <li>Services: demo-frontend and demo-backend</li> <li>Languages: Python</li> <li>Features: Auto-traffic generation, distributed tracing, Prometheus metrics</li> <li>Resource Usage: Low (~256MB)</li> <li>Best For: Quick testing, development, learning basics</li> </ul> <p>Key Highlights:</p> <ul> <li>Automatic traffic generation every 3-8 seconds</li> <li>Multiple endpoints (simple requests, calculations, complex orders, errors)</li> <li>Easy to understand and modify</li> <li>Minimal resource requirements</li> </ul> <p>View Documentation \u2192</p>"},{"location":"demos/#opentelemetry-demo","title":"OpenTelemetry Demo","text":"<p>The official OpenTelemetry demo application (\"Astronomy Shop\"):</p> <ul> <li>Services: 11+ microservices</li> <li>Languages: Go, Java, Python, .NET, Node.js, Rust, Ruby, PHP</li> <li>Features: E-commerce workflows, built-in load generator, multi-language instrumentation</li> <li>Resource Usage: High (~2-4GB)</li> <li>Best For: Comprehensive demos, training, polyglot environments</li> </ul> <p>Key Highlights:</p> <ul> <li>Production-realistic distributed system</li> <li>Showcases OTel instrumentation across 7+ languages</li> <li>Complex distributed traces (10-20 spans)</li> <li>Realistic user workflows (browse, cart, checkout, payment)</li> </ul> <p>View Documentation \u2192</p>"},{"location":"demos/#advanced-demos","title":"Advanced Demos","text":""},{"location":"demos/#ebpf-zero-code-tracing","title":"eBPF Zero-Code Tracing","text":"<p>Automatic distributed tracing without code changes using eBPF:</p> <ul> <li>Technology: eBPF with Beyla auto-instrumentation</li> <li>Features: Zero-code instrumentation, HTTP/gRPC tracing</li> <li>Best For: Legacy applications, no-modification observability</li> </ul> <p>View Documentation \u2192</p>"},{"location":"demos/#ai-agent-with-ollama","title":"AI Agent with Ollama","text":"<p>GenAI observability demo with LLM instrumentation:</p> <ul> <li>Technology: Ollama LLM with OTel instrumentation</li> <li>Features: LLM span attributes, token tracking, prompt/response logging</li> <li>Best For: AI/ML observability, GenAI applications</li> </ul> <p>View Documentation \u2192</p>"},{"location":"demos/#quick-comparison","title":"Quick Comparison","text":"Demo Complexity Services Languages Resource Usage Auto-Traffic Custom Demo Simple 2 Python Low \u2705 OTel Demo Complex 11+ 7+ High \u2705 eBPF Demo Medium 2 Go Low \u274c AI Agent Simple 1 Python Medium \u2705"},{"location":"demos/#deployment","title":"Deployment","text":"<p>All demos can be deployed via:</p>"},{"location":"demos/#helm","title":"Helm","text":"<pre><code># Custom Demo\nhelm install ollyscale-demos charts/ollyscale-demos \\\n  --namespace ollyscale-demos \\\n  --create-namespace\n\n# OTel Demo\nhelm install ollyscale-demos charts/ollyscale-demos \\\n  --set customDemo.enabled=false \\\n  --set otelDemo.enabled=true\n</code></pre>"},{"location":"demos/#terraformargocd-recommended","title":"Terraform/ArgoCD (Recommended)","text":"<pre><code>cd .kind\n\n# Enable Custom Demo\nterraform apply -var=\"custom_demo_enabled=true\"\n\n# Enable OTel Demo\nterraform apply -var=\"otel_demo_enabled=true\" -var=\"custom_demo_enabled=false\"\n</code></pre>"},{"location":"demos/#accessing-demos","title":"Accessing Demos","text":"<p>After deployment, demos are available via HTTPRoutes:</p> <ul> <li>Custom Demo Frontend: <code>https://demo-frontend.ollyscale.test:49443</code></li> <li>OTel Demo: <code>https://otel-demo.ollyscale.test:49443</code></li> <li>ollyScale UI: <code>https://ollyscale.ollyscale.test:49443</code></li> </ul>"},{"location":"demos/#viewing-telemetry","title":"Viewing Telemetry","text":"<p>All demos send telemetry to ollyScale. Open the ollyScale UI to explore:</p> <ol> <li>Service Map: Visual representation of service dependencies</li> <li>Traces: Distributed traces with timing and attributes</li> <li>Logs: Application logs with trace correlation</li> <li>Metrics: RED metrics (Rate, Errors, Duration)</li> </ol>"},{"location":"demos/#choosing-a-demo","title":"Choosing a Demo","text":"<p>For Learning/Development:</p> <ul> <li>Start with Custom Demo - simple, fast, easy to understand</li> </ul> <p>For Demonstrations/Training:</p> <ul> <li>Use OTel Demo - comprehensive, production-realistic</li> </ul> <p>For Polyglot Environments:</p> <ul> <li>Use OTel Demo - showcases multiple languages</li> </ul> <p>For Legacy Applications:</p> <ul> <li>Try eBPF Demo - no code changes required</li> </ul> <p>For AI/ML Workloads:</p> <ul> <li>Try AI Agent - GenAI-specific instrumentation</li> </ul>"},{"location":"demos/#migration","title":"Migration","text":"<p>If you're migrating from the old <code>k8s-demo/</code> or <code>k8s-otel-demo/</code> directories, see the migration sections in:</p> <ul> <li>Custom Demo Migration</li> <li>OTel Demo Migration</li> </ul>"},{"location":"demos/#support","title":"Support","text":"<p>For issues or questions:</p> <ul> <li>GitHub Issues</li> <li>Documentation</li> </ul>"},{"location":"demos/custom-demo/","title":"Custom Demo Applications","text":"<p>The custom demo applications provide a simple but realistic microservice architecture for demonstrating ollyScale's observability capabilities.</p>"},{"location":"demos/custom-demo/#overview","title":"Overview","text":"<p>The demo consists of two Python Flask applications that automatically generate OpenTelemetry traces, logs, and Prometheus metrics:</p> <ul> <li>demo-frontend: User-facing service with multiple endpoints</li> <li>demo-backend: Backend service for data processing</li> </ul> <p>Both applications feature automatic traffic generation - they create distributed traces every 3-8 seconds without external input.</p>"},{"location":"demos/custom-demo/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  demo-frontend  \u2502  Port 5000 (HTTP)\n\u2502   (Flask)       \u2502  Port 8000 (Prometheus metrics)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 HTTP\n         \u2502 OTLP gRPC \u2192 gateway-collector:4317\n         \u2502 Prom \u2192 gateway-collector:19291\n         \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  demo-backend   \u2502  Port 5000 (HTTP)\n\u2502   (Flask)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502 OTLP gRPC \u2192 gateway-collector:4317\n         \u2193\n    ollyScale UI\n</code></pre>"},{"location":"demos/custom-demo/#frontend-endpoints","title":"Frontend Endpoints","text":""},{"location":"demos/custom-demo/#-home","title":"<code>/</code> - Home","text":"<p>Returns service information and available endpoints.</p> <p>Example:</p> <pre><code>curl https://demo-frontend.ollyscale.test:49443/\n</code></pre>"},{"location":"demos/custom-demo/#hello-simple-request","title":"<code>/hello</code> - Simple Request","text":"<p>Basic endpoint that returns a greeting. Generates simple traces.</p> <p>Example:</p> <pre><code>curl https://demo-frontend.ollyscale.test:49443/hello\n</code></pre>"},{"location":"demos/custom-demo/#calculate-backend-interaction","title":"<code>/calculate</code> - Backend Interaction","text":"<p>Calls the backend to perform a calculation. Demonstrates service-to-service tracing.</p> <p>Example:</p> <pre><code>curl https://demo-frontend.ollyscale.test:49443/calculate\n</code></pre> <p>Response:</p> <pre><code>{\n  \"operation\": \"add\",\n  \"a\": 42,\n  \"b\": 17,\n  \"result\": 59\n}\n</code></pre>"},{"location":"demos/custom-demo/#error-error-scenario","title":"<code>/error</code> - Error Scenario","text":"<p>Intentionally triggers an exception to demonstrate error tracking.</p> <p>Example:</p> <pre><code>curl https://demo-frontend.ollyscale.test:49443/error\n</code></pre>"},{"location":"demos/custom-demo/#process-order-complex-distributed-trace","title":"<code>/process-order</code> - Complex Distributed Trace","text":"<p>Creates a multi-span distributed trace across frontend and backend:</p> <ol> <li>Frontend receives order request</li> <li>Backend validates order</li> <li>Backend processes payment</li> <li>Backend checks inventory</li> <li>Order completion</li> </ol> <p>Example:</p> <pre><code>curl https://demo-frontend.ollyscale.test:49443/process-order\n</code></pre> <p>Response:</p> <pre><code>{\n  \"status\": \"success\",\n  \"order_id\": 7342,\n  \"details\": {\n    \"status\": \"success\",\n    \"order_id\": 7342,\n    \"processing_time_ms\": 287\n  }\n}\n</code></pre>"},{"location":"demos/custom-demo/#backend-endpoints","title":"Backend Endpoints","text":""},{"location":"demos/custom-demo/#health-health-check","title":"<code>/health</code> - Health Check","text":"<p>Kubernetes liveness/readiness probe endpoint.</p>"},{"location":"demos/custom-demo/#calculate-math-operations","title":"<code>/calculate</code> - Math Operations","text":"<p>Performs simple calculations with span attributes showing operands and results.</p>"},{"location":"demos/custom-demo/#process-order-processing","title":"<code>/process</code> - Order Processing","text":"<p>Handles order processing with multiple sub-operations (validation, payment, inventory).</p>"},{"location":"demos/custom-demo/#observability-features","title":"Observability Features","text":""},{"location":"demos/custom-demo/#opentelemetry-traces","title":"OpenTelemetry Traces","text":"<p>Both services are instrumented with OpenTelemetry auto-instrumentation:</p> <ul> <li>Flask instrumentation: Automatic HTTP server spans</li> <li>Requests instrumentation: Automatic HTTP client spans</li> <li>Custom spans: Business logic operations with attributes</li> </ul> <p>Span attributes include:</p> <ul> <li><code>http.method</code>, <code>http.route</code>, <code>http.status_code</code></li> <li><code>calculation.operand_a</code>, <code>calculation.operand_b</code>, <code>calculation.result</code></li> <li><code>order.id</code>, <code>order.status</code></li> <li><code>payment.amount</code>, <code>payment.method</code></li> </ul>"},{"location":"demos/custom-demo/#prometheus-metrics","title":"Prometheus Metrics","text":"<p>Frontend exports custom metrics:</p> <ul> <li><code>demo_frontend_requests_total{endpoint, status}</code>: Request counter</li> <li><code>demo_frontend_request_duration_seconds{endpoint}</code>: Request histogram</li> </ul> <p>Metrics are scraped from port 8000 and pushed to the OTel Collector via remote write.</p>"},{"location":"demos/custom-demo/#automatic-traffic-generation","title":"Automatic Traffic Generation","text":"<p>The frontend includes a background thread that continuously generates requests:</p> <ul> <li>50%: Process orders (complex traces)</li> <li>20%: Calculate (service-to-service)</li> <li>20%: Hello (simple requests)</li> <li>10%: Errors (failure scenarios)</li> </ul> <p>Requests occur every 3-8 seconds with random intervals.</p>"},{"location":"demos/custom-demo/#deployment","title":"Deployment","text":""},{"location":"demos/custom-demo/#helm-installation","title":"Helm Installation","text":"<pre><code># Install demos chart\nhelm install ollyscale-demos charts/ollyscale-demos \\\n  --namespace ollyscale-demos \\\n  --create-namespace \\\n  --values charts/ollyscale-demos/values-local-dev.yaml\n</code></pre>"},{"location":"demos/custom-demo/#argocd-deployment-recommended","title":"ArgoCD Deployment (Recommended)","text":"<p>The demos are managed by ArgoCD via Terraform:</p> <pre><code># Enable custom demo in Terraform\ncd .kind\nterraform apply -var=\"custom_demo_enabled=true\"\n</code></pre>"},{"location":"demos/custom-demo/#configuration-options","title":"Configuration Options","text":"<p>Enable/disable via Helm values:</p> <pre><code>customDemo:\n  enabled: true # Set to false to disable\n\n  frontend:\n    image:\n      repository: ghcr.io/ryanfaircloth/demo-frontend\n      tag: latest\n\n    env:\n      otelExporterOtlpEndpoint: \"http://gateway-collector.ollyscale.svc.cluster.local:4317\"\n      otelServiceName: \"demo-frontend\"\n\n  backend:\n    image:\n      repository: ghcr.io/ryanfaircloth/demo-backend\n      tag: latest\n</code></pre>"},{"location":"demos/custom-demo/#access","title":"Access","text":"<p>After deployment, access the frontend via HTTPRoute:</p> <pre><code># Check deployment status\nkubectl get pods -n ollyscale-demos\n\n# Access frontend\ncurl https://demo-frontend.ollyscale.test:49443/\n\n# Or use port-forward\nkubectl port-forward -n ollyscale-demos svc/demo-frontend 5000:5000\ncurl http://localhost:5000/\n</code></pre>"},{"location":"demos/custom-demo/#viewing-telemetry","title":"Viewing Telemetry","text":"<p>Open ollyScale UI at <code>https://ollyscale.ollyscale.test</code> to see:</p> <ol> <li>Service Map: Visual graph showing frontend \u2192 backend relationships</li> <li>Traces: Distributed traces with detailed timing and attributes</li> <li>Logs: Application logs with trace context</li> <li>Metrics: RED metrics (Rate, Errors, Duration) per service</li> </ol>"},{"location":"demos/custom-demo/#development","title":"Development","text":""},{"location":"demos/custom-demo/#building-local-images","title":"Building Local Images","text":"<pre><code># Build and push to local registry\ncd charts\n./build-and-push-local.sh v2.1.x-custom-demo\n\n# Images will be built and pushed:\n# - registry.ollyscale.test:49443/ollyscale/demo-frontend:v2.1.x-custom-demo\n# - registry.ollyscale.test:49443/ollyscale/demo-backend:v2.1.x-custom-demo\n</code></pre>"},{"location":"demos/custom-demo/#source-code","title":"Source Code","text":"<p>Demo source code is located in:</p> <ul> <li><code>apps/demo/frontend.py</code> - Frontend Flask application</li> <li><code>apps/demo/backend.py</code> - Backend Flask application</li> <li><code>apps/demo/requirements.txt</code> - Python dependencies</li> </ul> <p>Dockerfiles:</p> <ul> <li><code>docker/dockerfiles/Dockerfile.demo-frontend</code></li> <li><code>docker/dockerfiles/Dockerfile.demo-backend</code></li> </ul>"},{"location":"demos/custom-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"demos/custom-demo/#pods-not-starting","title":"Pods not starting","text":"<pre><code># Check pod status\nkubectl describe pod -n ollyscale-demos -l app.kubernetes.io/name=demo-frontend\n\n# Check logs\nkubectl logs -n ollyscale-demos -l app.kubernetes.io/name=demo-frontend\n</code></pre>"},{"location":"demos/custom-demo/#no-telemetry-in-ollyscale","title":"No telemetry in ollyScale","text":"<ol> <li>Verify OTel Collector is running:</li> </ol> <pre><code>kubectl get pods -n ollyscale -l app.kubernetes.io/name=opentelemetry-collector\n</code></pre> <ol> <li>Check demo environment variables:</li> </ol> <pre><code>kubectl get deployment demo-frontend -n ollyscale-demos -o yaml | grep OTEL_\n</code></pre> <ol> <li>Test collector connectivity:</li> </ol> <pre><code>kubectl exec -n ollyscale-demos deployment/demo-frontend -- \\\n  curl -v gateway-collector.ollyscale.svc.cluster.local:4317\n</code></pre>"},{"location":"demos/custom-demo/#httproute-not-working","title":"HTTPRoute not working","text":"<pre><code># Check HTTPRoute status\nkubectl get httproute -n ollyscale-demos demo-frontend -o yaml\n\n# Verify Gateway\nkubectl get gateway -n envoy-gateway-system cluster-gateway\n</code></pre>"},{"location":"demos/custom-demo/#traffic-generation","title":"Traffic Generation","text":"<p>A traffic generation script is provided to continuously send requests to the custom demo and create realistic observability data.</p>"},{"location":"demos/custom-demo/#usage","title":"Usage","text":"<pre><code># From the charts/ollyscale-demos directory\ncd charts/ollyscale-demos\n./generate-custom-demo-traffic.sh\n</code></pre> <p>The script:</p> <ul> <li>Sends requests to <code>https://demo-frontend.ollyscale.test:49443</code> (no port-forward needed)</li> <li>Generates realistic traffic patterns:</li> <li>50%: <code>/process-order</code> - Complex distributed traces</li> <li>20%: <code>/calculate</code> - Service-to-service calls</li> <li>20%: <code>/hello</code> - Simple requests</li> <li>10%: <code>/error</code> - Error scenarios</li> <li>Displays real-time request status with color-coded output</li> <li>Uses random delays (0.5-2 seconds) between requests</li> </ul>"},{"location":"demos/custom-demo/#requirements","title":"Requirements","text":"<ul> <li>Custom demo deployed via Helm/ArgoCD</li> <li>HTTPRoute configured and working</li> <li>Envoy Gateway running</li> </ul> <p>Press <code>Ctrl+C</code> to stop the traffic generator.</p>"},{"location":"demos/custom-demo/#example-use-cases","title":"Example Use Cases","text":""},{"location":"demos/custom-demo/#testing-service-dependencies","title":"Testing Service Dependencies","text":"<p>Use the <code>/process-order</code> endpoint to generate complex traces showing multiple service interactions.</p>"},{"location":"demos/custom-demo/#error-tracking","title":"Error Tracking","text":"<p>The <code>/error</code> endpoint creates error traces with exception details for testing error monitoring.</p>"},{"location":"demos/custom-demo/#load-testing","title":"Load Testing","text":"<p>Run the traffic generation script or adjust its timing for sustained load testing.</p>"},{"location":"demos/custom-demo/#metrics-analysis","title":"Metrics Analysis","text":"<p>Export Prometheus metrics to analyze request rates, error rates, and latency distributions.</p>"},{"location":"demos/custom-demo/#migration-from-k8s-demo","title":"Migration from k8s-demo","text":"<p>If you previously used <code>k8s-demo/</code>, the new Helm chart provides:</p> <ul> <li>\u2705 Same functionality with cleaner deployment</li> <li>\u2705 HTTPRoute integration (no LoadBalancer needed)</li> <li>\u2705 GitOps-ready via ArgoCD</li> <li>\u2705 Easy enable/disable via values</li> <li>\u2705 Local registry support for development</li> </ul> <p>See Migration Guide for details.</p>"},{"location":"demos/otel-demo/","title":"OpenTelemetry Demo","text":"<p>The OpenTelemetry Demo is the official demo application from the OpenTelemetry project, showcasing a realistic microservices-based e-commerce system.</p>"},{"location":"demos/otel-demo/#overview","title":"Overview","text":"<p>The OTel Demo (also known as \"Astronomy Shop\") is a complete distributed system featuring:</p> <ul> <li>11+ microservices in multiple languages (Go, Java, Node.js, Python, .NET, Rust, PHP)</li> <li>Realistic workflows: Product browsing, shopping cart, checkout, payments</li> <li>Built-in load generator: Automatic traffic simulation</li> <li>Rich telemetry: Traces, metrics, logs across all services</li> </ul> <p>When deployed with ollyScale, all telemetry data is sent to ollyScale's collectors instead of bundled backends.</p>"},{"location":"demos/otel-demo/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  frontend       \u2502\u2500\u2500\u2500\u2500\u2192\u2502 productcatalog  \u2502     \u2502  currencyservice\u2502\n\u2502  (TypeScript)   \u2502     \u2502  (Go)           \u2502     \u2502  (.NET)         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502 cartservice     \u2502\u2500\u2500\u2500\u2500\u2192\u2502 redis           \u2502\n         \u2502              \u2502  (Go)           \u2502     \u2502  (cache)        \u2502\n         \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502 checkoutservice \u2502\u2500\u2500\u2500\u2500\u2192\u2502 paymentservice  \u2502\n                        \u2502  (Go)           \u2502     \u2502  (Node.js)      \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502 shippingservice \u2502\n                                 \u2502              \u2502  (Rust)         \u2502\n                                 \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 \u2502              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192\u2502 emailservice    \u2502\n                                                \u2502  (Ruby)         \u2502\n                                                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n                 \u2193 OTLP\n        gateway-collector \u2192 ollyScale UI\n</code></pre>"},{"location":"demos/otel-demo/#services","title":"Services","text":"Service Language Purpose frontend TypeScript Web UI and BFF (Backend for Frontend) productcatalog Go Product inventory and search cartservice Go Shopping cart management with Redis checkoutservice Go Order processing orchestration paymentservice Node.js Payment processing currencyservice .NET Currency conversion shippingservice Rust Shipping cost calculation emailservice Ruby Order confirmation emails recommendationservice Python Product recommendations adservice Java Advertisement serving loadgenerator Python/Locust Traffic simulation"},{"location":"demos/otel-demo/#features","title":"Features","text":""},{"location":"demos/otel-demo/#multi-language-instrumentation","title":"Multi-Language Instrumentation","text":"<p>The demo showcases OpenTelemetry instrumentation across:</p> <ul> <li>Go: Auto-instrumentation with <code>otelhttp</code>, <code>otelgrpc</code></li> <li>Java: Auto-instrumentation via agent</li> <li>Python: Auto-instrumentation via <code>opentelemetry-instrument</code></li> <li>.NET: Native OTel SDK</li> <li>Node.js: Auto-instrumentation via SDK</li> <li>Rust: Manual instrumentation</li> <li>Ruby: SDK instrumentation</li> </ul>"},{"location":"demos/otel-demo/#realistic-scenarios","title":"Realistic Scenarios","text":"<p>The load generator creates realistic user flows:</p> <ul> <li>Browse products</li> <li>Add items to cart</li> <li>View cart</li> <li>Checkout process</li> <li>Payment processing</li> <li>Shipping calculation</li> </ul>"},{"location":"demos/otel-demo/#rich-telemetry","title":"Rich Telemetry","text":"<p>Traces:</p> <ul> <li>Multi-service distributed traces</li> <li>gRPC and HTTP spans</li> <li>Database queries (Redis)</li> <li>External service calls</li> </ul> <p>Metrics:</p> <ul> <li>Request rates per service</li> <li>Error rates</li> <li>Latency histograms</li> <li>Custom business metrics</li> </ul> <p>Logs:</p> <ul> <li>Structured logs with trace context</li> <li>Error logs</li> <li>Business event logs</li> </ul>"},{"location":"demos/otel-demo/#deployment","title":"Deployment","text":""},{"location":"demos/otel-demo/#helm-installation","title":"Helm Installation","text":"<pre><code># Install OTel Demo via ollyscale-demos chart\nhelm install ollyscale-demos charts/ollyscale-demos \\\n  --namespace ollyscale-demos \\\n  --create-namespace \\\n  --set customDemo.enabled=false \\\n  --set otelDemo.enabled=true\n</code></pre>"},{"location":"demos/otel-demo/#argocd-deployment-recommended","title":"ArgoCD Deployment (Recommended)","text":"<p>Enable via Terraform:</p> <pre><code>cd .kind\nterraform apply -var=\"otel_demo_enabled=true\" -var=\"custom_demo_enabled=false\"\n</code></pre>"},{"location":"demos/otel-demo/#configuration","title":"Configuration","text":"<p>The demo is deployed as a subchart dependency with ollyScale configuration:</p> <pre><code>otelDemo:\n  enabled: true\n\n  httpRoute:\n    enabled: true\n    hostname: otel-demo.ollyscale.test\n\nopentelemetry-demo:\n  # Disable bundled observability backends\n  opentelemetry-collector:\n    enabled: false\n  jaeger:\n    enabled: false\n  grafana:\n    enabled: false\n  prometheus:\n    enabled: false\n  opensearch:\n    enabled: false\n\n  # Configure OTLP export to ollyScale\n  default:\n    env:\n      - name: OTEL_EXPORTER_OTLP_ENDPOINT\n        value: \"http://gateway-collector.ollyscale.svc.cluster.local:4318\"\n      - name: OTEL_EXPORTER_OTLP_PROTOCOL\n        value: \"http/protobuf\"\n</code></pre>"},{"location":"demos/otel-demo/#access","title":"Access","text":""},{"location":"demos/otel-demo/#web-ui","title":"Web UI","text":"<p>After deployment, access the frontend:</p> <pre><code># Via HTTPRoute\ncurl https://otel-demo.ollyscale.test/\n\n# Or browser\nopen https://otel-demo.ollyscale.test/\n</code></pre>"},{"location":"demos/otel-demo/#service-endpoints","title":"Service Endpoints","text":"<p>The frontend provides:</p> <ul> <li>Homepage: Product catalog</li> <li>Product details: Individual product pages</li> <li>Cart: Shopping cart management</li> <li>Checkout: Order placement</li> </ul>"},{"location":"demos/otel-demo/#viewing-telemetry","title":"Viewing Telemetry","text":"<p>Open ollyScale UI at <code>https://ollyscale.ollyscale.test</code> to explore:</p>"},{"location":"demos/otel-demo/#service-map","title":"Service Map","text":"<p>See all 11+ services and their relationships:</p> <ul> <li>Frontend calling multiple backend services</li> <li>Checkout orchestrating payment, shipping, email</li> <li>Product catalog serving frontend and recommendations</li> </ul>"},{"location":"demos/otel-demo/#distributed-traces","title":"Distributed Traces","text":"<p>Example trace flow for checkout:</p> <ol> <li><code>frontend</code> - User initiates checkout</li> <li><code>checkoutservice</code> - Orchestrates order processing</li> <li><code>cartservice</code> - Retrieves cart items from Redis</li> <li><code>productcatalog</code> - Gets product details</li> <li><code>currencyservice</code> - Converts prices</li> <li><code>paymentservice</code> - Processes payment</li> <li><code>shippingservice</code> - Calculates shipping</li> <li><code>emailservice</code> - Sends confirmation</li> </ol> <p>Traces show:</p> <ul> <li>Total duration: ~500ms-2s</li> <li>Service-level timings</li> <li>gRPC/HTTP method details</li> <li>Error conditions</li> </ul>"},{"location":"demos/otel-demo/#metrics","title":"Metrics","text":"<p>View RED metrics per service:</p> <ul> <li>Rate: Requests/second</li> <li>Errors: Error rate %</li> <li>Duration: p50, p95, p99 latencies</li> </ul>"},{"location":"demos/otel-demo/#logs","title":"Logs","text":"<p>Application logs with:</p> <ul> <li>Trace IDs for correlation</li> <li>Structured fields (service, level, message)</li> <li>Error stack traces</li> </ul>"},{"location":"demos/otel-demo/#resource-requirements","title":"Resource Requirements","text":"<p>The OTel Demo is more resource-intensive than the custom demo:</p> <pre><code># Recommended resources for local dev\nresources:\n  requests:\n    memory: \"2Gi\"\n    cpu: \"1000m\"\n  limits:\n    memory: \"4Gi\"\n    cpu: \"2000m\"\n</code></pre> <p>Note: For local KIND clusters, ensure Docker/Podman has sufficient resources allocated (at least 4GB RAM).</p>"},{"location":"demos/otel-demo/#load-generation","title":"Load Generation","text":"<p>The demo includes an automatic load generator that:</p> <ul> <li>Simulates realistic user behavior</li> <li>Generates continuous traffic</li> <li>Creates diverse trace patterns</li> <li>Includes error scenarios</li> </ul> <p>To adjust load intensity, modify the load generator settings in Helm values.</p>"},{"location":"demos/otel-demo/#use-cases","title":"Use Cases","text":""},{"location":"demos/otel-demo/#multi-language-observability","title":"Multi-Language Observability","text":"<p>Perfect for testing ollyScale with polyglot microservices. See how different languages export telemetry.</p>"},{"location":"demos/otel-demo/#complex-distributed-traces","title":"Complex Distributed Traces","text":"<p>Explore deeply nested traces with 5-10 spans across multiple services.</p>"},{"location":"demos/otel-demo/#service-mesh-testing","title":"Service Mesh Testing","text":"<p>If using Istio or Linkerd, see how mesh-generated spans integrate with application spans.</p>"},{"location":"demos/otel-demo/#performance-analysis","title":"Performance Analysis","text":"<p>Identify bottlenecks in the checkout flow by analyzing trace timings.</p>"},{"location":"demos/otel-demo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"demos/otel-demo/#high-resource-usage","title":"High Resource Usage","text":"<pre><code># Check pod resource usage\nkubectl top pods -n ollyscale-demos\n\n# Scale down load generator\nkubectl scale deployment loadgenerator -n ollyscale-demos --replicas=0\n</code></pre>"},{"location":"demos/otel-demo/#services-not-starting","title":"Services Not Starting","text":"<pre><code># Check pod status\nkubectl get pods -n ollyscale-demos\n\n# Describe failing pod\nkubectl describe pod &lt;pod-name&gt; -n ollyscale-demos\n\n# Check logs\nkubectl logs &lt;pod-name&gt; -n ollyscale-demos\n</code></pre>"},{"location":"demos/otel-demo/#no-telemetry","title":"No Telemetry","text":"<p>Verify OTLP endpoint configuration:</p> <pre><code># Check environment variable\nkubectl get deployment frontend -n ollyscale-demos -o yaml | grep OTEL_EXPORTER\n\n# Test collector connectivity\nkubectl exec -n ollyscale-demos deployment/frontend -- \\\n  curl -v gateway-collector.ollyscale.svc.cluster.local:4318\n</code></pre>"},{"location":"demos/otel-demo/#httproute-not-working","title":"HTTPRoute Not Working","text":"<pre><code># Check HTTPRoute\nkubectl get httproute otel-demo-frontend -n ollyscale-demos -o yaml\n\n# Verify backend service\nkubectl get svc -n ollyscale-demos | grep frontendproxy\n</code></pre>"},{"location":"demos/otel-demo/#chart-version","title":"Chart Version","text":"<p>The ollyscale-demos chart uses OpenTelemetry Demo Helm chart version:</p> <pre><code>dependencies:\n  - name: opentelemetry-demo\n    version: \"0.33.0\"\n    repository: https://open-telemetry.github.io/opentelemetry-helm-charts\n</code></pre> <p>To update to a newer version, modify <code>charts/ollyscale-demos/Chart.yaml</code> and run:</p> <pre><code>cd charts/ollyscale-demos\nhelm dependency update\n</code></pre>"},{"location":"demos/otel-demo/#comparison-with-custom-demo","title":"Comparison with Custom Demo","text":"Feature Custom Demo OTel Demo Services 2 11+ Languages Python Go, Java, Python, .NET, Node.js, Rust, Ruby Complexity Simple Production-realistic Resource Usage Low (~256MB) High (~2-4GB) Traces 3-5 spans 10-20 spans Best For Quick testing, development Comprehensive demos, training"},{"location":"demos/otel-demo/#migration-from-k8s-otel-demo","title":"Migration from k8s-otel-demo","text":"<p>If you previously used <code>k8s-otel-demo/</code>, the new Helm chart provides:</p> <ul> <li>\u2705 Same demo application, cleaner deployment</li> <li>\u2705 HTTPRoute integration for ingress</li> <li>\u2705 GitOps-ready via ArgoCD</li> <li>\u2705 Managed as subchart dependency</li> <li>\u2705 Automatic OTLP configuration</li> </ul> <p>See Migration Guide for details.</p>"},{"location":"demos/otel-demo/#references","title":"References","text":"<ul> <li>OpenTelemetry Demo Documentation</li> <li>Demo GitHub Repository</li> <li>Demo Helm Chart</li> </ul>"}]}